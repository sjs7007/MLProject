{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1038 characters, 46 unique.\n",
      "----\n",
      " rzsgc\"mzaG4jW:(tJsF1TFfnigvtA)gJhjW1SvfD.:AuekGdf)C(Fjuj,gij:pcbuWzOdm\"S1vopvfxCvJuDd\"p (krkd\"cwbkC1u(9jef.ycTxGa14j9FO:ur,dF9sihov:cACObh 4kgWDsDw9x4ybh fJw .emsT9s1mld\"Wocn..x,ss4WNJdTjSznrex1icmToN \n",
      "----\n",
      "iter 0, loss: 95.716032\n",
      "----\n",
      " sre)trae \"aoaalo ami )gs xhi dh, nDio\"tuTaey9a)nejpnnehl iSeense  to ce  iu4 ljSie)vC eg jenisnoifjo ateomani\"sf ai  ien wm1r )e am pj,e t e smyy\"zu9ihh eou fhot\"So  stroo  neCDswS hnNr\"jz arnh\"wg tco \n",
      "----\n",
      "iter 100, loss: 96.443984\n",
      "----\n",
      " nhoogynvpiterdiSohusxudnA iodtem  diw ueFaae  tynten wvnna,oopissams aOt enen e pluk inlhlsu  ee anieri hlsnprimea  aawohgechsienseee aopaa4 enonay , ifcsiyoha dgnsgae 49mxFat g ang uA.li seioehdclean \n",
      "----\n",
      "iter 200, loss: 94.912036\n",
      "----\n",
      " n xppiar vxp9rd maunpnact lp uane bs ecsedOpmiwattoyp Nr  mye4fieNAnstist Theeiop Wic dne kgn oTe af lvgp fsse,miafse so. yfe bx lpbt vmt rep ws sfseTba oTdeu ed yptvsa l1aratgm rhn A tiant dmseawhexe \n",
      "----\n",
      "iter 300, loss: 93.205453\n",
      "----\n",
      " o  noatiujeernosasatadlnm  ucui4dnS c s cnbepn tcfeiiw m refod hsczstenactTsstrsomrhcoleintnt (e1 dis  nWerscltaOmrnmotetwt nutsogmaesths taim acteec9enedlsn anJ   me sci h  ritpfe pesaz ncmliceaautta \n",
      "----\n",
      "iter 400, loss: 91.401670\n",
      "----\n",
      " ae4enehe ene s fopaouaT 9tiNman ie timnhabcveth (nnros nme d jufena)bmiles veh4cpesf raDwhhhief theeyuaidceW n nm9pwt r iy nuush9boeteTlame,: stissed dgecetiict94ttld T9m,ow nG a4 emid eseoWakFnrhts k \n",
      "----\n",
      "iter 500, loss: 89.424526\n",
      "----\n",
      " arete  ioxeien anm bieeweteradeit n hesanorooon petplest  f tadsaodpiols dinDxh ncorh igvf t Nhttoce4 ravs usthed ayfarr, e oeaabor npwlaens afay ha(omemes iampe trypel prhpoutmme apn  on mfathei nhaa \n",
      "----\n",
      "iter 600, loss: 87.428725\n",
      "----\n",
      " d tTs lnitorameeonheCaaancdief xemme epid th d tich st pt 1, pesmtrs Fhyen t geticefptsi\"nht bp mplhunans rh hltti awaes eor t mesl ne awaris Tad rys biediw Sh worc as yntis ruimp uitat.cre t anye nh  \n",
      "----\n",
      "iter 700, loss: 85.271896\n",
      "----\n",
      " 9xFpGd a,r hixaerotuggerrelaor ces ach ce feNe oAnfcebthe k f rey fhenm 4n ees pbedhey\"i z edethgtthorisertils tare evseons bywh rehet cFrt rt ecavhgsthos ij mt9verd vgen tlt pSnedcrtinbtede airsewrre \n",
      "----\n",
      "iter 800, loss: 83.223619\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-78763fcb83c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m   \u001b[0;31m# forward seq_length characters through the net and fetch gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m   \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWxh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlossFun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhprev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m   \u001b[0msmooth_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmooth_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.999\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'iter %d, loss: %f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# print progress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-78763fcb83c6>\u001b[0m in \u001b[0;36mlossFun\u001b[0;34m(inputs, targets, hprev)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# encode in 1-of-k representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWxh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWhh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# hidden state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mby\u001b[0m \u001b[0;31m# unnormalized log probabilities for next chars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# probabilities for next chars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)\n",
    "BSD License\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "# data I/O\n",
    "data = open('input.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters---> 43     loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "  \"\"\"\n",
    "  inputs,targets are both list of integers.\n",
    "  hprev is Hx1 array of initial hidden state\n",
    "  returns the loss, gradients on model parameters, and last hidden state\n",
    "  \"\"\"\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  loss = 0\n",
    "  # forward pass\n",
    "  for t in range(len(inputs)):\n",
    "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "    xs[t][inputs[t]] = 1\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "   # print(\"t\",t)\n",
    "   # print(\"shape\",(ps[t]).shape)\n",
    "   # print(\"targets\",targets[t])\n",
    "    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "  # backward pass: compute gradients going backwards\n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "  for t in reversed(range(len(inputs))):\n",
    "    dy = np.copy(ps[t])\n",
    "    dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    dby += dy\n",
    "    dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "    dbh += dhraw\n",
    "    dWxh += np.dot(dhraw, xs[t].T)\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "    dhnext = np.dot(Whh.T, dhraw)\n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "  \"\"\" \n",
    "  sample a sequence of integers from the model \n",
    "  h is memory state, seed_ix is seed letter for first time step\n",
    "  \"\"\"\n",
    "  x = np.zeros((vocab_size, 1))\n",
    "  x[seed_ix] = 1\n",
    "  ixes = []\n",
    "  for t in range(n):\n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    y = np.dot(Why, h) + by\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[ix] = 1\n",
    "    ixes.append(ix)\n",
    "  return ixes\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "while True:\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  if p+seq_length+1 >= len(data) or n == 0: \n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "    p = 0 # go from start of data\n",
    "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "  # sample from the model now and then\n",
    "  if n % 100 == 0:\n",
    "    sample_ix = sample(hprev, inputs[0], 200)\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient\n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "  if n % 100 == 0: print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "  \n",
    "  # perform parameter update with Adagrad\n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "    mem += dparam * dparam\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "  p += seq_length # move data pointer\n",
    "  n += 1 # iteration counter \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable        Type        Data/Info\n",
      "-------------------------------------\n",
      "Whh             ndarray     100x100: 10000 elems, type `float64`, 80000 bytes\n",
      "Why             ndarray     46x100: 4600 elems, type `float64`, 36800 bytes\n",
      "Wxh             ndarray     100x46: 4600 elems, type `float64`, 36800 bytes\n",
      "bh              ndarray     100x1: 100 elems, type `float64`, 800 bytes\n",
      "by              ndarray     46x1: 46 elems, type `float64`, 368 bytes\n",
      "char_to_ix      dict        n=46\n",
      "chars           list        n=46\n",
      "dWhh            ndarray     100x100: 10000 elems, type `float64`, 80000 bytes\n",
      "dWhy            ndarray     46x100: 4600 elems, type `float64`, 36800 bytes\n",
      "dWxh            ndarray     100x46: 4600 elems, type `float64`, 36800 bytes\n",
      "data            str         Nominative determinism is<...>es for those professions.\n",
      "data_size       int         1038\n",
      "dbh             ndarray     100x1: 100 elems, type `float64`, 800 bytes\n",
      "dby             ndarray     46x1: 46 elems, type `float64`, 368 bytes\n",
      "dparam          ndarray     46x1: 46 elems, type `float64`, 368 bytes\n",
      "hidden_size     int         100\n",
      "hprev           ndarray     100x1: 100 elems, type `float64`, 800 bytes\n",
      "inputs          list        n=25\n",
      "ix_to_char      dict        n=46\n",
      "learning_rate   float       0.1\n",
      "loss            float64     54.1473756192\n",
      "lossFun         function    <function lossFun at 0x7ff320287f28>\n",
      "mWhh            ndarray     100x100: 10000 elems, type `float64`, 80000 bytes\n",
      "mWhy            ndarray     46x100: 4600 elems, type `float64`, 36800 bytes\n",
      "mWxh            ndarray     100x46: 4600 elems, type `float64`, 36800 bytes\n",
      "mbh             ndarray     100x1: 100 elems, type `float64`, 800 bytes\n",
      "mby             ndarray     46x1: 46 elems, type `float64`, 368 bytes\n",
      "mem             ndarray     46x1: 46 elems, type `float64`, 368 bytes\n",
      "n               int         888\n",
      "np              module      <module 'numpy' from '/ho<...>kages/numpy/__init__.py'>\n",
      "p               int         675\n",
      "param           ndarray     46x1: 46 elems, type `float64`, 368 bytes\n",
      "sample          function    <function sample at 0x7ff32018bc80>\n",
      "sample_ix       list        n=200\n",
      "seq_length      int         25\n",
      "smooth_loss     float64     81.3867699148\n",
      "targets         list        n=25\n",
      "txt             str         9xFpGd a,r hixaerotuggerr<...>pSnedcrtinbtede airsewrre\n",
      "vocab_size      int         46\n"
     ]
    }
   ],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nominative determinism is the hypothesis that people are drawn to professions that fit their name. The term was first used in the magazine New Scientist in 1994, after its humorous Feedback column mentioned a book on polar explorations by Daniel Snowman and an article on urology by researchers named Splatt and Weedon. The hypothesis had been suggested by psychologist Carl Jung, citing as an example Sigmund Freud (German for \"joy\"), who studied pleasure. A few recent empirical studies have indicated that certain professions are disproportionately represented by people with appropriate surnames, though the methods of these studies have been challenged. One explanation for nominative determinism is the theory of implicit egotism, which states that humans have an unconscious preference for things they associate with themselves. An alternative explanation is genetic: an ancestor might have been named Smith or Taylor according to their occupation, and the genes they passed down might correlate to aptitudes for those professions.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "k\n",
    "k\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "34\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 612 characters, 45 unique.\n",
      "----\n",
      " Pöy,rbdslnäSiLwäohfggPü dSuatnvyjw.ffäIöiOnLnHGSccnb'rhAyorCWkihsäGäsyC'rzd.SbgcqmblOnO,tLWWwsfiäSjPSGr.ükEdjyBgIdbnnPCRjvhBrRywmjw,EwaHql'Aü'BsOEüLlüsscSRykehAafPönLWnbvz ze'fuCvRvjfebbcPGoBIEOeuälfW \n",
      "----\n",
      "iter 0, loss: 95.166564\n",
      "----\n",
      " iiüeb. cakhBroeanagt eo if z  hi.t nach atedhm ,äaathesH oh c  Huneceileceznaneäane at k  . eh  g.wenodtedr t .ecemct .eüthBatm tm dane aBewhn.ugisHicm t   .k.eefhi.eftäd.chihhchBeR.Er  leuraseiG ceic \n",
      "----\n",
      "iter 1000, loss: 63.742458\n",
      "----\n",
      " gceitklh lt kakürnsnet hihhüieide  rtelhüAef gufechrda frifewes.er faGh dhift eidenLaibhshisHis inewu e.aln.al rt n c.k.tnewuaeftt  Wioedascec m ataeAem ntE gOfwfüIInof dt  ifnet necer,eOr f .räO nech \n",
      "----\n",
      "iter 2000, loss: 25.679307\n",
      "----\n",
      "  du cet  iaezr eiteihech dhihiii.eü rtae Hi    t dhaott nth.säenhih .aeO naneienwr eicenh hm taneihihich danh .t  enhihilHiodtfezchi.ttenst ntnemch atakssenf it dane onnid mtaaauter eh hnnd  ezueidule \n",
      "----\n",
      "iter 3000, loss: 9.975359\n",
      "----\n",
      " ledh hHit  nth ur iineit fecezseiwec.rth nah hig, daklenadufenseidufnruei sniwIn leakhBt nehüE hm t .Iseif om daklsencu eduedEch .aenr o o,eiG nt  gkl enLnautefezdhr aüt ig füft danece hiit iguwnntr h \n",
      "----\n",
      "iter 4000, loss: 3.951292\n",
      "----\n",
      " er kortecer eh snener mtanecanned m ch du iwüühieh ecezegeded mtceIseiwhnsneäezsdifec.rda cz.nünanf lnakiidefhzyIerue.Iseifafhnst nt ezchiwaünsainech .aechsg wer ltaneieOhich  ech at daaEch Iutesaichi \n",
      "----\n",
      "iter 5000, loss: 1.641609\n",
      "----\n",
      " a.tr et ecerdh nsenzafnrdh ntaeEaeOr enhhln. .t dalece h ch du wneidenefwur enim eOhEakiwenz nt dh  tanrtheHch häAt dakfwen.anht t   Annalewr  oIeihich da .ec. dhaEtedhähm tanech .tr gr nr en. en.unes \n",
      "----\n",
      "iter 6000, loss: 0.743753\n",
      "----\n",
      " f dt dakbseiweceIr drkild dt eioeig f am dakCwn h ct dakrweneie nrt  Hiceie  taniin f ft dakhwechIatr t nw.t dand   rtenaklr hn m ft ztu ertm at  gut dr Hle rakiwü Lhifnit eiiiben.h h  t,nnch öttt eBu \n",
      "----\n",
      "iter 7000, loss: 0.384339\n",
      "----\n",
      " ensu bsi.eiasfininet  wakGrnanehhEeg wüadhi.h dkürtgnreit du ehengu tscenhihm tar in dadt eAer f hHceicer.kifeie weih snenetezwgOf Aer enewünaecer enhiWh ininedefIntr ftrda .ece ia LeceIr naueenst .ec \n",
      "----\n",
      "iter 8000, loss: 0.232988\n",
      "----\n",
      " hihieh  n.r enhkAt  ätöggufeüt dumteioeid fec.rch wuünaseicIr m ceIst ned m cO du bsnida ürce h ch ka üif m,idhihm taotif mu eIr  ntei serdecezdhiütSdr Rl  anr n.s aseilewer.E fr aünedh dklnaut.rseedu \n",
      "----\n",
      "iter 9000, loss: 0.163517\n",
      "----\n",
      " ch ntaeouensaenhih ntannaolh u bhn.euanewech .t danhich dur ecerdh nah ütheoueisHec tanech ceanr tm dakzrn daftrdea ut nth .ten.tünnuS.rch nth nah h at zgu ertt gu ertm .t dakhieiben.I h .th .aenrutnn \n",
      "----\n",
      "iter 10000, loss: 0.127631\n",
      "----\n",
      " Ich nte .aet duno e f anabh LeiotineGenem unaEaOhüür.tr defezsninetezsgifaBh ener   haAeI Lt danber hich dh .aün.antr enh An  hm ttr dakwnech  t dakErennch ntt  aüEhiu bh  eh   idk ie dafhzin.nch htac \n",
      "----\n",
      "iter 11000, loss: 0.106371\n",
      "----\n",
      " eihech dasnit ntr m lh snewech dhiwse fait duaoenen d bhioeidibhioeie .trtr eOhiihben.r h  aanach g dt  iOhece du bhniduaeIseidutüzueistanesenwelnaseewer daom du otr gfn stanach hihmefanhihhiwerdeneni \n",
      "----\n",
      "iter 12000, loss: 0.092106\n",
      "----\n",
      " gce seiwech du leihbznhhih .aao sn wech dh scen kiüein da hzsnifed   itr et danedh lea .aüt dakwneOhigt dautecertSeEaen.aen  olt dhben.Ich .te .eit dakfrenaeeühig beichiheEted ttt dakhwürdeähr gif at  \n",
      "----\n",
      "iter 13000, loss: 0.081572\n",
      "----\n",
      "  dakf en. en.tannu eIueidufezseodu eIrhintei  ot  unene Lü.ecal  ztm danI enhece ce häcehiur eceIseifIceIdh .kln.In h dt dauteehr et klr.easm n.h h  tiftetezuleIseifaohrid ngfür ei. ftr onfecez ut et  \n",
      "----\n",
      "iter 14000, loss: 0.073322\n",
      "----\n",
      " l  hm tanIzhmaäechich dhiftSnehn fach igiwech dakwrenzuensanch at dhmttanechihich du wec mtanüweOhiei in.r   Ertasranask cech dakIrenaut.nseewh  dadaf dt eAeh aki en.acerdh nae,h gtwnen.anthih .tanawe \n",
      "----\n",
      "iter 15000, loss: 0.066631\n",
      "----\n",
      " er IüOh gtwnhacech daküie .aüt dakfren.eäh iniweceEdhr tSnEer m  t dakwre  aaneihibech ihiEif faortaäencg fücezceiLholfer naat ig wnr h ch du .ech dakwrneiden. .r eioeigifece chihice e chih lel .ant   \n",
      "----\n",
      "iter 16000, loss: 0.061068\n",
      "----\n",
      " aünr  naeAeh at dasf dh deOhio wnasf it du edannened  ewAeihIl akifei.ertlnnse ftanach .tan.tenrsence h iniwen Ltakiwer  olr f ötidewür nr ftaseie esanned mtakiüeifecezdhileioeinewet mtr m it dakfwen  \n",
      "----\n",
      "iter 17000, loss: 0.056322\n",
      "----\n",
      " f dt dar Rontag f ct daklse Lutessenm it daköcenadenenendu ezseidu erce .tatr enh Rt danf hmmtakörenace.Iwaseiiiwe   aakhit bsnim nth at dakGseiwhnewu  sHen et  it mich dakhieihIüOr etr Rnesiodef zcak \n",
      "----\n",
      "iter 18000, loss: 0.052226\n",
      "----\n",
      " enst bhrom ce kurbhn tt e ch htA,h e   oonrt nse .t nrtheoaenaIn f  t igkwnach lt kgneieiwenhd dhbhn.eih lthnsaenhitm dakwr nasn nah.öse .ace dhnnth nae h om danwnasf ln.aeOh utaseidefhzseif lt änum n \n",
      "----\n",
      "iter 19000, loss: 0.048676\n",
      "----\n",
      " h Etm naeOainenr   t  dae    r Iälenu bhnidaaeIsnenet   .r s.eded mich duküenifach danhih Etanecenh tt dann feGIakhit   ln.antr enh tt danwnenh ot dau erthcu bsn.d nt  akidenh ft dakidenh At  h ecen   \n",
      "----\n",
      "iter 20000, loss: 0.045567\n",
      "----\n",
      " ch .tanasn.rt n st fech dakicenh ht dakfrenasendu ezsen.acerdh .aütr en  ile.g f ftr enhichr eicsl  hSdIi m cakgif strnach .aeoh lt iSkiwen.alür enr fnisend ftrdh lse .aio ice du ownenec   hlIut üuthz \n",
      "----\n",
      "iter 21000, loss: 0.042819\n",
      "----\n",
      " Ich ngg m cn aar BanIch .aet danzüg ftrt danf hrwnan.u ezce hähm tanach .tenr iofaler Lt dakören.aenhich den  tm danhicaIrnanch.Ir  ftatöit nih .aüOr en.eio  lt danena frot dauwnnawid feceidenhih .tin \n",
      "----\n",
      "iter 22000, loss: 0.040368\n",
      "----\n",
      " eifefeEdh caAr kilt  t  aObit m  t daklceih ot Baneie h lt eauteaeIu ensh.ase .aitrdh haanaziodef  cakhieifefezchiwench h  t nnat neaem a hiAeh gufer m nt ig f .t  anh ceztedhäol  enh h  t n klrhiceiw \n",
      "----\n",
      "iter 23000, loss: 0.038180\n",
      "----\n",
      " gu oIg mich dur eä.Ise nt eiwaer g r dt  ünh at  akzieiwer dtaeiwei ei.eieiweb.Ich htAm daköwenidanenürwhiseidifeneI h  tt  aoldu ezueissur eitt na  gum ce ch Eihtrtakeseifun.Ich .ten  hm da efasnin n \n",
      "----\n",
      "iter 24000, loss: 0.036207\n",
      "----\n",
      "  dakfweiheür   n  tm ceihich dakhieihInth ur eileig fürt dakfcech ht denh akhieihiceidanaeeoh gufeceIsn wtenaseewh  nanihiu beihtwnOh gufeAeh hküteiLt n a n ih Iih ltansHi.ece heAeh gkö er eoleih Lten \n",
      "----\n",
      "iter 25000, loss: 0.034427\n",
      "----\n",
      " lt etr onzaüüsHif ih  tanr en  anh Atr en hifeid zwgOf dtleioeigif fm daklweOhrnah en.ua.rseedefeErnif ftr er kiOeinewhninetezdeiwech inifeferdh .afsfen.hihi.eeLr en.uferce .te  tSlhilh  echäht  on  t \n",
      "----\n",
      "iter 26000, loss: 0.032805\n",
      "----\n",
      " en.ecerdh .aenh lt kaküinif Ws if naaem ech at danr enh hm tanach .tr zr noäenr ilenen.ait duklse Lu .sseidef.rse .ao.rseifalh igim f.r en  olgir saünhigim dt eälh nth h  tanash.sr n itm da fta häarm  \n",
      "----\n",
      "iter 27000, loss: 0.031329\n",
      "----\n",
      " at  nech hm Iachih .tr .ech iu ner m ce chiüecezcn ntnenutezdeihtüe.ul .wgOhbAe  bh ceihich ieiwer i tr danf rt  ar iiüeieidefüiwev  enhi.ech cIa  hit beninetenag wecerce hkir  omöBanta ezseidu eEseiw \n",
      "----\n",
      "iter 28000, loss: 0.029971\n",
      "----\n",
      " f dt  üOhit ztSdIaakIrnsned m nthih  tS .eeh dgklüenh cr kar.anth at  lneäfnr enh tm dakwr nasfünse .Irt nautid fw.ez,tSnefez  eGut sHih ihiE f rtäledh ntennat mich dkLr lnchioedh .tr .aitceu eWür eio \n",
      "----\n",
      "iter 29000, loss: 0.028718\n",
      "----\n",
      " enute dahweh.rch .ten  .t dücI  nauR.aseedu ezcei.eech ch dhiüutezufeIu i seiwer mtannch .tanaEn.s aseii nateEdnifen.r h naeh dhmatanecenhihmhBaseieifeneIIh Etaeofenh tm danfert dakfrenakRr aiOöau oef \n",
      "----\n",
      "iter 30000, loss: 0.027568\n",
      "----\n",
      " h Etg wnanhich dhibek. daklreioen.Iü  Er nicenh ht tauoen m ht ,akmr  .aüüceu Lrüsf en.alnaaln.alnh tm dakfren.tS .aeOf en.eoedeneG  t dakGwen  t  wakhineh ut  auteteIdniwen.Ich htutateihech du wrt .t \n",
      "----\n",
      "iter 31000, loss: 0.026505\n",
      "----\n",
      " ch hihh sniwed zaathiheEtedaätlh gt  akhierwnceccu gär sHih ig f Aer etr fnese dWch dhiwren.afesHes in nag mich dakicen. .t dakörenr ln.an.hiu bhioei  ft onseif dt daubeu eii wei  i   dannef lcanasnid \n",
      "----\n",
      "iter 32000, loss: 0.025519\n",
      "----\n",
      " Ich nte .aeO  ar ieiwedhmeneh ut itreieiden.r h hthnatenIgRt daabet eihihiceöIaseieidug r et nautenhm echic. danhch.hih  t nechih er kiOein fa t du enen.a.t kichich dhaEce    m Banech ceh.asn nt eicen \n",
      "----\n",
      "iter 33000, loss: 0.024596\n",
      "----\n",
      " eiher d fü  c. dakgif rtenasendt hrefezuteEut.schnhih htakürenaten.aln.aünhitm dakicer eold nhf ln.an.r d   lnh ch dhiüif fhideit m dt easeieiceuogir ft nech EtrnEecesgRn it nid m dh däkhiech lt  olda \n",
      "----\n",
      "iter 34000, loss: 0.023739\n",
      "----\n",
      " ggüf et dakwren.eio i  gih sHih ieicech du cet m ch ihiErnestrnasneweihesnifech du wce  Wen ki eih .arh dakhid defhzseidefer gufert  gkf fezuReIse .alert eonenf  naseidener gu ertm .r kilein fach dhiw \n",
      "----\n",
      "iter 35000, loss: 0.022936\n",
      "----\n",
      "  dakf enr fnaseidechzIu feeo. e   iledh .ei   ürf nthBumtttrnao mtakiceih lt kürded m ch du  ei nat gih sHit da mtgnfeotrdakhwAtr enh lm takiwech st danhih .eit Bakicen.alnhaüthit .eiohBunana .tanant  \n",
      "----\n",
      "iter 36000, loss: 0.022187\n",
      "----\n",
      " le g d dh dn.r dt ei.eih ceakgin ntaeese natezwnch Am tautetemhniwech du ceh.Ich htathwAtruenr en.aenz tm danhicssBaneäannch rtannsf.wr nnae.r gür enh ut danf  nanenh ot dautenan h.Iah nae h ca ceih c \n",
      "----\n",
      "iter 37000, loss: 0.021482\n",
      "----\n",
      " er IlOhiüt dakfseicet Ica .aütr enhith daewer f  t ig f ft  auteni eS mkar in.neh.Ech Ethlsaar iiüwer m  chihibznhh nt  arf lt daklseiwh  wsendWch dtiüef Iaakhieiten.Iih dt eEutensnenu hzsninat m akhi \n",
      "----\n",
      "iter 38000, loss: 0.020823\n",
      "----\n",
      " ao dr noaen.se .ace dhiEtS .aütaecezt  oaech lt  olernascenh gnf Lt danG enh h ceiom nth at  gutedenif neceIBakwcenh hnede  eileih dteeatenauteEu erch .tet dhmazendid   lh en n  t cakhinideWer h wuens \n",
      "----\n",
      "iter 39000, loss: 0.020199\n",
      "----\n",
      " f dt  üOhit faler    äifeid bech daawcech  t danedanf eOhigufecerdh Iaeni olaeenh  mhBurten.r e h At dakGreiceI  tulesendu ezsh nt   aoltened  hzsnenetemwkOögu bsniteih ikhdug rten.aen.afür eO  n.e en \n",
      "----\n",
      "iter 40000, loss: 0.019606\n",
      "----\n",
      " enut  snedef nst nso na h inibinh  gif Aer g f ceIseifeo.rse .tanash.Ech Eae. nüifenhmdh  tt  glr hiOeigifecerdh .aenr en  hnecenh Anr en  enhihm tannst  ait ir hich deO dh  Wen ,lseieidenen dh ener e \n",
      "----\n",
      "iter 41000, loss: 0.019050\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)\n",
    "BSD License\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "# data I/O\n",
    "data = open('input_en_de.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters---> 43     loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "  \"\"\"\n",
    "  inputs,targets are both list of integers.\n",
    "  hprev is Hx1 array of initial hidden state\n",
    "  returns the loss, gradients on model parameters, and last hidden state\n",
    "  \"\"\"\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  loss = 0\n",
    "  # forward pass\n",
    "  for t in range(len(inputs)):\n",
    "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "    xs[t][inputs[t]] = 1\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "   # print(\"t\",t)\n",
    "   # print(\"shape\",(ps[t]).shape)\n",
    "   # print(\"targets\",targets[t])\n",
    "    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "  # backward pass: compute gradients going backwards\n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "  for t in reversed(range(len(inputs))):\n",
    "    dy = np.copy(ps[t])\n",
    "    dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    dby += dy\n",
    "    dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "    dbh += dhraw\n",
    "    dWxh += np.dot(dhraw, xs[t].T)\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "    dhnext = np.dot(Whh.T, dhraw)\n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "  \"\"\" \n",
    "  sample a sequence of integers from the model \n",
    "  h is memory state, seed_ix is seed letter for first time step\n",
    "  \"\"\"\n",
    "  x = np.zeros((vocab_size, 1))\n",
    "  x[seed_ix] = 1\n",
    "  ixes = []\n",
    "  for t in range(n):\n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    y = np.dot(Why, h) + by\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[ix] = 1\n",
    "    ixes.append(ix)\n",
    "  return ixes\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "while True:\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  if p+seq_length+1 >= len(data[0:282]) or n == 0: \n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "    p = 0 # go from start of data\n",
    "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_to_ix[ch] for ch in data[p+282:p+seq_length+282]]\n",
    "\n",
    "  # sample from the model now and then\n",
    "  if n % 1000 == 0:\n",
    "    sample_ix = sample(hprev, inputs[0], 200)\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient\n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "  if n % 1000 == 0: print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "  \n",
    "  # perform parameter update with Adagrad\n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "    mem += dparam * dparam\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "  p += seq_length # move data pointer\n",
    "  n += 1 # iteration counter \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ich nahm'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[282:290]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
