{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1038 characters, 46 unique.\n",
      "(46, 1) 46\n",
      "----\n",
      " kWC,k1DF,ket9OFj4yOodtdSaTpv11jSWzb eiWWSGS9CGvOmcsvsSlb(c4O9 d(caknOpp)ziDls,sOA. me::i.fm 9Gm(fv(Tm9,S(9(1\"hjtpW,WcG9AyA4m,v 4lyOedD lhC1olfpkFN)pmfmw\",AejnWDuCW) kav4SppCNwSWJWcdb\")OG ,vS . W,fxiO) \n",
      "----\n",
      "iter 0, loss: 95.716027\n",
      "(46, 1) 46\n",
      "----\n",
      " n rh,oier\"ld\"hpedjudo\" stb d ssesomj\",\"\",ed Gadadomf o\"jn),\"Snp  aslsus pt .)p,\"or\"i eud wepydidd  h r\"o.sexn oom\"jct\" )ra j\"vtg,ojjjgr\"w luw\"r   lcawd\"n\"dp suoo \"p\",hdje tjdode)owd  d ,mrdiooolnmdceo \n",
      "----\n",
      "iter 100, loss: 96.382929\n",
      "(46, 1) 46\n",
      "----\n",
      " e   t aae  ttom senc n t fsticest tyesec a eeattebpetss un mants iaitotitn eennavi i seny  sepeh ml  od ua p enm  nga ts eAnehat .   t n  x bet testys thsg: s lm iutettlhnntttmpoovttivvd rstgh:sthti e \n",
      "----\n",
      "iter 200, loss: 94.669321\n",
      "(46, 1) 46\n",
      "----\n",
      "  miydafietngtnte len e c dSTinoderod e pxenaSate f r  adeod ort cbeSc d y y n  aSk e  e ce4Sedor atd lat rlSd 9  y turWyyme somemeoShk y Wed snyenTh.he  udgofexuoryshd ason.wsi  pmA esant cocunenySSac \n",
      "----\n",
      "iter 300, loss: 92.727025\n",
      "(46, 1) 46\n",
      "----\n",
      "  metovh  an nheoratinhonscrhahharhhcaci anioncah, yihy ea Sbehannia sts,animatid tn h as ast hl liglu as e otath OenhshFayonusthdoanasunhctssi e  a bihoeha anh s omanoy,inuphha ham.honk iotereishi hhe \n",
      "----\n",
      "iter 400, loss: 90.695554\n",
      "(46, 1) 46\n",
      "----\n",
      " , nis sge t ne mater,dtanm cachrtic, b9sauchmotunamemum9ssd eetn ts 4at tturnutumeame, aneet nhn neeenasecaisheud nNlotc be t4gewr maTuude utitmmeegi thes t lt ,y nsdsrenwmemFfinafminde. sd pfcumtewit \n",
      "----\n",
      "iter 500, loss: 88.608208\n",
      "(46, 1) 46\n",
      "----\n",
      " ori aed tihtyp e rles noee eiachere plas. whe thhapdedpre coee pantuse iif os purtelh 1hhosipepeeelatkti,coantts aniathse s ore   tes teesly9forofeti reseeubn stetialjotate gafeunsve s s a nspbeshches \n",
      "----\n",
      "iter 600, loss: 86.531226\n",
      "(46, 1) 46\n",
      "----\n",
      "  neon s  id baplt lewator me sit fe hsry tiplosmoreshetismenatigem ar :eoormhass us he t mennot, tt poccorsed wotithace pr he thee : mos tads en bf tan nimeroteon us yafeom ropr at temeterpla atetas i \n",
      "----\n",
      "iter 700, loss: 84.368046\n",
      "(46, 1) 46\n",
      "----\n",
      " t tons tt armaor Aan.hhaucScinn. rodtif raterscatisudivpor. as atescavoreesprchtuutitivs iramesthavusG fagheScilsmisss tioter psssttcfharod, ad el anitilt nomifded atiti\"scy rad palprtibastyiit plher  \n",
      "----\n",
      "iter 800, loss: 82.347982\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a5f411651d99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m   \u001b[0;31m# forward seq_length characters through the net and fetch gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m   \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWxh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlossFun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhprev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m   \u001b[0msmooth_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmooth_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.999\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'iter %d, loss: %f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# print progress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-a5f411651d99>\u001b[0m in \u001b[0;36mlossFun\u001b[0;34m(inputs, targets, hprev)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mdhraw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdh\u001b[0m \u001b[0;31m# backprop through tanh nonlinearity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mdbh\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdhraw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mdWxh\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdhraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0mdWhh\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdhraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mdhnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWhh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdhraw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)\n",
    "BSD License\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "# data I/O\n",
    "data = open('input.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters---> 43     loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "  \"\"\"\n",
    "  inputs,targets are both list of integers.\n",
    "  hprev is Hx1 array of initial hidden state\n",
    "  returns the loss, gradients on model parameters, and last hidden state\n",
    "  \"\"\"\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  loss = 0\n",
    "  # forward pass\n",
    "  for t in range(len(inputs)):\n",
    "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "    xs[t][inputs[t]] = 1\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "   # print(\"t\",t)\n",
    "   # print(\"shape\",(ps[t]).shape)\n",
    "   # print(\"targets\",targets[t])\n",
    "    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "  # backward pass: compute gradients going backwards\n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "  for t in reversed(range(len(inputs))):\n",
    "    dy = np.copy(ps[t])\n",
    "    dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    dby += dy\n",
    "    dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "    dbh += dhraw\n",
    "    dWxh += np.dot(dhraw, xs[t].T)\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "    dhnext = np.dot(Whh.T, dhraw)\n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "  \"\"\" \n",
    "  sample a sequence of integers from the model \n",
    "  h is memory state, seed_ix is seed letter for first time step\n",
    "  \"\"\"\n",
    "  x = np.zeros((vocab_size, 1))\n",
    "  x[seed_ix] = 1\n",
    "  ixes = []\n",
    "  for t in range(n):\n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    y = np.dot(Why, h) + by\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[ix] = 1\n",
    "    ixes.append(ix)\n",
    "  print(p.shape,vocab_size)\n",
    "  return ixes\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "while True:\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  if p+seq_length+1 >= len(data) or n == 0: \n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "    p = 0 # go from start of data\n",
    "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "  # sample from the model now and then\n",
    "  if n % 100 == 0:\n",
    "    sample_ix = sample(hprev, inputs[0], 200)\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient\n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "  if n % 100 == 0: print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "  \n",
    "  # perform parameter update with Adagrad\n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "    mem += dparam * dparam\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "  p += seq_length # move data pointer\n",
    "  n += 1 # iteration counter \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable        Type        Data/Info\n",
      "-------------------------------------\n",
      "Whh             ndarray     100x100: 10000 elems, type `float64`, 80000 bytes\n",
      "Why             ndarray     46x100: 4600 elems, type `float64`, 36800 bytes\n",
      "Wxh             ndarray     100x46: 4600 elems, type `float64`, 36800 bytes\n",
      "bh              ndarray     100x1: 100 elems, type `float64`, 800 bytes\n",
      "by              ndarray     46x1: 46 elems, type `float64`, 368 bytes\n",
      "char_to_ix      dict        n=46\n",
      "chars           list        n=46\n",
      "dWhh            ndarray     100x100: 10000 elems, type `float64`, 80000 bytes\n",
      "dWhy            ndarray     46x100: 4600 elems, type `float64`, 36800 bytes\n",
      "dWxh            ndarray     100x46: 4600 elems, type `float64`, 36800 bytes\n",
      "data            str         Nominative determinism is<...>es for those professions.\n",
      "data_size       int         1038\n",
      "dbh             ndarray     100x1: 100 elems, type `float64`, 800 bytes\n",
      "dby             ndarray     46x1: 46 elems, type `float64`, 368 bytes\n",
      "dparam          ndarray     46x1: 46 elems, type `float64`, 368 bytes\n",
      "hidden_size     int         100\n",
      "hprev           ndarray     100x1: 100 elems, type `float64`, 800 bytes\n",
      "inputs          list        n=25\n",
      "ix_to_char      dict        n=46\n",
      "learning_rate   float       0.1\n",
      "loss            float64     54.1473756192\n",
      "lossFun         function    <function lossFun at 0x7ff320287f28>\n",
      "mWhh            ndarray     100x100: 10000 elems, type `float64`, 80000 bytes\n",
      "mWhy            ndarray     46x100: 4600 elems, type `float64`, 36800 bytes\n",
      "mWxh            ndarray     100x46: 4600 elems, type `float64`, 36800 bytes\n",
      "mbh             ndarray     100x1: 100 elems, type `float64`, 800 bytes\n",
      "mby             ndarray     46x1: 46 elems, type `float64`, 368 bytes\n",
      "mem             ndarray     46x1: 46 elems, type `float64`, 368 bytes\n",
      "n               int         888\n",
      "np              module      <module 'numpy' from '/ho<...>kages/numpy/__init__.py'>\n",
      "p               int         675\n",
      "param           ndarray     46x1: 46 elems, type `float64`, 368 bytes\n",
      "sample          function    <function sample at 0x7ff32018bc80>\n",
      "sample_ix       list        n=200\n",
      "seq_length      int         25\n",
      "smooth_loss     float64     81.3867699148\n",
      "targets         list        n=25\n",
      "txt             str         9xFpGd a,r hixaerotuggerr<...>pSnedcrtinbtede airsewrre\n",
      "vocab_size      int         46\n"
     ]
    }
   ],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nominative determinism is the hypothesis that people are drawn to professions that fit their name. The term was first used in the magazine New Scientist in 1994, after its humorous Feedback column mentioned a book on polar explorations by Daniel Snowman and an article on urology by researchers named Splatt and Weedon. The hypothesis had been suggested by psychologist Carl Jung, citing as an example Sigmund Freud (German for \"joy\"), who studied pleasure. A few recent empirical studies have indicated that certain professions are disproportionately represented by people with appropriate surnames, though the methods of these studies have been challenged. One explanation for nominative determinism is the theory of implicit egotism, which states that humans have an unconscious preference for things they associate with themselves. An alternative explanation is genetic: an ancestor might have been named Smith or Taylor according to their occupation, and the genes they passed down might correlate to aptitudes for those professions.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "k\n",
    "k\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "34\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 612 characters, 45 unique.\n",
      "----\n",
      " OHnePin \n",
      "----\n",
      "iter 0, loss: 95.166565\n",
      "----\n",
      " chn.Hum \n",
      "----\n",
      "iter 10000, loss: 0.225711\n",
      "----\n",
      " k n dt  \n",
      "----\n",
      "iter 20000, loss: 0.076315\n",
      "----\n",
      " enBr tt \n",
      "----\n",
      "iter 30000, loss: 0.045610\n",
      "----\n",
      " Ientwtä \n",
      "----\n",
      "iter 40000, loss: 0.031964\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)\n",
    "BSD License\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "# data I/O\n",
    "data = open('input_en_de.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters---> 43     loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "  \"\"\"\n",
    "  inputs,targets are both list of integers.\n",
    "  hprev is Hx1 array of initial hidden state\n",
    "  returns the loss, gradients on model parameters, and last hidden state\n",
    "  \"\"\"\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  loss = 0\n",
    "  # forward pass\n",
    "  for t in range(len(inputs)):\n",
    "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "    xs[t][inputs[t]] = 1\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "   # print(\"t\",t)\n",
    "   # print(\"shape\",(ps[t]).shape)\n",
    "   # print(\"targets\",targets[t])\n",
    "    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "  # backward pass: compute gradients going backwards\n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "  for t in reversed(range(len(inputs))):\n",
    "    dy = np.copy(ps[t])\n",
    "    dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    dby += dy\n",
    "    dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "    dbh += dhraw\n",
    "    dWxh += np.dot(dhraw, xs[t].T)\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "    dhnext = np.dot(Whh.T, dhraw)\n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "  \"\"\" \n",
    "  sample a sequence of integers from the model \n",
    "  h is memory state, seed_ix is seed letter for first time step\n",
    "  \"\"\"\n",
    "  test_sentence = \"I study\"\n",
    "  n = len(test_sentence)\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "  ixes = []\n",
    "  for t in range(n):\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[char_to_ix[test_sentence[t]]] = 1\n",
    "        \n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    y = np.dot(Why, h) + by\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "    \n",
    "    \n",
    "    \n",
    "    #x = np.zeros((vocab_size, 1))\n",
    "    #x[ix] = 1\n",
    "    \n",
    "    ixes.append(ix)\n",
    "  return ixes\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "while True:\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  if p+seq_length+1 >= len(data[0:282]) or n == 0: \n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "    p = 0 # go from start of data\n",
    "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_to_ix[ch] for ch in data[p+282:p+seq_length+282]]\n",
    "\n",
    "  # sample from the model now and then\n",
    "  if n % 10000 == 0:\n",
    "    test_sentence = \"I study\"\n",
    "    slen = len(test_sentence)\n",
    "    sample_ix = sample(hprev, inputs[0], slen)\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient\n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "  if n % 10000 == 0: print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "  \n",
    "  # perform parameter update with Adagrad\n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "    mem += dparam * dparam\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "  p += seq_length # move data pointer\n",
    "  n += 1 # iteration counter \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable        Type        Data/Info\n",
      "-------------------------------------\n",
      "Whh             ndarray     100x100: 10000 elems, type `float64`, 80000 bytes\n",
      "Why             ndarray     45x100: 4500 elems, type `float64`, 36000 bytes\n",
      "Wxh             ndarray     100x45: 4500 elems, type `float64`, 36000 bytes\n",
      "bh              ndarray     100x1: 100 elems, type `float64`, 800 bytes\n",
      "by              ndarray     45x1: 45 elems, type `float64`, 360 bytes\n",
      "char_to_ix      dict        n=45\n",
      "chars           list        n=45\n",
      "dWhh            ndarray     100x100: 10000 elems, type `float64`, 80000 bytes\n",
      "dWhy            ndarray     45x100: 4500 elems, type `float64`, 36000 bytes\n",
      "dWxh            ndarray     100x45: 4500 elems, type `float64`, 36000 bytes\n",
      "data            str         I took the bus back.I'm n<...>s war wohl unvermeidlich.\n",
      "data_size       int         612\n",
      "dbh             ndarray     100x1: 100 elems, type `float64`, 800 bytes\n",
      "dby             ndarray     45x1: 45 elems, type `float64`, 360 bytes\n",
      "dparam          ndarray     45x1: 45 elems, type `float64`, 360 bytes\n",
      "hidden_size     int         100\n",
      "hprev           ndarray     100x1: 100 elems, type `float64`, 800 bytes\n",
      "inputs          list        n=25\n",
      "ix_to_char      dict        n=45\n",
      "learning_rate   float       0.1\n",
      "loss            float64     39.6915738316\n",
      "lossFun         function    <function lossFun at 0x7f386c1509d8>\n",
      "mWhh            ndarray     100x100: 10000 elems, type `float64`, 80000 bytes\n",
      "mWhy            ndarray     45x100: 4500 elems, type `float64`, 36000 bytes\n",
      "mWxh            ndarray     100x45: 4500 elems, type `float64`, 36000 bytes\n",
      "mbh             ndarray     100x1: 100 elems, type `float64`, 800 bytes\n",
      "mby             ndarray     45x1: 45 elems, type `float64`, 360 bytes\n",
      "mem             ndarray     45x1: 45 elems, type `float64`, 360 bytes\n",
      "n               int         1473\n",
      "np              module      <module 'numpy' from '/ho<...>kages/numpy/__init__.py'>\n",
      "p               int         250\n",
      "param           ndarray     45x1: 45 elems, type `float64`, 360 bytes\n",
      "sample          function    <function sample at 0x7f386c150620>\n",
      "sample_ix       list        n=200\n",
      "seq_length      int         25\n",
      "smooth_loss     float64     66.4734983815\n",
      "targets         list        n=25\n",
      "txt             str         eeeeeeeeeeeeeeeeeeeeeeeee<...>eeeeeeeeeeeeeeeeeeeeeeeee\n",
      "vocab_size      int         45\n"
     ]
    }
   ],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 60957 characters, 82 unique.\n",
      "----\n",
      " -50n0'hK'a50'pMK'a0noDLwüm \n",
      "----\n",
      "iter 0, loss: 110.167982\n",
      "----\n",
      " enaseeenaseeenaseeenaseeen \n",
      "----\n",
      "iter 100, loss: 109.732456\n",
      "----\n",
      "  a   e a   e a   e a   e a \n",
      "----\n",
      "iter 200, loss: 107.294718\n",
      "----\n",
      "    aen    en    en   aen   \n",
      "----\n",
      "iter 300, loss: 105.122088\n",
      "----\n",
      " een   een   een   een   ee \n",
      "----\n",
      "iter 400, loss: 102.989262\n",
      "----\n",
      "   ee    ee    ee    ee     \n",
      "----\n",
      "iter 500, loss: 101.173466\n",
      "----\n",
      " er    er    er    er    er \n",
      "----\n",
      "iter 600, loss: 99.344558\n",
      "----\n",
      "  eee   eee   eee   eee   e \n",
      "----\n",
      "iter 700, loss: 97.696556\n",
      "----\n",
      "  ee c  ee c  ee c  ee c  e \n",
      "----\n",
      "iter 800, loss: 96.216638\n",
      "----\n",
      " ch neech neech neech neech \n",
      "----\n",
      "iter 900, loss: 94.860358\n",
      "----\n",
      "  n e i n e i n e i n e i n \n",
      "----\n",
      "iter 1000, loss: 93.600496\n",
      "----\n",
      " eee   eee   eee   eee   ee \n",
      "----\n",
      "iter 1100, loss: 92.387846\n",
      "----\n",
      "         e                  \n",
      "----\n",
      "iter 1200, loss: 91.369046\n",
      "----\n",
      "   eeen  eeen  eeen  eeen   \n",
      "----\n",
      "iter 1300, loss: 90.356119\n",
      "----\n",
      " iee   iee   iee   iee   ie \n",
      "----\n",
      "iter 1400, loss: 89.470082\n",
      "----\n",
      "    eee   eee   eee   eee   \n",
      "----\n",
      "iter 1500, loss: 88.689046\n",
      "----\n",
      "  e e   e e   e e   e e   e \n",
      "----\n",
      "iter 1600, loss: 88.024259\n",
      "----\n",
      " eee   eee   eee   eee   ee \n",
      "----\n",
      "iter 1700, loss: 87.370592\n",
      "----\n",
      " neeeeeneeeeene eeeneeeeene \n",
      "----\n",
      "iter 1800, loss: 86.746883\n",
      "----\n",
      " e   e e   e e   e e   e e  \n",
      "----\n",
      "iter 1900, loss: 86.227397\n",
      "----\n",
      "     ee    ee    ee    ee   \n",
      "----\n",
      "iter 2000, loss: 85.722309\n",
      "----\n",
      "                            \n",
      "----\n",
      "iter 2100, loss: 85.279929\n",
      "----\n",
      "    ei    ei    ei    ei    \n",
      "----\n",
      "iter 2200, loss: 84.802815\n",
      "----\n",
      "   e     e     e     e      \n",
      "----\n",
      "iter 2300, loss: 84.436465\n",
      "----\n",
      "   eee   eee   eee   eee    \n",
      "----\n",
      "iter 2400, loss: 84.085812\n",
      "----\n",
      "  een   een   een   een   e \n",
      "----\n",
      "iter 2500, loss: 83.679372\n",
      "----\n",
      "   ene   ene   ene   ene    \n",
      "----\n",
      "iter 2600, loss: 83.443745\n",
      "----\n",
      " e   eee   eee   eee   eee  \n",
      "----\n",
      "iter 2700, loss: 83.119457\n",
      "----\n",
      "    een   een   een   een   \n",
      "----\n",
      "iter 2800, loss: 82.947431\n",
      "----\n",
      "  eee   eee   eee   eee   e \n",
      "----\n",
      "iter 2900, loss: 82.712420\n",
      "----\n",
      "      e     e   e e     e   \n",
      "----\n",
      "iter 3000, loss: 82.541831\n",
      "----\n",
      "    eei   eei   eei   eei   \n",
      "----\n",
      "iter 3100, loss: 82.362327\n",
      "----\n",
      "     ee    ee    ee    ee   \n",
      "----\n",
      "iter 3200, loss: 82.201866\n",
      "----\n",
      "  nee   eee   eee   eee   e \n",
      "----\n",
      "iter 3300, loss: 81.975245\n",
      "----\n",
      "   eee   eee   eee   eee    \n",
      "----\n",
      "iter 3400, loss: 81.814790\n",
      "----\n",
      "  e     e     e     e     e \n",
      "----\n",
      "iter 3500, loss: 81.743897\n",
      "----\n",
      " ee   eee   eee   eee   eee \n",
      "----\n",
      "iter 3600, loss: 81.551072\n",
      "----\n",
      " e e   e e   e e   e e   e  \n",
      "----\n",
      "iter 3700, loss: 81.509708\n",
      "----\n",
      "   ee    ee    ee    ee     \n",
      "----\n",
      "iter 3800, loss: 81.369422\n",
      "----\n",
      "     ee    ee    ee    ee   \n",
      "----\n",
      "iter 3900, loss: 81.383268\n",
      "----\n",
      "   eee   eee   eee   eee    \n",
      "----\n",
      "iter 4000, loss: 81.265771\n",
      "----\n",
      "  eee   eee   eee   eee   e \n",
      "----\n",
      "iter 4100, loss: 81.192943\n",
      "----\n",
      "   e     e     e     e      \n",
      "----\n",
      "iter 4200, loss: 81.136326\n",
      "----\n",
      " ee    ee    ee   eee    ee \n",
      "----\n",
      "iter 4300, loss: 81.079172\n",
      "----\n",
      "      e     e     e     e   \n",
      "----\n",
      "iter 4400, loss: 81.023279\n",
      "----\n",
      "     ee    ee    ee    ee   \n",
      "----\n",
      "iter 4500, loss: 80.903342\n",
      "----\n",
      "                            \n",
      "----\n",
      "iter 4600, loss: 80.868041\n",
      "----\n",
      "     ee    ee    ee    ee   \n",
      "----\n",
      "iter 4700, loss: 80.775688\n",
      "----\n",
      "                            \n",
      "----\n",
      "iter 4800, loss: 80.714664\n",
      "----\n",
      " e    ee    ee    ee    ee  \n",
      "----\n",
      "iter 4900, loss: 80.688081\n",
      "----\n",
      "  e e   e e   e e   e e   e \n",
      "----\n",
      "iter 5000, loss: 80.699232\n",
      "----\n",
      "    ee    ee    ee    ee    \n",
      "----\n",
      "iter 5100, loss: 80.665051\n",
      "----\n",
      "    eee   eee   eee   eee   \n",
      "----\n",
      "iter 5200, loss: 80.603748\n",
      "----\n",
      " e     e     e     e     e  \n",
      "----\n",
      "iter 5300, loss: 80.608940\n",
      "----\n",
      " eee   eee   eee   eee   ee \n",
      "----\n",
      "iter 5400, loss: 80.584910\n",
      "----\n",
      "    e     e     e     e     \n",
      "----\n",
      "iter 5500, loss: 80.561886\n",
      "----\n",
      "      e e                 e \n",
      "----\n",
      "iter 5600, loss: 80.473766\n",
      "----\n",
      "     ee    ee    ee    ee   \n",
      "----\n",
      "iter 5700, loss: 80.470923\n",
      "----\n",
      "     ee    ee    ee    ee   \n",
      "----\n",
      "iter 5800, loss: 80.435477\n",
      "----\n",
      "    ee    ee    ee    ee    \n",
      "----\n",
      "iter 5900, loss: 80.344091\n",
      "----\n",
      "                            \n",
      "----\n",
      "iter 6000, loss: 80.365190\n",
      "----\n",
      " eee   eee   eee   eee   ee \n",
      "----\n",
      "iter 6100, loss: 80.301509\n",
      "----\n",
      " e   e e   e e   e e   e e  \n",
      "----\n",
      "iter 6200, loss: 80.349409\n",
      "----\n",
      "    e     e     e     e     \n",
      "----\n",
      "iter 6300, loss: 80.324550\n",
      "----\n",
      " ee   eee   eee   eee   eee \n",
      "----\n",
      "iter 6400, loss: 80.337220\n",
      "----\n",
      " ee    ee    ee    ee    ee \n",
      "----\n",
      "iter 6500, loss: 80.330929\n",
      "----\n",
      " ee   eee   eee   eee   eee \n",
      "----\n",
      "iter 6600, loss: 80.319493\n",
      "----\n",
      "    ne    ne    ne    ne    \n",
      "----\n",
      "iter 6700, loss: 80.243482\n",
      "----\n",
      " e   eee   eee   eee   eee  \n",
      "----\n",
      "iter 6800, loss: 80.208084\n",
      "----\n",
      "    e     e     e     e     \n",
      "----\n",
      "iter 6900, loss: 80.234842\n",
      "----\n",
      "  eee   eee   eee   eee   e \n",
      "----\n",
      "iter 7000, loss: 80.152365\n",
      "----\n",
      "                            \n",
      "----\n",
      "iter 7100, loss: 80.190681\n",
      "----\n",
      "                            \n",
      "----\n",
      "iter 7200, loss: 80.137721\n",
      "----\n",
      " e     e     e     e     e  \n",
      "----\n",
      "iter 7300, loss: 80.225237\n",
      "----\n",
      "     ee    ee    ee    ee   \n",
      "----\n",
      "iter 7400, loss: 80.184134\n",
      "----\n",
      "  e e e e e e e e e e e e e \n",
      "----\n",
      "iter 7500, loss: 80.172667\n",
      "----\n",
      "     e     e     e     e    \n",
      "----\n",
      "iter 7600, loss: 80.177188\n",
      "----\n",
      "  eee   eee   eee   eee   e \n",
      "----\n",
      "iter 7700, loss: 80.161737\n",
      "----\n",
      "  e     e     e     e     e \n",
      "----\n",
      "iter 7800, loss: 80.174656\n",
      "----\n",
      " ee    ee    ee    ee    ee \n",
      "----\n",
      "iter 7900, loss: 80.092097\n",
      "----\n",
      " e     e     e     e     e  \n",
      "----\n",
      "iter 8000, loss: 80.107162\n",
      "----\n",
      "   e     e     e     e      \n",
      "----\n",
      "iter 8100, loss: 80.055724\n",
      "----\n",
      " e     e     e     e     e  \n",
      "----\n",
      "iter 8200, loss: 80.032207\n",
      "----\n",
      "  ee    ee    ee    ee    e \n",
      "----\n",
      "iter 8300, loss: 80.042592\n",
      "----\n",
      "    e e   e e   e e   e e   \n",
      "----\n",
      "iter 8400, loss: 80.083415\n",
      "----\n",
      "     ee    ee    ee    ee   \n",
      "----\n",
      "iter 8500, loss: 80.064027\n",
      "----\n",
      " eee   eee   eee   eee   ee \n",
      "----\n",
      "iter 8600, loss: 80.058382\n",
      "----\n",
      "                            \n",
      "----\n",
      "iter 8700, loss: 80.086677\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-dffce6f7f388>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    127\u001b[0m                                 [mWxh, mWhh, mWhy, mbh, mby]):\n\u001b[1;32m    128\u001b[0m     \u001b[0mmem\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdparam\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdparam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0mparam\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdparam\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmem\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-8\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# adagrad update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m   \u001b[0mp\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mseq_length\u001b[0m \u001b[0;31m# move data pointer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)\n",
    "BSD License\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "# data I/O\n",
    "data = open('input_en_de_1000.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters---> 43     loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "#globals\n",
    "en_ch_cnt = 28357\n",
    "test_sentence = \"Where is the bus terminal?\"\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "  \"\"\"\n",
    "  inputs,targets are both list of integers.\n",
    "  hprev is Hx1 array of initial hidden state\n",
    "  returns the loss, gradients on model parameters, and last hidden state\n",
    "  \"\"\"\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  loss = 0\n",
    "  # forward pass\n",
    "  for t in range(len(inputs)):\n",
    "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "    xs[t][inputs[t]] = 1\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "   # print(\"t\",t)\n",
    "   # print(\"shape\",(ps[t]).shape)\n",
    "   # print(\"targets\",targets[t])\n",
    "    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "  # backward pass: compute gradients going backwards\n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "  for t in reversed(range(len(inputs))):\n",
    "    dy = np.copy(ps[t])\n",
    "    dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    dby += dy\n",
    "    dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "    dbh += dhraw\n",
    "    dWxh += np.dot(dhraw, xs[t].T)\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "    dhnext = np.dot(Whh.T, dhraw)\n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "  \"\"\" \n",
    "  sample a sequence of integers from the model \n",
    "  h is memory state, seed_ix is seed letter for first time step\n",
    "  \"\"\"\n",
    "\n",
    "  n = len(test_sentence)\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "  ixes = []\n",
    "  for t in range(n):\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[char_to_ix[test_sentence[t]]] = 1\n",
    "        \n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    y = np.dot(Why, h) + by\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "    #ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "    ix = np.argmax(p.ravel())\n",
    "    \n",
    "    \n",
    "    #x = np.zeros((vocab_size, 1))\n",
    "    #x[ix] = 1\n",
    "    \n",
    "    ixes.append(ix)\n",
    "  return ixes\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "while True:\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  if p+seq_length+1 >= len(data[0:en_ch_cnt]) or n == 0: \n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "    p = 0 # go from start of data\n",
    "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_to_ix[ch] for ch in data[p+en_ch_cnt:p+seq_length+en_ch_cnt]]\n",
    "\n",
    "  # sample from the model now and then\n",
    "  if n % 100 == 0:\n",
    "    slen = len(test_sentence)\n",
    "    sample_ix = sample(hprev, inputs[0], slen)\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient\n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "  if n % 100 == 0: print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "  \n",
    "  # perform parameter update with Adagrad\n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "    mem += dparam * dparam\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "  p += seq_length # move data pointer\n",
    "  n += 1 # iteration counter \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\n",
      ".\n",
      "?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'hello I m saravanan  Who are you ewwe wsw'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replacePunctuation(\"hello I'm saravanan. Who are you?ewwe wsw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
