{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length after processing : 21680\n"
     ]
    }
   ],
   "source": [
    "#DATA PREPROCESSING\n",
    "\n",
    "import json\n",
    "\n",
    "data = json.load(open('en_de_corpus.json', 'r'))\n",
    "\n",
    "#Script to remove punctuation, ensure <= max allowed length + padding\n",
    "\n",
    "import string\n",
    "import random\n",
    "import math\n",
    "\n",
    "#replace punctuation with space, ensure all sentences with  1 space\n",
    "#also all capital to small \n",
    "def replacePunctuation(x):\n",
    "    puncAndNum = string.punctuation+'01233456789'\n",
    "    x = x.lower()\n",
    "    for char in x:\n",
    "        if char in puncAndNum:\n",
    "            #print(char)\n",
    "            x=x.replace(char,' ')\n",
    "    return ' '.join(x.split())\n",
    "\n",
    "#max allowed raw length=5, pad to make it 5, allow only equal pairs\n",
    "tempEn = list(map(replacePunctuation,data['en']))\n",
    "tempDe = list(map(replacePunctuation,data['de']))\n",
    "maxRawLength = 5\n",
    "maxFinalLength = maxRawLength + 2\n",
    "\n",
    "def wordCount(x):\n",
    "    return len(x.split())\n",
    "\n",
    "data_en_processed = list()\n",
    "data_de_processed = list()\n",
    "for i in range(len(tempEn)):\n",
    "    if(wordCount(tempEn[i]) <= maxRawLength and wordCount(tempDe[i]) <= maxRawLength\n",
    "       and wordCount(tempEn[i])==wordCount(tempDe[i])):\n",
    "        #if(i%10==0):\n",
    "        #    print((maxFinalLength -1 - wordCount(tempEn[i])))\n",
    "        data_en_processed.append('< '+tempEn[i]+' >'*(maxFinalLength -1 - wordCount(tempEn[i])))\n",
    "        data_de_processed.append('< '+tempDe[i]+' >'*(maxFinalLength -1 - wordCount(tempDe[i])))\n",
    "\n",
    "print(\"length after processing : \"+str(len(data_en_processed)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< be quiet for a moment > | < sei mal einen augenblick still >\n",
      "Ich bin für einen Augenblick still\n",
      "\n",
      "< tom was promoted to foreman > | < tom wurde zum steiger befördert >\n",
      "<Tom wurde zum Vorarbeiter befördert>\n",
      "\n",
      "< there s a serious problem > | < es besteht ein gravierendes problem >\n",
      "<Da ​​ist ein ernstes Problem>\n",
      "\n",
      "< i majored in psychology > > | < mein hauptfach war psychologie > >\n",
      "Ich habe in Psychologie>>\n",
      "\n",
      "< he s not ready > > | < er ist nicht bereit > >\n",
      "<Er ist nicht bereit>>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#pick 5 random samples and display, compare with google translate\n",
    "import time\n",
    "from mtranslate import translate\n",
    "\n",
    "\n",
    "start = math.ceil(random.random()*1000)\n",
    "for i in range(start,start+5):\n",
    "    print(data_en_processed[i]+\" | \"+data_de_processed[i])\n",
    "    print(translate(data_en_processed[i],'de','en'))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<', 'er', 'unterrichtet', 'arabisch', '>', '>', '>']\n",
      "****************\n",
      "['<', 'i', 'took', 'the', 'bus', 'back', '>']\n",
      "['<', 'ich', 'nahm', 'den', 'bus', 'zurück', '>']\n"
     ]
    }
   ],
   "source": [
    "# Convert to list of lists from list of strings, remove space\n",
    "\n",
    "data_en_processed_tokens = [s.split() for s in data_en_processed]\n",
    "data_de_processed_tokens = [s.split() for s in data_de_processed]\n",
    "print(data_de_processed_tokens[77])\n",
    "print(\"****************\")\n",
    "\n",
    "#Lower the size\n",
    "data_en_processed_tokens = data_en_processed_tokens[:100]\n",
    "data_de_processed_tokens = data_de_processed_tokens[:100]\n",
    "data_full = data_en_processed_tokens + data_de_processed_tokens\n",
    "len(data_full)\n",
    "\n",
    "data_full = sum(data_full, [])\n",
    "len(data_full)\n",
    "\n",
    "print(data_full[0:7])\n",
    "print(data_full[(len(data_full)//2)+0 :(len(data_full)//2)+7])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700\n",
      "700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "501"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Partition back into En and De\n",
    "\n",
    "data_en =  data_full[:(len(data_full)//2)]\n",
    "data_de =  data_full[(len(data_full)//2):]\n",
    "print(len(data_en))\n",
    "print(len(data_de))\n",
    "data_en.index('tea')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<', 'i', 'took', 'the', 'bus', 'back', '>']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_de)\n",
    "data_en[0:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<', 'she', 'was', 'making', 'tea', '>', '>'] ['<', 'sie', 'machte', 'gerade', 'tee', '>', '>']\n"
     ]
    }
   ],
   "source": [
    "tempEnList = list()\n",
    "for i in range(int(len(data_en)/7)):\n",
    "    tempEnList.append(data_en[i*7:(i+1)*7])\n",
    "    \n",
    "#tempEnList.append(['tea'])\n",
    "\n",
    "tempDeList = list()\n",
    "for i in range(int(len(data_de)/7)):\n",
    "    tempDeList.append(data_de[i*7:(i+1)*7])\n",
    "    \n",
    "print(tempEnList[71],tempDeList[71])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable                   Type        Data/Info\n",
      "------------------------------------------------\n",
      "data                       dict        n=2\n",
      "data_de                    list        n=700\n",
      "data_de_processed          list        n=21680\n",
      "data_de_processed_tokens   list        n=100\n",
      "data_en                    list        n=700\n",
      "data_en_processed          list        n=21680\n",
      "data_en_processed_tokens   list        n=100\n",
      "data_full                  list        n=1400\n",
      "i                          int         99\n",
      "json                       module      <module 'json' from '/usr<...>hon3.5/json/__init__.py'>\n",
      "math                       module      <module 'math' (built-in)>\n",
      "maxFinalLength             int         7\n",
      "maxRawLength               int         5\n",
      "random                     module      <module 'random' from '/h<...>lib/python3.5/random.py'>\n",
      "replacePunctuation         function    <function replacePunctuation at 0x7fa26fc3b268>\n",
      "start                      int         955\n",
      "string                     module      <module 'string' from '/u<...>lib/python3.5/string.py'>\n",
      "tempDe                     list        n=132173\n",
      "tempDeList                 list        n=100\n",
      "tempEn                     list        n=132173\n",
      "tempEnList                 list        n=100\n",
      "time                       module      <module 'time' (built-in)>\n",
      "translate                  function    <function translate at 0x7fa26ead66a8>\n",
      "wordCount                  function    <function wordCount at 0x7fa26ee94f28>\n"
     ]
    }
   ],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<', 'she', 'was', 'making', 'tea', '>', '>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Slow version of gensim.models.doc2vec is being used\n",
      "2017-04-20 18:47:31,747 : WARNING : Slow version of gensim.models.word2vec is being used\n",
      "2017-04-20 18:47:31,748 : INFO : collecting all words and their counts\n",
      "2017-04-20 18:47:31,749 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-04-20 18:47:31,750 : INFO : collected 247 word types from a corpus of 700 raw words and 100 sentences\n",
      "2017-04-20 18:47:31,752 : INFO : Loading a fresh vocabulary\n",
      "2017-04-20 18:47:31,754 : INFO : min_count=1 retains 247 unique words (100% of original 247, drops 0)\n",
      "2017-04-20 18:47:31,755 : INFO : min_count=1 leaves 700 word corpus (100% of original 700, drops 0)\n",
      "2017-04-20 18:47:31,760 : INFO : deleting the raw counts dictionary of 247 items\n",
      "2017-04-20 18:47:31,763 : INFO : sample=0.001 downsamples 52 most-common words\n",
      "2017-04-20 18:47:31,765 : INFO : downsampling leaves estimated 334 word corpus (47.8% of prior 700)\n",
      "2017-04-20 18:47:31,768 : INFO : estimated required memory for 247 words and 100 dimensions: 321100 bytes\n",
      "2017-04-20 18:47:31,772 : INFO : resetting layer weights\n",
      "/home/shinchan/MLProject/venv/lib/python3.5/site-packages/gensim/models/word2vec.py:772: UserWarning: C extension not loaded for Word2Vec, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  warnings.warn(\"C extension not loaded for Word2Vec, training will be slow. \"\n",
      "2017-04-20 18:47:31,780 : INFO : training model with 4 workers on 247 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-04-20 18:47:31,784 : INFO : expecting 100 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-04-20 18:47:31,796 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-04-20 18:47:31,801 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-04-20 18:47:31,803 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-04-20 18:47:32,108 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-04-20 18:47:32,109 : INFO : training on 3500 raw words (1697 effective words) took 0.3s, 5338 effective words/s\n",
      "2017-04-20 18:47:32,110 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-04-20 18:47:32,111 : INFO : saving Word2Vec object under english_word2vec_new, separately None\n",
      "2017-04-20 18:47:32,112 : INFO : not storing attribute syn0norm\n",
      "2017-04-20 18:47:32,113 : INFO : not storing attribute cum_table\n",
      "2017-04-20 18:47:32,122 : INFO : saved english_word2vec_new\n",
      "2017-04-20 18:47:32,124 : WARNING : Slow version of gensim.models.word2vec is being used\n",
      "2017-04-20 18:47:32,125 : INFO : collecting all words and their counts\n",
      "2017-04-20 18:47:32,126 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-04-20 18:47:32,128 : INFO : collected 267 word types from a corpus of 700 raw words and 100 sentences\n",
      "2017-04-20 18:47:32,129 : INFO : Loading a fresh vocabulary\n",
      "2017-04-20 18:47:32,131 : INFO : min_count=1 retains 267 unique words (100% of original 267, drops 0)\n",
      "2017-04-20 18:47:32,132 : INFO : min_count=1 leaves 700 word corpus (100% of original 700, drops 0)\n",
      "2017-04-20 18:47:32,135 : INFO : deleting the raw counts dictionary of 267 items\n",
      "2017-04-20 18:47:32,136 : INFO : sample=0.001 downsamples 52 most-common words\n",
      "2017-04-20 18:47:32,137 : INFO : downsampling leaves estimated 350 word corpus (50.1% of prior 700)\n",
      "2017-04-20 18:47:32,138 : INFO : estimated required memory for 267 words and 100 dimensions: 347100 bytes\n",
      "2017-04-20 18:47:32,140 : INFO : resetting layer weights\n",
      "2017-04-20 18:47:32,147 : INFO : training model with 4 workers on 267 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-04-20 18:47:32,148 : INFO : expecting 100 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-04-20 18:47:32,154 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-04-20 18:47:32,157 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-04-20 18:47:32,158 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-04-20 18:47:32,471 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-04-20 18:47:32,472 : INFO : training on 3500 raw words (1758 effective words) took 0.3s, 5519 effective words/s\n",
      "2017-04-20 18:47:32,473 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-04-20 18:47:32,475 : INFO : saving Word2Vec object under german_word2vec_new, separately None\n",
      "2017-04-20 18:47:32,476 : INFO : not storing attribute syn0norm\n",
      "2017-04-20 18:47:32,478 : INFO : not storing attribute cum_table\n",
      "2017-04-20 18:47:32,486 : INFO : saved german_word2vec_new\n"
     ]
    }
   ],
   "source": [
    "print(tempEnList[71])\n",
    "\n",
    "import gensim, logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "\n",
    "modelEn = gensim.models.Word2Vec(tempEnList,size=100,workers=4,min_count=1)\n",
    "modelEn.save('english_word2vec_new')\n",
    "\n",
    "modelDe = gensim.models.Word2Vec(tempDeList,size=100,workers=4,min_count=1)\n",
    "modelDe.save('german_word2vec_new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -4.73628519e-03,  -4.73593874e-03,   7.35284062e-04,\n",
       "         7.66898505e-04,  -1.28498510e-03,   2.13941606e-03,\n",
       "        -4.58152359e-03,   1.70954247e-03,   1.38603931e-03,\n",
       "         3.60729755e-03,  -4.26432071e-03,   6.87931024e-04,\n",
       "         3.07859760e-03,  -4.23170300e-03,  -1.36448292e-03,\n",
       "         1.04446197e-03,  -3.20789032e-03,  -2.51956470e-03,\n",
       "        -2.77759484e-03,  -1.66425923e-06,  -2.24990561e-03,\n",
       "        -3.00988834e-03,  -1.97166315e-04,  -2.90036923e-03,\n",
       "         1.68665277e-03,  -3.56142945e-03,  -3.81568680e-03,\n",
       "         3.11863405e-04,   3.31604038e-03,  -9.84616112e-04,\n",
       "         2.00261944e-03,  -1.71005900e-03,  -1.13032048e-03,\n",
       "        -8.25506810e-04,   3.23621114e-03,  -4.48995829e-03,\n",
       "        -4.54470562e-03,  -3.40136141e-03,   4.76405164e-03,\n",
       "        -2.40558106e-03,  -1.42524403e-03,  -4.96013975e-03,\n",
       "        -2.17804196e-03,   3.04285972e-03,   2.52219895e-03,\n",
       "        -2.67080939e-03,   6.66689244e-04,  -5.62526344e-04,\n",
       "        -1.45264866e-03,   1.02989958e-03,   6.67355373e-04,\n",
       "         1.74060394e-03,  -8.50238430e-04,   2.93994788e-03,\n",
       "         2.60886620e-04,  -3.51921120e-03,   4.70616994e-03,\n",
       "        -3.96171585e-03,  -2.74183019e-03,  -1.11152034e-03,\n",
       "         1.59198549e-04,  -4.56743035e-03,  -4.23490623e-04,\n",
       "         4.81700571e-03,   5.02822781e-03,  -1.05010904e-03,\n",
       "        -4.44102706e-03,  -4.33626678e-03,  -9.72020556e-04,\n",
       "        -1.56224065e-04,   2.49405135e-03,   3.14663048e-03,\n",
       "        -4.58861934e-03,  -8.17691383e-04,   1.32936833e-03,\n",
       "        -2.70663109e-03,  -2.68975098e-04,  -4.81527299e-03,\n",
       "         4.25470440e-04,  -1.67439214e-03,   7.66681856e-04,\n",
       "        -7.88365665e-04,  -4.42222401e-04,  -1.80083769e-03,\n",
       "         3.48048354e-03,   3.13650211e-03,   7.82325398e-04,\n",
       "         3.58287175e-03,  -4.17588605e-03,  -2.27898522e-03,\n",
       "         1.16531970e-03,   2.68390728e-03,  -1.89875346e-03,\n",
       "         1.70759764e-03,  -1.93125219e-03,  -2.22456641e-03,\n",
       "         4.61149914e-03,  -2.85170134e-03,  -4.63470258e-03,\n",
       "         4.10686992e-03], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelEn['tea']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data_en = [ 'i' if (e == '<' or e == '>') else e for e in data_en ]\n",
    "#data_de = [ 'ich' if (e == '<' or e == '>') else e for e in data_de ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data_en = ['i', 'i', 'took', 'the', 'bus', 'back', 'i', 'i', 'without', 'air', 'we', 'would', 'die', 'i', 'i', 'i', 'study', 'chinese', 'in', 'beijing', 'i']\n",
    "#data_de = ['ich', 'ich', 'nahm', 'den', 'bus', 'zurück', 'ich', 'ich', 'ohne', 'luft', 'würden', 'wir', 'sterben', 'ich', 'ich', 'ich', 'lerne', 'in', 'peking', 'chinesisch', 'ich']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(data_en[21:], data_de[21:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 700 en_chars, 700 de_chars, 100 vec size.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-20 18:47:43,376 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "Input Sentence : < i took the bus back >\n",
      "Expected Translation by Google : <Ich nahm den Bus zurück>\n",
      "word 1 : herr 0.27  ,  kinder 0.25  ,  ohne 0.24  ,  \n",
      "word 2 : gestern 0.30  ,  war 0.26  ,  trägt 0.24  ,  \n",
      "word 3 : alleine 0.36  ,  botschaft 0.21  ,  lernen 0.19  ,  \n",
      "word 4 : tot 0.26  ,  höre 0.24  ,  nach 0.19  ,  \n",
      "word 5 : löwen 0.27  ,  dich 0.22  ,  bei 0.22  ,  \n",
      "word 6 : kein 0.26  ,  mach 0.21  ,  könnten 0.20  ,  \n",
      "word 7 : am 0.25  ,  gekleidet 0.23  ,  ohne 0.22  ,  \n",
      "---------------\n",
      "iter 0, loss: 32.203955\n",
      "iter 1000, loss: 11.841259\n",
      "iter 2000, loss: 4.353987\n",
      "iter 3000, loss: 1.600950\n",
      "iter 4000, loss: 0.588671\n",
      "iter 5000, loss: 0.216460\n",
      "iter 6000, loss: 0.079599\n",
      "iter 7000, loss: 0.029276\n",
      "iter 8000, loss: 0.010772\n",
      "iter 9000, loss: 0.003968\n",
      "---------------\n",
      "Input Sentence : < i took the bus back >\n",
      "Expected Translation by Google : <Ich nahm den Bus zurück>\n",
      "word 1 : < 0.99  ,  > 0.50  ,  kinder 0.35  ,  \n",
      "word 2 : ich 0.99  ,  unterrichtet 0.38  ,  > 0.30  ,  \n",
      "word 3 : nahm 0.56  ,  zweierreihen 0.37  ,  begann 0.32  ,  \n",
      "word 4 : die 0.63  ,  den 0.56  ,  für 0.33  ,  \n",
      "word 5 : bus 0.69  ,  salz 0.35  ,  wo 0.33  ,  \n",
      "word 6 : zurück 0.62  ,  ich 0.37  ,  geschlossen 0.36  ,  \n",
      "word 7 : > 1.00  ,  < 0.47  ,  unterrichtet 0.40  ,  \n",
      "---------------\n",
      "iter 10000, loss: 0.001466\n",
      "iter 11000, loss: 0.000546\n",
      "iter 12000, loss: 0.000207\n",
      "iter 13000, loss: 0.000083\n",
      "iter 14000, loss: 0.000037\n",
      "iter 15000, loss: 0.000020\n",
      "iter 16000, loss: 0.000014\n",
      "iter 17000, loss: 0.000011\n",
      "iter 18000, loss: 0.000010\n",
      "iter 19000, loss: 0.000010\n",
      "---------------\n",
      "Input Sentence : < i took the bus back >\n",
      "Expected Translation by Google : <Ich nahm den Bus zurück>\n",
      "word 1 : < 0.98  ,  > 0.46  ,  kinder 0.35  ,  \n",
      "word 2 : ich 0.98  ,  unterrichtet 0.36  ,  gefällt 0.31  ,  \n",
      "word 3 : nahm 0.58  ,  begann 0.35  ,  zweierreihen 0.34  ,  \n",
      "word 4 : den 0.64  ,  die 0.46  ,  einen 0.36  ,  \n",
      "word 5 : bus 0.79  ,  wo 0.33  ,  salz 0.32  ,  \n",
      "word 6 : zurück 0.75  ,  ohne 0.31  ,  roman 0.31  ,  \n",
      "word 7 : > 0.99  ,  < 0.48  ,  war 0.39  ,  \n",
      "---------------\n",
      "iter 20000, loss: 0.000010\n",
      "iter 21000, loss: 0.000009\n",
      "iter 22000, loss: 0.000009\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from mtranslate import translate\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "# data I/O\n",
    "#data = open('input_en_de_1000.txt', 'r').read() # should be simple plain text file\n",
    "#data = data_full\n",
    "chars_en = list(set(data_en))\n",
    "chars_de = list(set(data_de))\n",
    "vocab_size = 100\n",
    "\n",
    "print('data has %d en_chars, %d de_chars, %d vec size.' % (len(data_en), len(data_de), vocab_size))\n",
    "\n",
    "char_en_to_vec = { ch:modelEn[ch] for ch in chars_en }\n",
    "char_de_to_vec = { ch:modelDe[ch] for ch in chars_de }\n",
    "\n",
    "#vec_en_to_char = {  }\n",
    "#vec_de_to_char = {  }\n",
    "\n",
    "#char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "#ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters---> 43     loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "\n",
    "hidden_size = 150 # size of hidden layer of neurons\n",
    "seq_length = 7 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-2 #1e-1\n",
    "\n",
    "#globals\n",
    "#en_ch_cnt = (len(data_full)//2) #len(data_full)//2\n",
    "test_sentence = ['<', 'i', 'took', 'the', 'bus', 'back', '>']\n",
    "#test_sentence = [ '<', 'the', 'boy', 'is', 'wearing', 'glasses', '>']\n",
    "#test_sentence = [ '<', 'i', 'ate', 'a', 'delicious', 'apple', '>']\n",
    "test_sentence_de= translate(' '.join(test_sentence),'de', 'en').split()\n",
    "\n",
    "test_sentence_vec = [char_en_to_vec[ch] for ch in test_sentence]\n",
    "#'<', 'ich', 'nahm', 'den', 'bus', 'zurück', '>'\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "  \"\"\"\n",
    "  inputs,targets are both list of integers.\n",
    "  hprev is Hx1 array of initial hidden state\n",
    "  returns the loss, gradients on model parameters, and last hidden state\n",
    "  \"\"\"\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  loss = 0\n",
    "  # forward pass\n",
    "  for t in range(len(inputs)):\n",
    "    #xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "    #xs[t][inputs[t]] = 1\n",
    "    xs[t] = np.reshape(np.array(inputs[t]), (vocab_size,1) )\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "    #ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "    \n",
    "\n",
    "    #loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "    \n",
    "    loss += 0.5*np.square(np.subtract(np.reshape(np.array(targets[t]),(vocab_size,1)) , ys[t]))\n",
    "    \n",
    "  # backward pass: compute gradients going backwards\n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "  for t in reversed(range(len(inputs))):\n",
    "    #dy = np.copy(ps[t])\n",
    "    #dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "    dy = -1*np.subtract(np.reshape(np.array(targets[t]),(vocab_size,1)) , ys[t])\n",
    "    \n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    dby += dy\n",
    "    dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "    dbh += dhraw\n",
    "    dWxh += np.dot(dhraw, xs[t].T)\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "    dhnext = np.dot(Whh.T, dhraw)\n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "\n",
    "\n",
    "def sample(h, n):\n",
    "\n",
    "  n = len(test_sentence_vec)\n",
    "\n",
    "  translated_vecs = []\n",
    "  for t in range(n):\n",
    "    #x = np.zeros((vocab_size, 1))\n",
    "    #x[char_to_ix[test_sentence[t]]] = 1\n",
    "    x = np.reshape(np.array(test_sentence_vec[t]), (vocab_size,1) )\n",
    "        \n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    y = np.dot(Why, h) + by   \n",
    "    translated_vecs.append(y)\n",
    "  return translated_vecs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "while True:\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  if p+seq_length+1 >= len(data_en) or n == 0: \n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "    p = 0 # go from start of data\n",
    "  inputs = [char_en_to_vec[ch] for ch in data_en[p:p+seq_length]]\n",
    "  #targets = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_de_to_vec[ch] for ch in data_de[p:p+seq_length]]\n",
    "\n",
    "  # sample from the model now and then\n",
    "  if n % 10000 == 0:\n",
    "    slen = len(test_sentence_vec)\n",
    "    sample_vecs = sample(hprev, slen)\n",
    "    #ans = [modelDe.similar_by_vector(v.flatten(), topn=3, restrict_vocab=None)[0][0] for v in sample_vecs]\n",
    "    '''for a in ans:\n",
    "        pprint(a[0])\n",
    "        pprint(\"***\")\n",
    "    '''\n",
    "    \n",
    "    print('---------------')\n",
    "    print('Input Sentence : '+' '.join(test_sentence))\n",
    "    print('Expected Translation by Google : '+' '.join(test_sentence_de))\n",
    "    \n",
    "    i = 0\n",
    "    for v in sample_vecs:\n",
    "        top_n = modelDe.similar_by_vector(v.flatten(), topn=3, restrict_vocab=None)\n",
    "        print(\"word \"+str(i+1)+\" : \",end='')\n",
    "        for j in range(3):\n",
    "            print(top_n[j][0],\"%.2f\" %top_n[j][1] ,\" , \" ,end=' ')\n",
    "        i=i+1\n",
    "        print()\n",
    "    #print(ans)\n",
    "    #print(translate(' '.join(ans),'en', 'de'))\n",
    "    print('---------------')\n",
    "    \n",
    "\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient\n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "  if n % 1000 == 0: print('iter %d, loss: %f' % (n, np.mean(smooth_loss)) ) # print progress\n",
    "  \n",
    "  # perform parameter update with Adagrad\n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "    mem += dparam * dparam\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "  p += seq_length # move data pointer\n",
    "  n += 1 # iteration counter \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('wunder', 0.9999999403953552)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelDe.similar_by_vector(yy, topn=1, restrict_vocab=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['geht',\n",
       " 'schwer',\n",
       " 'reich',\n",
       " '>',\n",
       " 'haare',\n",
       " 'lied',\n",
       " 'wir',\n",
       " 'das',\n",
       " 'hat',\n",
       " 'ich',\n",
       " 'war',\n",
       " 'salz',\n",
       " 'für',\n",
       " 'zu',\n",
       " 'geduld',\n",
       " 'nahm',\n",
       " 'kennen',\n",
       " 'ins',\n",
       " 'bus',\n",
       " 'mir',\n",
       " 'der',\n",
       " '<',\n",
       " 'blonde',\n",
       " 'sterben',\n",
       " 'peking',\n",
       " 'glauben',\n",
       " 'ohne',\n",
       " 'früh',\n",
       " 'er',\n",
       " 'trägt',\n",
       " 'eine',\n",
       " 'tom',\n",
       " 'junge',\n",
       " 'würden',\n",
       " 'den',\n",
       " 'lerne',\n",
       " 'danke',\n",
       " 'bett',\n",
       " 'bitte',\n",
       " 'brille',\n",
       " 'chinesisch',\n",
       " 'zurück',\n",
       " 'dieses',\n",
       " 'in',\n",
       " 'luft',\n",
       " 'ihre']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars_de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_vecs[0].flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_sentence = ['i', 'i', 'took', 'the', 'bus', 'back', 'i']\n",
    "test_sentence_vec = [char_en_to_vec[ch] for ch in test_sentence]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Variable                   Type        Data/Info\n",
    "------------------------------------------------\n",
    "Whh                        ndarray     50x50: 2500 elems, type `float64`, 20000 bytes\n",
    "Why                        ndarray     497x50: 24850 elems, type `float64`, 198800 bytes (194.140625 kb)\n",
    "Wxh                        ndarray     50x497: 24850 elems, type `float64`, 198800 bytes (194.140625 kb)\n",
    "bh                         ndarray     50x1: 50 elems, type `float64`, 400 bytes\n",
    "by                         ndarray     497x1: 497 elems, type `float64`, 3976 bytes\n",
    "char_to_ix                 dict        n=497\n",
    "chars                      list        n=497\n",
    "dWhh                       ndarray     50x50: 2500 elems, type `float64`, 20000 bytes\n",
    "dWhy                       ndarray     497x50: 24850 elems, type `float64`, 198800 bytes (194.140625 kb)\n",
    "dWxh                       ndarray     50x497: 24850 elems, type `float64`, 198800 bytes (194.140625 kb)\n",
    "data                       list        n=1400\n",
    "data_de_processed          list        n=21680\n",
    "data_de_processed_tokens   list        n=100\n",
    "data_en_processed          list        n=21680\n",
    "data_en_processed_tokens   list        n=100\n",
    "data_full                  list        n=1400\n",
    "data_size                  int         1400\n",
    "dbh                        ndarray     50x1: 50 elems, type `float64`, 400 bytes\n",
    "dby                        ndarray     497x1: 497 elems, type `float64`, 3976 bytes\n",
    "dparam                     ndarray     497x1: 497 elems, type `float64`, 3976 bytes\n",
    "en_ch_cnt                  int         700\n",
    "gensim                     module      <module 'gensim' from '/h<...>ages/gensim/__init__.py'>\n",
    "hidden_size                int         50\n",
    "hprev                      ndarray     50x1: 50 elems, type `float64`, 400 bytes\n",
    "i                          int         259\n",
    "inputs                     list        n=7\n",
    "ix_to_char                 dict        n=497\n",
    "json                       module      <module 'json' from '/hom<...>hon3.5/json/__init__.py'>\n",
    "learning_rate              float       0.01\n",
    "loss                       float64     23.793629885\n",
    "lossFun                    function    <function lossFun at 0x7f61930aeb70>\n",
    "mWhh                       ndarray     50x50: 2500 elems, type `float64`, 20000 bytes\n",
    "mWhy                       ndarray     497x50: 24850 elems, type `float64`, 198800 bytes (194.140625 kb)\n",
    "mWxh                       ndarray     50x497: 24850 elems, type `float64`, 198800 bytes (194.140625 kb)\n",
    "math                       module      <module 'math' from '/hom<...>3.5/lib-dynload/math.so'>\n",
    "maxFinalLength             int         7\n",
    "maxRawLength               int         5\n",
    "mbh                        ndarray     50x1: 50 elems, type `float64`, 400 bytes\n",
    "mby                        ndarray     497x1: 497 elems, type `float64`, 3976 bytes\n",
    "mem                        ndarray     497x1: 497 elems, type `float64`, 3976 bytes\n",
    "modelEn                    Word2Vec    Word2Vec(vocab=6972, size=100, alpha=0.025)\n",
    "modelGe                    Word2Vec    Word2Vec(vocab=9279, size=100, alpha=0.025)\n",
    "n                          int         2459\n",
    "np                         module      <module 'numpy' from '/ho<...>kages/numpy/__init__.py'>\n",
    "p                          int         581\n",
    "param                      ndarray     497x1: 497 elems, type `float64`, 3976 bytes\n",
    "random                     module      <module 'random' from '/h<...>lib/python3.5/random.py'>\n",
    "replacePunctuation         function    <function replacePunctuation at 0x7f61b38527b8>\n",
    "sample                     function    <function sample at 0x7f61c4e2b0d0>\n",
    "sample_ix                  list        n=7\n",
    "seq_length                 int         7\n",
    "slen                       int         7\n",
    "smooth_loss                float64     21.9345595918\n",
    "start                      int         255\n",
    "string                     module      <module 'string' from '/h<...>lib/python3.5/string.py'>\n",
    "targets                    list        n=7\n",
    "tempDe                     list        n=132173\n",
    "tempEn                     list        n=132173\n",
    "test_sentence              list        n=7\n",
    "time                       module      <module 'time' (built-in)>\n",
    "translate                  function    <function translate at 0x7f61b27228c8>\n",
    "txt                        str         < ich ist ist > > >\n",
    "vocab_size                 int         497\n",
    "wordCount                  function    <function wordCount at 0x7f61b2b3d510>\n",
    "x                          ndarray     100: 100 elems, type `float32`, 400 bytes\n",
    "y                          ndarray     100: 100 elems, type `float32`, 400 bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
