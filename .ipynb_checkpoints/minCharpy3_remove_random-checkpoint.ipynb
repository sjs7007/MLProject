{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1038 characters, 46 unique.\n",
      "----\n",
      " rzsgc\"mzaG4jW:(tJsF1TFfnigvtA)gJhjW1SvfD.:AuekGdf)C(Fjuj,gij:pcbuWzOdm\"S1vopvfxCvJuDd\"p (krkd\"cwbkC1u(9jef.ycTxGa14j9FO:ur,dF9sihov:cACObh 4kgWDsDw9x4ybh fJw .emsT9s1mld\"Wocn..x,ss4WNJdTjSznrex1icmToN \n",
      "----\n",
      "iter 0, loss: 95.716032\n",
      "----\n",
      " sre)trae \"aoaalo ami )gs xhi dh, nDio\"tuTaey9a)nejpnnehl iSeense  to ce  iu4 ljSie)vC eg jenisnoifjo ateomani\"sf ai  ien wm1r )e am pj,e t e smyy\"zu9ihh eou fhot\"So  stroo  neCDswS hnNr\"jz arnh\"wg tco \n",
      "----\n",
      "iter 100, loss: 96.443984\n",
      "----\n",
      " nhoogynvpiterdiSohusxudnA iodtem  diw ueFaae  tynten wvnna,oopissams aOt enen e pluk inlhlsu  ee anieri hlsnprimea  aawohgechsienseee aopaa4 enonay , ifcsiyoha dgnsgae 49mxFat g ang uA.li seioehdclean \n",
      "----\n",
      "iter 200, loss: 94.912036\n",
      "----\n",
      " n xppiar vxp9rd maunpnact lp uane bs ecsedOpmiwattoyp Nr  mye4fieNAnstist Theeiop Wic dne kgn oTe af lvgp fsse,miafse so. yfe bx lpbt vmt rep ws sfseTba oTdeu ed yptvsa l1aratgm rhn A tiant dmseawhexe \n",
      "----\n",
      "iter 300, loss: 93.205453\n",
      "----\n",
      " o  noatiujeernosasatadlnm  ucui4dnS c s cnbepn tcfeiiw m refod hsczstenactTsstrsomrhcoleintnt (e1 dis  nWerscltaOmrnmotetwt nutsogmaesths taim acteec9enedlsn anJ   me sci h  ritpfe pesaz ncmliceaautta \n",
      "----\n",
      "iter 400, loss: 91.401670\n",
      "----\n",
      " ae4enehe ene s fopaouaT 9tiNman ie timnhabcveth (nnros nme d jufena)bmiles veh4cpesf raDwhhhief theeyuaidceW n nm9pwt r iy nuush9boeteTlame,: stissed dgecetiict94ttld T9m,ow nG a4 emid eseoWakFnrhts k \n",
      "----\n",
      "iter 500, loss: 89.424526\n",
      "----\n",
      " arete  ioxeien anm bieeweteradeit n hesanorooon petplest  f tadsaodpiols dinDxh ncorh igvf t Nhttoce4 ravs usthed ayfarr, e oeaabor npwlaens afay ha(omemes iampe trypel prhpoutmme apn  on mfathei nhaa \n",
      "----\n",
      "iter 600, loss: 87.428725\n",
      "----\n",
      " d tTs lnitorameeonheCaaancdief xemme epid th d tich st pt 1, pesmtrs Fhyen t geticefptsi\"nht bp mplhunans rh hltti awaes eor t mesl ne awaris Tad rys biediw Sh worc as yntis ruimp uitat.cre t anye nh  \n",
      "----\n",
      "iter 700, loss: 85.271896\n",
      "----\n",
      " 9xFpGd a,r hixaerotuggerrelaor ces ach ce feNe oAnfcebthe k f rey fhenm 4n ees pbedhey\"i z edethgtthorisertils tare evseons bywh rehet cFrt rt ecavhgsthos ij mt9verd vgen tlt pSnedcrtinbtede airsewrre \n",
      "----\n",
      "iter 800, loss: 83.223619\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-78763fcb83c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m   \u001b[0;31m# forward seq_length characters through the net and fetch gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m   \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWxh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlossFun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhprev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m   \u001b[0msmooth_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmooth_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.999\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'iter %d, loss: %f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# print progress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-78763fcb83c6>\u001b[0m in \u001b[0;36mlossFun\u001b[0;34m(inputs, targets, hprev)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# encode in 1-of-k representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWxh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWhh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# hidden state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mby\u001b[0m \u001b[0;31m# unnormalized log probabilities for next chars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# probabilities for next chars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)\n",
    "BSD License\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "# data I/O\n",
    "data = open('input.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters---> 43     loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "  \"\"\"\n",
    "  inputs,targets are both list of integers.\n",
    "  hprev is Hx1 array of initial hidden state\n",
    "  returns the loss, gradients on model parameters, and last hidden state\n",
    "  \"\"\"\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  loss = 0\n",
    "  # forward pass\n",
    "  for t in range(len(inputs)):\n",
    "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "    xs[t][inputs[t]] = 1\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "   # print(\"t\",t)\n",
    "   # print(\"shape\",(ps[t]).shape)\n",
    "   # print(\"targets\",targets[t])\n",
    "    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "  # backward pass: compute gradients going backwards\n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "  for t in reversed(range(len(inputs))):\n",
    "    dy = np.copy(ps[t])\n",
    "    dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    dby += dy\n",
    "    dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "    dbh += dhraw\n",
    "    dWxh += np.dot(dhraw, xs[t].T)\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "    dhnext = np.dot(Whh.T, dhraw)\n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "  \"\"\" \n",
    "  sample a sequence of integers from the model \n",
    "  h is memory state, seed_ix is seed letter for first time step\n",
    "  \"\"\"\n",
    "  x = np.zeros((vocab_size, 1))\n",
    "  x[seed_ix] = 1\n",
    "  ixes = []\n",
    "  for t in range(n):\n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    y = np.dot(Why, h) + by\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[ix] = 1\n",
    "    ixes.append(ix)\n",
    "  return ixes\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "while True:\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  if p+seq_length+1 >= len(data) or n == 0: \n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "    p = 0 # go from start of data\n",
    "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "  # sample from the model now and then\n",
    "  if n % 100 == 0:\n",
    "    sample_ix = sample(hprev, inputs[0], 200)\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient\n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "  if n % 100 == 0: print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "  \n",
    "  # perform parameter update with Adagrad\n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "    mem += dparam * dparam\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "  p += seq_length # move data pointer\n",
    "  n += 1 # iteration counter \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable        Type        Data/Info\n",
      "-------------------------------------\n",
      "Whh             ndarray     100x100: 10000 elems, type `float64`, 80000 bytes\n",
      "Why             ndarray     46x100: 4600 elems, type `float64`, 36800 bytes\n",
      "Wxh             ndarray     100x46: 4600 elems, type `float64`, 36800 bytes\n",
      "bh              ndarray     100x1: 100 elems, type `float64`, 800 bytes\n",
      "by              ndarray     46x1: 46 elems, type `float64`, 368 bytes\n",
      "char_to_ix      dict        n=46\n",
      "chars           list        n=46\n",
      "dWhh            ndarray     100x100: 10000 elems, type `float64`, 80000 bytes\n",
      "dWhy            ndarray     46x100: 4600 elems, type `float64`, 36800 bytes\n",
      "dWxh            ndarray     100x46: 4600 elems, type `float64`, 36800 bytes\n",
      "data            str         Nominative determinism is<...>es for those professions.\n",
      "data_size       int         1038\n",
      "dbh             ndarray     100x1: 100 elems, type `float64`, 800 bytes\n",
      "dby             ndarray     46x1: 46 elems, type `float64`, 368 bytes\n",
      "dparam          ndarray     46x1: 46 elems, type `float64`, 368 bytes\n",
      "hidden_size     int         100\n",
      "hprev           ndarray     100x1: 100 elems, type `float64`, 800 bytes\n",
      "inputs          list        n=25\n",
      "ix_to_char      dict        n=46\n",
      "learning_rate   float       0.1\n",
      "loss            float64     54.1473756192\n",
      "lossFun         function    <function lossFun at 0x7ff320287f28>\n",
      "mWhh            ndarray     100x100: 10000 elems, type `float64`, 80000 bytes\n",
      "mWhy            ndarray     46x100: 4600 elems, type `float64`, 36800 bytes\n",
      "mWxh            ndarray     100x46: 4600 elems, type `float64`, 36800 bytes\n",
      "mbh             ndarray     100x1: 100 elems, type `float64`, 800 bytes\n",
      "mby             ndarray     46x1: 46 elems, type `float64`, 368 bytes\n",
      "mem             ndarray     46x1: 46 elems, type `float64`, 368 bytes\n",
      "n               int         888\n",
      "np              module      <module 'numpy' from '/ho<...>kages/numpy/__init__.py'>\n",
      "p               int         675\n",
      "param           ndarray     46x1: 46 elems, type `float64`, 368 bytes\n",
      "sample          function    <function sample at 0x7ff32018bc80>\n",
      "sample_ix       list        n=200\n",
      "seq_length      int         25\n",
      "smooth_loss     float64     81.3867699148\n",
      "targets         list        n=25\n",
      "txt             str         9xFpGd a,r hixaerotuggerr<...>pSnedcrtinbtede airsewrre\n",
      "vocab_size      int         46\n"
     ]
    }
   ],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nominative determinism is the hypothesis that people are drawn to professions that fit their name. The term was first used in the magazine New Scientist in 1994, after its humorous Feedback column mentioned a book on polar explorations by Daniel Snowman and an article on urology by researchers named Splatt and Weedon. The hypothesis had been suggested by psychologist Carl Jung, citing as an example Sigmund Freud (German for \"joy\"), who studied pleasure. A few recent empirical studies have indicated that certain professions are disproportionately represented by people with appropriate surnames, though the methods of these studies have been challenged. One explanation for nominative determinism is the theory of implicit egotism, which states that humans have an unconscious preference for things they associate with themselves. An alternative explanation is genetic: an ancestor might have been named Smith or Taylor according to their occupation, and the genes they passed down might correlate to aptitudes for those professions.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "k\n",
    "k\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "34\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 612 characters, 45 unique.\n",
      "----\n",
      " OHnePin \n",
      "----\n",
      "iter 0, loss: 95.166565\n",
      "----\n",
      " chn.Hum \n",
      "----\n",
      "iter 10000, loss: 0.225711\n",
      "----\n",
      " k n dt  \n",
      "----\n",
      "iter 20000, loss: 0.076315\n",
      "----\n",
      " enBr tt \n",
      "----\n",
      "iter 30000, loss: 0.045610\n",
      "----\n",
      " Ientwtä \n",
      "----\n",
      "iter 40000, loss: 0.031964\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)\n",
    "BSD License\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "# data I/O\n",
    "data = open('input_en_de.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters---> 43     loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "  \"\"\"\n",
    "  inputs,targets are both list of integers.\n",
    "  hprev is Hx1 array of initial hidden state\n",
    "  returns the loss, gradients on model parameters, and last hidden state\n",
    "  \"\"\"\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  loss = 0\n",
    "  # forward pass\n",
    "  for t in range(len(inputs)):\n",
    "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "    xs[t][inputs[t]] = 1\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "   # print(\"t\",t)\n",
    "   # print(\"shape\",(ps[t]).shape)\n",
    "   # print(\"targets\",targets[t])\n",
    "    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "  # backward pass: compute gradients going backwards\n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "  for t in reversed(range(len(inputs))):\n",
    "    dy = np.copy(ps[t])\n",
    "    dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    dby += dy\n",
    "    dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "    dbh += dhraw\n",
    "    dWxh += np.dot(dhraw, xs[t].T)\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "    dhnext = np.dot(Whh.T, dhraw)\n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "  \"\"\" \n",
    "  sample a sequence of integers from the model \n",
    "  h is memory state, seed_ix is seed letter for first time step\n",
    "  \"\"\"\n",
    "  test_sentence = \"I study\"\n",
    "  n = len(test_sentence)\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "  ixes = []\n",
    "  for t in range(n):\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[char_to_ix[test_sentence[t]]] = 1\n",
    "        \n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    y = np.dot(Why, h) + by\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "    \n",
    "    \n",
    "    \n",
    "    #x = np.zeros((vocab_size, 1))\n",
    "    #x[ix] = 1\n",
    "    \n",
    "    ixes.append(ix)\n",
    "  return ixes\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "while True:\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  if p+seq_length+1 >= len(data[0:282]) or n == 0: \n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "    p = 0 # go from start of data\n",
    "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_to_ix[ch] for ch in data[p+282:p+seq_length+282]]\n",
    "\n",
    "  # sample from the model now and then\n",
    "  if n % 10000 == 0:\n",
    "    test_sentence = \"I study\"\n",
    "    slen = len(test_sentence)\n",
    "    sample_ix = sample(hprev, inputs[0], slen)\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient\n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "  if n % 10000 == 0: print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "  \n",
    "  # perform parameter update with Adagrad\n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "    mem += dparam * dparam\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "  p += seq_length # move data pointer\n",
    "  n += 1 # iteration counter \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable        Type        Data/Info\n",
      "-------------------------------------\n",
      "Whh             ndarray     100x100: 10000 elems, type `float64`, 80000 bytes\n",
      "Why             ndarray     45x100: 4500 elems, type `float64`, 36000 bytes\n",
      "Wxh             ndarray     100x45: 4500 elems, type `float64`, 36000 bytes\n",
      "bh              ndarray     100x1: 100 elems, type `float64`, 800 bytes\n",
      "by              ndarray     45x1: 45 elems, type `float64`, 360 bytes\n",
      "char_to_ix      dict        n=45\n",
      "chars           list        n=45\n",
      "dWhh            ndarray     100x100: 10000 elems, type `float64`, 80000 bytes\n",
      "dWhy            ndarray     45x100: 4500 elems, type `float64`, 36000 bytes\n",
      "dWxh            ndarray     100x45: 4500 elems, type `float64`, 36000 bytes\n",
      "data            str         I took the bus back.I'm n<...>s war wohl unvermeidlich.\n",
      "data_size       int         612\n",
      "dbh             ndarray     100x1: 100 elems, type `float64`, 800 bytes\n",
      "dby             ndarray     45x1: 45 elems, type `float64`, 360 bytes\n",
      "dparam          ndarray     45x1: 45 elems, type `float64`, 360 bytes\n",
      "hidden_size     int         100\n",
      "hprev           ndarray     100x1: 100 elems, type `float64`, 800 bytes\n",
      "inputs          list        n=25\n",
      "ix_to_char      dict        n=45\n",
      "learning_rate   float       0.1\n",
      "loss            float64     39.6915738316\n",
      "lossFun         function    <function lossFun at 0x7f386c1509d8>\n",
      "mWhh            ndarray     100x100: 10000 elems, type `float64`, 80000 bytes\n",
      "mWhy            ndarray     45x100: 4500 elems, type `float64`, 36000 bytes\n",
      "mWxh            ndarray     100x45: 4500 elems, type `float64`, 36000 bytes\n",
      "mbh             ndarray     100x1: 100 elems, type `float64`, 800 bytes\n",
      "mby             ndarray     45x1: 45 elems, type `float64`, 360 bytes\n",
      "mem             ndarray     45x1: 45 elems, type `float64`, 360 bytes\n",
      "n               int         1473\n",
      "np              module      <module 'numpy' from '/ho<...>kages/numpy/__init__.py'>\n",
      "p               int         250\n",
      "param           ndarray     45x1: 45 elems, type `float64`, 360 bytes\n",
      "sample          function    <function sample at 0x7f386c150620>\n",
      "sample_ix       list        n=200\n",
      "seq_length      int         25\n",
      "smooth_loss     float64     66.4734983815\n",
      "targets         list        n=25\n",
      "txt             str         eeeeeeeeeeeeeeeeeeeeeeeee<...>eeeeeeeeeeeeeeeeeeeeeeeee\n",
      "vocab_size      int         45\n"
     ]
    }
   ],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 60957 characters, 82 unique.\n",
      "----\n",
      " qÄ'vWD3,,CjSGqlANp8„R'WVQ“ \n",
      "----\n",
      "iter 0, loss: 110.167986\n",
      "----\n",
      " h,mnw  esTkknekmlSFi rrh,n \n",
      "----\n",
      "iter 10000, loss: 79.983323\n",
      "----\n",
      " eteae cuekteuncImduc cll c \n",
      "----\n",
      "iter 20000, loss: 79.608685\n",
      "----\n",
      " srö b rbeBiecnmr nler i  e \n",
      "----\n",
      "iter 30000, loss: 79.374141\n",
      "----\n",
      " nas fmem gue  tt.c.ac hlul \n",
      "----\n",
      "iter 40000, loss: 79.081469\n",
      "----\n",
      "  .ör. ke i ssg.  sm uh nlo \n",
      "----\n",
      "iter 50000, loss: 78.807603\n",
      "----\n",
      " am.ncgnfctnur uertsaradbrg \n",
      "----\n",
      "iter 60000, loss: 78.544043\n",
      "----\n",
      "  gli rttel in einhrc  i nW \n",
      "----\n",
      "iter 70000, loss: 78.351953\n",
      "----\n",
      " einehe ei effn f.t rB itta \n",
      "----\n",
      "iter 80000, loss: 77.967776\n",
      "----\n",
      " eTont f enmeeIleteee eekrw \n",
      "----\n",
      "iter 90000, loss: 77.546493\n",
      "----\n",
      " esaei rieeßwIOdD. edmrEo?h \n",
      "----\n",
      "iter 100000, loss: 77.114905\n",
      "----\n",
      "  bshedweendästfeonurir drm \n",
      "----\n",
      "iter 110000, loss: 76.916603\n",
      "----\n",
      " mrs!g  hngliih do .üleubai \n",
      "----\n",
      "iter 120000, loss: 76.681081\n",
      "----\n",
      " u asneieiog a imegsazeliEb \n",
      "----\n",
      "iter 130000, loss: 76.257358\n",
      "----\n",
      " eetb..lnrs rt ei.dareunsFu \n",
      "----\n",
      "iter 140000, loss: 75.802506\n",
      "----\n",
      " i r.ri  SeeoA ndiekMe e ne \n",
      "----\n",
      "iter 150000, loss: 75.504588\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)\n",
    "BSD License\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "# data I/O\n",
    "data = open('input_en_de_1000.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters---> 43     loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "#globals\n",
    "en_ch_cnt = 28357\n",
    "test_sentence = \"Where is the bus terminal?\"\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "  \"\"\"\n",
    "  inputs,targets are both list of integers.\n",
    "  hprev is Hx1 array of initial hidden state\n",
    "  returns the loss, gradients on model parameters, and last hidden state\n",
    "  \"\"\"\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  loss = 0\n",
    "  # forward pass\n",
    "  for t in range(len(inputs)):\n",
    "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "    xs[t][inputs[t]] = 1\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "   # print(\"t\",t)\n",
    "   # print(\"shape\",(ps[t]).shape)\n",
    "   # print(\"targets\",targets[t])\n",
    "    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "  # backward pass: compute gradients going backwards\n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "  for t in reversed(range(len(inputs))):\n",
    "    dy = np.copy(ps[t])\n",
    "    dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    dby += dy\n",
    "    dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "    dbh += dhraw\n",
    "    dWxh += np.dot(dhraw, xs[t].T)\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "    dhnext = np.dot(Whh.T, dhraw)\n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "  \"\"\" \n",
    "  sample a sequence of integers from the model \n",
    "  h is memory state, seed_ix is seed letter for first time step\n",
    "  \"\"\"\n",
    "\n",
    "  n = len(test_sentence)\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "  ixes = []\n",
    "  for t in range(n):\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[char_to_ix[test_sentence[t]]] = 1\n",
    "        \n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    y = np.dot(Why, h) + by\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "    \n",
    "    \n",
    "    \n",
    "    #x = np.zeros((vocab_size, 1))\n",
    "    #x[ix] = 1\n",
    "    \n",
    "    ixes.append(ix)\n",
    "  return ixes\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "while True:\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  if p+seq_length+1 >= len(data[0:en_ch_cnt]) or n == 0: \n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "    p = 0 # go from start of data\n",
    "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_to_ix[ch] for ch in data[p+en_ch_cnt:p+seq_length+en_ch_cnt]]\n",
    "\n",
    "  # sample from the model now and then\n",
    "  if n % 10000 == 0:\n",
    "    slen = len(test_sentence)\n",
    "    sample_ix = sample(hprev, inputs[0], slen)\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient\n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "  if n % 10000 == 0: print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "  \n",
    "  # perform parameter update with Adagrad\n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "    mem += dparam * dparam\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "  p += seq_length # move data pointer\n",
    "  n += 1 # iteration counter \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
