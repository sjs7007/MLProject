{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length after processing : 21680\n"
     ]
    }
   ],
   "source": [
    "#DATA PREPROCESSING\n",
    "\n",
    "import json\n",
    "\n",
    "data = json.load(open('en_de_corpus.json', 'r'))\n",
    "\n",
    "#Script to remove punctuation, ensure <= max allowed length + padding\n",
    "\n",
    "import string\n",
    "import random\n",
    "import math\n",
    "\n",
    "#replace punctuation with space, ensure all sentences with  1 space\n",
    "#also all capital to small \n",
    "def replacePunctuation(x):\n",
    "    puncAndNum = string.punctuation+'01233456789'\n",
    "    x = x.lower()\n",
    "    for char in x:\n",
    "        if char in puncAndNum:\n",
    "            #print(char)\n",
    "            x=x.replace(char,' ')\n",
    "    return ' '.join(x.split())\n",
    "\n",
    "#max allowed raw length=5, pad to make it 5, allow only equal pairs\n",
    "tempEn = list(map(replacePunctuation,data['en']))\n",
    "tempDe = list(map(replacePunctuation,data['de']))\n",
    "maxRawLength = 5\n",
    "maxFinalLength = maxRawLength + 2\n",
    "\n",
    "def wordCount(x):\n",
    "    return len(x.split())\n",
    "\n",
    "data_en_processed = list()\n",
    "data_de_processed = list()\n",
    "for i in range(len(tempEn)):\n",
    "    if(wordCount(tempEn[i]) <= maxRawLength and wordCount(tempDe[i]) <= maxRawLength\n",
    "       and wordCount(tempEn[i])==wordCount(tempDe[i])):\n",
    "        #if(i%10==0):\n",
    "        #    print((maxFinalLength -1 - wordCount(tempEn[i])))\n",
    "        data_en_processed.append('< '+tempEn[i]+' >'*(maxFinalLength -1 - wordCount(tempEn[i])))\n",
    "        data_de_processed.append('< '+tempDe[i]+' >'*(maxFinalLength -1 - wordCount(tempDe[i])))\n",
    "\n",
    "print(\"length after processing : \"+str(len(data_en_processed)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< are you ill > > > | < bist du krank > > >\n",
      "<Bist du krank>>>\n",
      "\n",
      "< damn > > > > > | < verdammt > > > > >\n",
      "<Verdammt>>>>>\n",
      "\n",
      "< the statement was not timely > | < die stellungnahme kam nicht rechtzeitig >\n",
      "<Die Aussage war nicht rechtzeitig>\n",
      "\n",
      "< where s my tea > > | < wo bleibt mein tee > >\n",
      "<Wo s mein Tee>>\n",
      "\n",
      "< bill was in his bedroom > | < bill war in seinem schlafzimmer >\n",
      "<Rechnung war in seinem schlafzimmer>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#pick 5 random samples and display, compare with google translate\n",
    "import time\n",
    "from mtranslate import translate\n",
    "\n",
    "\n",
    "start = math.ceil(random.random()*1000)\n",
    "for i in range(start,start+5):\n",
    "    print(data_en_processed[i]+\" | \"+data_de_processed[i])\n",
    "    print(translate(data_en_processed[i],'de','en'))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<', 'er', 'unterrichtet', 'arabisch', '>', '>', '>']\n",
      "****************\n",
      "['<', 'i', 'took', 'the', 'bus', 'back', '>']\n",
      "['<', 'ich', 'nahm', 'den', 'bus', 'zurück', '>']\n"
     ]
    }
   ],
   "source": [
    "# Convert to list of lists from list of strings, remove space\n",
    "\n",
    "data_en_processed_tokens = [s.split() for s in data_en_processed]\n",
    "data_de_processed_tokens = [s.split() for s in data_de_processed]\n",
    "print(data_de_processed_tokens[77])\n",
    "print(\"****************\")\n",
    "\n",
    "#Lower the size\n",
    "data_en_processed_tokens = data_en_processed_tokens[:5]\n",
    "data_de_processed_tokens = data_de_processed_tokens[:5]\n",
    "data_full = data_en_processed_tokens + data_de_processed_tokens\n",
    "len(data_full)\n",
    "\n",
    "data_full = sum(data_full, [])\n",
    "len(data_full)\n",
    "\n",
    "print(data_full[0:7])\n",
    "print(data_full[(len(data_full)//2)+0 :(len(data_full)//2)+7])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<', 'the', 'boy', 'is', 'wearing', 'glasses', '>']\n",
      "['<', 'der', 'junge', 'trägt', 'eine', 'brille', '>']\n"
     ]
    }
   ],
   "source": [
    "#Partition back into En and De\n",
    "\n",
    "data_en =  data_full[:(len(data_full)//2)]\n",
    "data_de =  data_full[(len(data_full)//2):]\n",
    "print(data_en[28:])\n",
    "print(data_de[28:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('wunder', 1.0),\n",
       " ('beweis', 0.8489072322845459),\n",
       " ('scherz', 0.8470996618270874)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "modelEn = gensim.models.Word2Vec.load('english_word2vec_100')\n",
    "modelDe = gensim.models.Word2Vec.load('german_word2vec_100')\n",
    "xx=modelEn['wonder']\n",
    "#print(x)\n",
    "yy=modelDe['wunder']\n",
    "#print(y)\n",
    "modelDe.similar_by_vector(yy, topn=3, restrict_vocab=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data_en = [ 'i' if (e == '<' or e == '>') else e for e in data_en ]\n",
    "#data_de = [ 'ich' if (e == '<' or e == '>') else e for e in data_de ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data_en = ['i', 'i', 'took', 'the', 'bus', 'back', 'i', 'i', 'without', 'air', 'we', 'would', 'die', 'i', 'i', 'i', 'study', 'chinese', 'in', 'beijing', 'i']\n",
    "#data_de = ['ich', 'ich', 'nahm', 'den', 'bus', 'zurück', 'ich', 'ich', 'ohne', 'luft', 'würden', 'wir', 'sterben', 'ich', 'ich', 'ich', 'lerne', 'in', 'peking', 'chinesisch', 'ich']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(data_en[21:], data_de[21:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 35 en_chars, 35 de_chars, 100 vec size.\n",
      "(\"[('tom', 0.2188602089881897), ('er', 0.18748769164085388), ('verfügung', \"\n",
      " '0.16768741607666016)]')\n",
      "'***'\n",
      "(\"[('umgehen', 0.21708013117313385), ('benutzen', 0.21208728849887848), \"\n",
      " \"('finden', 0.20869877934455872)]\")\n",
      "'***'\n",
      "(\"[('weder', 0.2934654653072357), ('viele', 0.24722672998905182), ('viel', \"\n",
      " '0.22067561745643616)]')\n",
      "'***'\n",
      "(\"[('welche', 0.25996163487434387), ('eure', 0.2593446969985962), ('soviel', \"\n",
      " '0.2442854642868042)]')\n",
      "'***'\n",
      "(\"[('seit', 0.2939399182796478), ('tom', 0.26461637020111084), ('zusammen', \"\n",
      " '0.23696781694889069)]')\n",
      "'***'\n",
      "(\"[('der', 0.24813231825828552), ('zur', 0.24230194091796875), ('ganzen', \"\n",
      " '0.2302095890045166)]')\n",
      "'***'\n",
      "(\"[('sei', 0.18035529553890228), ('bin', 0.1770460605621338), ('fühle', \"\n",
      " '0.17323382198810577)]')\n",
      "'***'\n",
      "iter 0, loss: 32.206366\n",
      "iter 1000, loss: 11.879407\n",
      "iter 2000, loss: 4.368110\n",
      "iter 3000, loss: 1.606155\n",
      "iter 4000, loss: 0.590588\n",
      "iter 5000, loss: 0.217166\n",
      "iter 6000, loss: 0.079858\n",
      "iter 7000, loss: 0.029369\n",
      "iter 8000, loss: 0.010804\n",
      "iter 9000, loss: 0.003977\n",
      "(\"[('<', 0.9999926090240479), ('aber', 0.5310841798782349), ('doch', \"\n",
      " '0.5219225883483887)]')\n",
      "'***'\n",
      "(\"[('das', 0.7514971494674683), ('politik', 0.5766829252243042), ('dieses', \"\n",
      " '0.5645521879196167)]')\n",
      "'***'\n",
      "(\"[('laden', 0.7382166385650635), ('unfall', 0.7275861501693726), \"\n",
      " \"('zeitpunkt', 0.7231755256652832)]\")\n",
      "'***'\n",
      "(\"[('wir', 0.5111038088798523), ('menschen', 0.4542797803878784), ('lasst', \"\n",
      " '0.4444386959075928)]')\n",
      "'***'\n",
      "(\"[('den', 0.5811079740524292), ('alten', 0.5363435745239258), ('der', \"\n",
      " '0.5206496119499207)]')\n",
      "'***'\n",
      "(\"[('sofort', 0.7106700539588928), ('nachhauseweg', 0.6842608451843262), \"\n",
      " \"('weg', 0.6815042495727539)]\")\n",
      "'***'\n",
      "(\"[('>', 0.9617127180099487), ('damit', 0.4749760031700134), ('zumute', \"\n",
      " '0.44207412004470825)]')\n",
      "'***'\n",
      "iter 10000, loss: 0.001466\n",
      "iter 11000, loss: 0.000542\n",
      "iter 12000, loss: 0.000202\n",
      "iter 13000, loss: 0.000077\n",
      "iter 14000, loss: 0.000031\n",
      "iter 15000, loss: 0.000013\n",
      "iter 16000, loss: 0.000007\n",
      "iter 17000, loss: 0.000004\n",
      "iter 18000, loss: 0.000003\n",
      "iter 19000, loss: 0.000003\n",
      "(\"[('<', 0.9999966025352478), ('aber', 0.5313443541526794), ('doch', \"\n",
      " '0.52203369140625)]')\n",
      "'***'\n",
      "(\"[('das', 0.7520437240600586), ('politik', 0.5768604278564453), ('dieses', \"\n",
      " '0.5645885467529297)]')\n",
      "'***'\n",
      "(\"[('laden', 0.7382760047912598), ('unfall', 0.7275696992874146), \"\n",
      " \"('zeitpunkt', 0.7230619192123413)]\")\n",
      "'***'\n",
      "(\"[('wir', 0.5076067447662354), ('menschen', 0.4511595368385315), ('lasst', \"\n",
      " '0.44344812631607056)]')\n",
      "'***'\n",
      "(\"[('den', 0.5848993062973022), ('alten', 0.5348011255264282), ('der', \"\n",
      " '0.5217968225479126)]')\n",
      "'***'\n",
      "(\"[('sofort', 0.7123732566833496), ('nachhauseweg', 0.6843306422233582), \"\n",
      " \"('weg', 0.6819486618041992)]\")\n",
      "'***'\n",
      "(\"[('>', 0.9617122411727905), ('damit', 0.4749515652656555), ('zumute', \"\n",
      " '0.4419005513191223)]')\n",
      "'***'\n",
      "iter 20000, loss: 0.000003\n",
      "iter 21000, loss: 0.000002\n",
      "iter 22000, loss: 0.000002\n",
      "iter 23000, loss: 0.000002\n",
      "iter 24000, loss: 0.000002\n",
      "iter 25000, loss: 0.000002\n",
      "iter 26000, loss: 0.000002\n",
      "iter 27000, loss: 0.000002\n",
      "iter 28000, loss: 0.000002\n",
      "iter 29000, loss: 0.000002\n",
      "(\"[('<', 0.9999977946281433), ('aber', 0.5314172506332397), ('doch', \"\n",
      " '0.5220682621002197)]')\n",
      "'***'\n",
      "(\"[('das', 0.7523951530456543), ('politik', 0.5770026445388794), ('dieses', \"\n",
      " '0.5646157264709473)]')\n",
      "'***'\n",
      "(\"[('laden', 0.7384284734725952), ('unfall', 0.7276455163955688), \"\n",
      " \"('zeitpunkt', 0.7231317162513733)]\")\n",
      "'***'\n",
      "(\"[('wir', 0.5055230855941772), ('menschen', 0.4492999315261841), ('lasst', \"\n",
      " '0.4427229166030884)]')\n",
      "'***'\n",
      "(\"[('den', 0.5867148637771606), ('alten', 0.5340379476547241), ('der', \"\n",
      " '0.5223544836044312)]')\n",
      "'***'\n",
      "(\"[('sofort', 0.7128793001174927), ('nachhauseweg', 0.684491753578186), \"\n",
      " \"('weg', 0.6826667785644531)]\")\n",
      "'***'\n",
      "(\"[('>', 0.9617642760276794), ('damit', 0.4749656319618225), ('zumute', \"\n",
      " '0.44175344705581665)]')\n",
      "'***'\n",
      "iter 30000, loss: 0.000002\n",
      "iter 31000, loss: 0.000001\n",
      "iter 32000, loss: 0.000001\n",
      "iter 33000, loss: 0.000001\n",
      "iter 34000, loss: 0.000001\n",
      "iter 35000, loss: 0.000001\n",
      "iter 36000, loss: 0.000001\n",
      "iter 37000, loss: 0.000001\n",
      "iter 38000, loss: 0.000001\n",
      "iter 39000, loss: 0.000001\n",
      "(\"[('<', 0.9999984502792358), ('aber', 0.5314534902572632), ('doch', \"\n",
      " '0.5220884084701538)]')\n",
      "'***'\n",
      "(\"[('das', 0.7526410818099976), ('politik', 0.5771101117134094), ('dieses', \"\n",
      " '0.5646439790725708)]')\n",
      "'***'\n",
      "(\"[('laden', 0.7385849952697754), ('unfall', 0.7277014255523682), \"\n",
      " \"('zeitpunkt', 0.7232469916343689)]\")\n",
      "'***'\n",
      "(\"[('wir', 0.5040323138237), ('menschen', 0.4479144215583801), ('lasst', \"\n",
      " '0.4421756863594055)]')\n",
      "'***'\n",
      "(\"[('den', 0.5877845287322998), ('alten', 0.5335206985473633), ('der', \"\n",
      " '0.5226789116859436)]')\n",
      "'***'\n",
      "(\"[('sofort', 0.7130295038223267), ('nachhauseweg', 0.6845914125442505), \"\n",
      " \"('weg', 0.6832233667373657)]\")\n",
      "'***'\n",
      "(\"[('>', 0.9618237614631653), ('damit', 0.4749959409236908), ('zumute', \"\n",
      " '0.4416302442550659)]')\n",
      "'***'\n",
      "iter 40000, loss: 0.000001\n",
      "iter 41000, loss: 0.000001\n",
      "iter 42000, loss: 0.000001\n",
      "iter 43000, loss: 0.000001\n",
      "iter 44000, loss: 0.000001\n",
      "iter 45000, loss: 0.000001\n",
      "iter 46000, loss: 0.000001\n",
      "iter 47000, loss: 0.000001\n",
      "iter 48000, loss: 0.000001\n",
      "iter 49000, loss: 0.000001\n",
      "(\"[('<', 0.9999988675117493), ('aber', 0.5314775705337524), ('doch', \"\n",
      " '0.5221042037010193)]')\n",
      "'***'\n",
      "(\"[('das', 0.7528239488601685), ('politik', 0.5771937370300293), ('dieses', \"\n",
      " '0.5646723508834839)]')\n",
      "'***'\n",
      "(\"[('laden', 0.7387158870697021), ('unfall', 0.7277290225028992), \"\n",
      " \"('zeitpunkt', 0.7233576774597168)]\")\n",
      "'***'\n",
      "(\"[('wir', 0.5028877854347229), ('menschen', 0.4468132257461548), ('lasst', \"\n",
      " '0.44175201654434204)]')\n",
      "'***'\n",
      "(\"[('den', 0.5885018706321716), ('alten', 0.5331215858459473), ('der', \"\n",
      " '0.5228935480117798)]')\n",
      "'***'\n",
      "(\"[('sofort', 0.7130650877952576), ('nachhauseweg', 0.6846404075622559), \"\n",
      " \"('weg', 0.6836255788803101)]\")\n",
      "'***'\n",
      "(\"[('>', 0.9618743658065796), ('damit', 0.47503694891929626), ('zumute', \"\n",
      " '0.4415300190448761)]')\n",
      "'***'\n",
      "iter 50000, loss: 0.000001\n",
      "iter 51000, loss: 0.000001\n",
      "iter 52000, loss: 0.000001\n",
      "iter 53000, loss: 0.000001\n",
      "iter 54000, loss: 0.000001\n",
      "iter 55000, loss: 0.000001\n",
      "iter 56000, loss: 0.000001\n",
      "iter 57000, loss: 0.000001\n",
      "iter 58000, loss: 0.000001\n",
      "iter 59000, loss: 0.000001\n",
      "(\"[('<', 0.9999989867210388), ('aber', 0.531495988368988), ('doch', \"\n",
      " '0.5221177935600281)]')\n",
      "'***'\n",
      "(\"[('das', 0.7529670000076294), ('politik', 0.5772610902786255), ('dieses', \"\n",
      " '0.5647003650665283)]')\n",
      "'***'\n",
      "(\"[('laden', 0.7388197779655457), ('unfall', 0.7277363538742065), \"\n",
      " \"('zeitpunkt', 0.7234519720077515)]\")\n",
      "'***'\n",
      "(\"[('wir', 0.5019659996032715), ('menschen', 0.4459041357040405), ('lasst', \"\n",
      " '0.4414128363132477)]')\n",
      "'***'\n",
      "(\"[('den', 0.589026689529419), ('alten', 0.5327932238578796), ('der', \"\n",
      " '0.5230480432510376)]')\n",
      "'***'\n",
      "(\"[('sofort', 0.7130624651908875), ('nachhauseweg', 0.6846588850021362), \"\n",
      " \"('weg', 0.6839210987091064)]\")\n",
      "'***'\n",
      "(\"[('>', 0.9619144201278687), ('damit', 0.4750848710536957), ('zumute', \"\n",
      " '0.44144904613494873)]')\n",
      "'***'\n",
      "iter 60000, loss: 0.000001\n",
      "iter 61000, loss: 0.000001\n",
      "iter 62000, loss: 0.000001\n",
      "iter 63000, loss: 0.000001\n",
      "iter 64000, loss: 0.000001\n",
      "iter 65000, loss: 0.000001\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from mtranslate import translate\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "# data I/O\n",
    "#data = open('input_en_de_1000.txt', 'r').read() # should be simple plain text file\n",
    "#data = data_full\n",
    "chars_en = list(set(data_en))\n",
    "chars_de = list(set(data_de))\n",
    "vocab_size = 100\n",
    "\n",
    "print('data has %d en_chars, %d de_chars, %d vec size.' % (len(data_en), len(data_de), vocab_size))\n",
    "\n",
    "char_en_to_vec = { ch:modelEn[ch] for ch in chars_en }\n",
    "char_de_to_vec = { ch:modelDe[ch] for ch in chars_de }\n",
    "\n",
    "#vec_en_to_char = {  }\n",
    "#vec_de_to_char = {  }\n",
    "\n",
    "#char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "#ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters---> 43     loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "\n",
    "hidden_size = 50 # size of hidden layer of neurons\n",
    "seq_length = 7 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-2 #1e-1\n",
    "\n",
    "#globals\n",
    "#en_ch_cnt = (len(data_full)//2) #len(data_full)//2\n",
    "#test_sentence = ['i', 'i', 'took', 'the', 'bus', 'back', 'i']\n",
    "test_sentence = [ '<', 'the', 'boy', 'is', 'wearing', 'glasses', '>']\n",
    "test_sentence_vec = [char_en_to_vec[ch] for ch in test_sentence]\n",
    "#'<', 'ich', 'nahm', 'den', 'bus', 'zurück', '>'\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "  \"\"\"\n",
    "  inputs,targets are both list of integers.\n",
    "  hprev is Hx1 array of initial hidden state\n",
    "  returns the loss, gradients on model parameters, and last hidden state\n",
    "  \"\"\"\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  loss = 0\n",
    "  # forward pass\n",
    "  for t in range(len(inputs)):\n",
    "    #xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "    #xs[t][inputs[t]] = 1\n",
    "    xs[t] = np.reshape(np.array(inputs[t]), (vocab_size,1) )\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "    #ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "    \n",
    "\n",
    "    #loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "    \n",
    "    loss += 0.5*np.square(np.subtract(np.reshape(np.array(targets[t]),(vocab_size,1)) , ys[t]))\n",
    "    \n",
    "  # backward pass: compute gradients going backwards\n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "  for t in reversed(range(len(inputs))):\n",
    "    #dy = np.copy(ps[t])\n",
    "    #dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "    dy = -1*np.subtract(np.reshape(np.array(targets[t]),(vocab_size,1)) , ys[t])\n",
    "    \n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    dby += dy\n",
    "    dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "    dbh += dhraw\n",
    "    dWxh += np.dot(dhraw, xs[t].T)\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "    dhnext = np.dot(Whh.T, dhraw)\n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "\n",
    "\n",
    "def sample(h, n):\n",
    "\n",
    "  n = len(test_sentence_vec)\n",
    "\n",
    "  translated_vecs = []\n",
    "  for t in range(n):\n",
    "    #x = np.zeros((vocab_size, 1))\n",
    "    #x[char_to_ix[test_sentence[t]]] = 1\n",
    "    x = np.reshape(np.array(test_sentence_vec[t]), (vocab_size,1) )\n",
    "        \n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    y = np.dot(Why, h) + by   \n",
    "    translated_vecs.append(y)\n",
    "  return translated_vecs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "while True:\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  if p+seq_length+1 >= len(data_en) or n == 0: \n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "    p = 0 # go from start of data\n",
    "  inputs = [char_en_to_vec[ch] for ch in data_en[p:p+seq_length]]\n",
    "  #targets = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_de_to_vec[ch] for ch in data_de[p:p+seq_length]]\n",
    "\n",
    "  # sample from the model now and then\n",
    "  if n % 10000 == 0:\n",
    "    slen = len(test_sentence_vec)\n",
    "    sample_vecs = sample(hprev, slen)\n",
    "    ans = [modelDe.similar_by_vector(v.flatten(), topn=3, restrict_vocab=None) for v in sample_vecs]\n",
    "    for a in ans:\n",
    "        pprint( str(a))\n",
    "        pprint(\"***\")\n",
    "    \n",
    "    #print(translate(txt,'en', 'de'))\n",
    "    #print('---------------')\n",
    "    \n",
    "\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient\n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "  if n % 1000 == 0: print('iter %d, loss: %f' % (n, np.mean(smooth_loss)) ) # print progress\n",
    "  \n",
    "  # perform parameter update with Adagrad\n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "    mem += dparam * dparam\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "  p += seq_length # move data pointer\n",
    "  n += 1 # iteration counter \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('wunder', 0.9999999403953552)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelDe.similar_by_vector(yy, topn=1, restrict_vocab=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['geht',\n",
       " 'schwer',\n",
       " 'reich',\n",
       " '>',\n",
       " 'haare',\n",
       " 'lied',\n",
       " 'wir',\n",
       " 'das',\n",
       " 'hat',\n",
       " 'ich',\n",
       " 'war',\n",
       " 'salz',\n",
       " 'für',\n",
       " 'zu',\n",
       " 'geduld',\n",
       " 'nahm',\n",
       " 'kennen',\n",
       " 'ins',\n",
       " 'bus',\n",
       " 'mir',\n",
       " 'der',\n",
       " '<',\n",
       " 'blonde',\n",
       " 'sterben',\n",
       " 'peking',\n",
       " 'glauben',\n",
       " 'ohne',\n",
       " 'früh',\n",
       " 'er',\n",
       " 'trägt',\n",
       " 'eine',\n",
       " 'tom',\n",
       " 'junge',\n",
       " 'würden',\n",
       " 'den',\n",
       " 'lerne',\n",
       " 'danke',\n",
       " 'bett',\n",
       " 'bitte',\n",
       " 'brille',\n",
       " 'chinesisch',\n",
       " 'zurück',\n",
       " 'dieses',\n",
       " 'in',\n",
       " 'luft',\n",
       " 'ihre']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars_de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_vecs[0].flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = ['i', 'i', 'took', 'the', 'bus', 'back', 'i']\n",
    "test_sentence_vec = [char_en_to_vec[ch] for ch in test_sentence]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Variable                   Type        Data/Info\n",
    "------------------------------------------------\n",
    "Whh                        ndarray     50x50: 2500 elems, type `float64`, 20000 bytes\n",
    "Why                        ndarray     497x50: 24850 elems, type `float64`, 198800 bytes (194.140625 kb)\n",
    "Wxh                        ndarray     50x497: 24850 elems, type `float64`, 198800 bytes (194.140625 kb)\n",
    "bh                         ndarray     50x1: 50 elems, type `float64`, 400 bytes\n",
    "by                         ndarray     497x1: 497 elems, type `float64`, 3976 bytes\n",
    "char_to_ix                 dict        n=497\n",
    "chars                      list        n=497\n",
    "dWhh                       ndarray     50x50: 2500 elems, type `float64`, 20000 bytes\n",
    "dWhy                       ndarray     497x50: 24850 elems, type `float64`, 198800 bytes (194.140625 kb)\n",
    "dWxh                       ndarray     50x497: 24850 elems, type `float64`, 198800 bytes (194.140625 kb)\n",
    "data                       list        n=1400\n",
    "data_de_processed          list        n=21680\n",
    "data_de_processed_tokens   list        n=100\n",
    "data_en_processed          list        n=21680\n",
    "data_en_processed_tokens   list        n=100\n",
    "data_full                  list        n=1400\n",
    "data_size                  int         1400\n",
    "dbh                        ndarray     50x1: 50 elems, type `float64`, 400 bytes\n",
    "dby                        ndarray     497x1: 497 elems, type `float64`, 3976 bytes\n",
    "dparam                     ndarray     497x1: 497 elems, type `float64`, 3976 bytes\n",
    "en_ch_cnt                  int         700\n",
    "gensim                     module      <module 'gensim' from '/h<...>ages/gensim/__init__.py'>\n",
    "hidden_size                int         50\n",
    "hprev                      ndarray     50x1: 50 elems, type `float64`, 400 bytes\n",
    "i                          int         259\n",
    "inputs                     list        n=7\n",
    "ix_to_char                 dict        n=497\n",
    "json                       module      <module 'json' from '/hom<...>hon3.5/json/__init__.py'>\n",
    "learning_rate              float       0.01\n",
    "loss                       float64     23.793629885\n",
    "lossFun                    function    <function lossFun at 0x7f61930aeb70>\n",
    "mWhh                       ndarray     50x50: 2500 elems, type `float64`, 20000 bytes\n",
    "mWhy                       ndarray     497x50: 24850 elems, type `float64`, 198800 bytes (194.140625 kb)\n",
    "mWxh                       ndarray     50x497: 24850 elems, type `float64`, 198800 bytes (194.140625 kb)\n",
    "math                       module      <module 'math' from '/hom<...>3.5/lib-dynload/math.so'>\n",
    "maxFinalLength             int         7\n",
    "maxRawLength               int         5\n",
    "mbh                        ndarray     50x1: 50 elems, type `float64`, 400 bytes\n",
    "mby                        ndarray     497x1: 497 elems, type `float64`, 3976 bytes\n",
    "mem                        ndarray     497x1: 497 elems, type `float64`, 3976 bytes\n",
    "modelEn                    Word2Vec    Word2Vec(vocab=6972, size=100, alpha=0.025)\n",
    "modelGe                    Word2Vec    Word2Vec(vocab=9279, size=100, alpha=0.025)\n",
    "n                          int         2459\n",
    "np                         module      <module 'numpy' from '/ho<...>kages/numpy/__init__.py'>\n",
    "p                          int         581\n",
    "param                      ndarray     497x1: 497 elems, type `float64`, 3976 bytes\n",
    "random                     module      <module 'random' from '/h<...>lib/python3.5/random.py'>\n",
    "replacePunctuation         function    <function replacePunctuation at 0x7f61b38527b8>\n",
    "sample                     function    <function sample at 0x7f61c4e2b0d0>\n",
    "sample_ix                  list        n=7\n",
    "seq_length                 int         7\n",
    "slen                       int         7\n",
    "smooth_loss                float64     21.9345595918\n",
    "start                      int         255\n",
    "string                     module      <module 'string' from '/h<...>lib/python3.5/string.py'>\n",
    "targets                    list        n=7\n",
    "tempDe                     list        n=132173\n",
    "tempEn                     list        n=132173\n",
    "test_sentence              list        n=7\n",
    "time                       module      <module 'time' (built-in)>\n",
    "translate                  function    <function translate at 0x7f61b27228c8>\n",
    "txt                        str         < ich ist ist > > >\n",
    "vocab_size                 int         497\n",
    "wordCount                  function    <function wordCount at 0x7f61b2b3d510>\n",
    "x                          ndarray     100: 100 elems, type `float32`, 400 bytes\n",
    "y                          ndarray     100: 100 elems, type `float32`, 400 bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
