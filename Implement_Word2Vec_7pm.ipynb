{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length after processing : 21680\n"
     ]
    }
   ],
   "source": [
    "#DATA PREPROCESSING\n",
    "\n",
    "import json\n",
    "\n",
    "data = json.load(open('en_de_corpus.json', 'r'))\n",
    "\n",
    "#Script to remove punctuation, ensure <= max allowed length + padding\n",
    "\n",
    "import string\n",
    "import random\n",
    "import math\n",
    "\n",
    "#replace punctuation with space, ensure all sentences with  1 space\n",
    "#also all capital to small \n",
    "def replacePunctuation(x):\n",
    "    puncAndNum = string.punctuation+'01233456789'\n",
    "    x = x.lower()\n",
    "    for char in x:\n",
    "        if char in puncAndNum:\n",
    "            #print(char)\n",
    "            x=x.replace(char,' ')\n",
    "    return ' '.join(x.split())\n",
    "\n",
    "#max allowed raw length=5, pad to make it 5, allow only equal pairs\n",
    "tempEn = list(map(replacePunctuation,data['en']))\n",
    "tempDe = list(map(replacePunctuation,data['de']))\n",
    "maxRawLength = 5\n",
    "maxFinalLength = maxRawLength + 2\n",
    "\n",
    "def wordCount(x):\n",
    "    return len(x.split())\n",
    "\n",
    "data_en_processed = list()\n",
    "data_de_processed = list()\n",
    "for i in range(len(tempEn)):\n",
    "    if(wordCount(tempEn[i]) <= maxRawLength and wordCount(tempDe[i]) <= maxRawLength\n",
    "       and wordCount(tempEn[i])==wordCount(tempDe[i])):\n",
    "        #if(i%10==0):\n",
    "        #    print((maxFinalLength -1 - wordCount(tempEn[i])))\n",
    "        data_en_processed.append('< '+tempEn[i]+' >'*(maxFinalLength -1 - wordCount(tempEn[i])))\n",
    "        data_de_processed.append('< '+tempDe[i]+' >'*(maxFinalLength -1 - wordCount(tempDe[i])))\n",
    "\n",
    "print(\"length after processing : \"+str(len(data_en_processed)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< she explained a joke > > | < sie erklärte einen witz > >\n",
      "<Sie erklärte einen Witz>>\n",
      "\n",
      "< he left the door unlocked > | < er ließ die tür unverschlossen >\n",
      "<Er hat die Tür entriegelt\n",
      "\n",
      "< they re gone > > > | < sie sind weg > > >\n",
      "<Sie sind weg>>>\n",
      "\n",
      "< i want them > > > | < ich will sie > > >\n",
      "Ich möchte sie>>>\n",
      "\n",
      "< fishing is prohibited here > > | < angeln ist hier verboten > >\n",
      "<Fischen ist hier verboten>>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#pick 5 random samples and display, compare with google translate\n",
    "import time\n",
    "from mtranslate import translate\n",
    "\n",
    "\n",
    "start = math.ceil(random.random()*1000)\n",
    "for i in range(start,start+5):\n",
    "    print(data_en_processed[i]+\" | \"+data_de_processed[i])\n",
    "    print(translate(data_en_processed[i],'de','en'))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<', 'er', 'unterrichtet', 'arabisch', '>', '>', '>']\n",
      "****************\n",
      "['<', 'i', 'took', 'the', 'bus', 'back', '>']\n",
      "['<', 'ich', 'nahm', 'den', 'bus', 'zurück', '>']\n"
     ]
    }
   ],
   "source": [
    "# Convert to list of lists from list of strings, remove space\n",
    "\n",
    "data_en_processed_tokens = [s.split() for s in data_en_processed]\n",
    "data_de_processed_tokens = [s.split() for s in data_de_processed]\n",
    "print(data_de_processed_tokens[77])\n",
    "print(\"****************\")\n",
    "\n",
    "#Lower the size\n",
    "data_en_processed_tokens = data_en_processed_tokens[:200] #senCount\n",
    "data_de_processed_tokens = data_de_processed_tokens[:200]\n",
    "data_full = data_en_processed_tokens + data_de_processed_tokens\n",
    "len(data_full)\n",
    "\n",
    "data_full = sum(data_full, [])\n",
    "len(data_full)\n",
    "\n",
    "print(data_full[0:7])\n",
    "print(data_full[(len(data_full)//2)+0 :(len(data_full)//2)+7])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1400\n",
      "1400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "501"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Partition back into En and De\n",
    "\n",
    "data_en =  data_full[:(len(data_full)//2)]\n",
    "data_de =  data_full[(len(data_full)//2):]\n",
    "print(len(data_en))\n",
    "print(len(data_de))\n",
    "data_en.index('tea')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<', 'i', 'took', 'the', 'bus', 'back', '>']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_de)\n",
    "data_en[0:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<', 'she', 'was', 'making', 'tea', '>', '>'] ['<', 'sie', 'machte', 'gerade', 'tee', '>', '>']\n"
     ]
    }
   ],
   "source": [
    "tempEnList = list()\n",
    "for i in range(int(len(data_en)/7)):\n",
    "    tempEnList.append(data_en[i*7:(i+1)*7])\n",
    "    \n",
    "#tempEnList.append(['tea'])\n",
    "\n",
    "tempDeList = list()\n",
    "for i in range(int(len(data_de)/7)):\n",
    "    tempDeList.append(data_de[i*7:(i+1)*7])\n",
    "    \n",
    "print(tempEnList[71],tempDeList[71])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable                   Type        Data/Info\n",
      "------------------------------------------------\n",
      "data                       dict        n=2\n",
      "data_de                    list        n=1400\n",
      "data_de_processed          list        n=21680\n",
      "data_de_processed_tokens   list        n=200\n",
      "data_en                    list        n=1400\n",
      "data_en_processed          list        n=21680\n",
      "data_en_processed_tokens   list        n=200\n",
      "data_full                  list        n=2800\n",
      "i                          int         199\n",
      "json                       module      <module 'json' from '/usr<...>hon3.5/json/__init__.py'>\n",
      "math                       module      <module 'math' (built-in)>\n",
      "maxFinalLength             int         7\n",
      "maxRawLength               int         5\n",
      "random                     module      <module 'random' from '/h<...>lib/python3.5/random.py'>\n",
      "replacePunctuation         function    <function replacePunctuation at 0x7f40a08f9268>\n",
      "start                      int         114\n",
      "string                     module      <module 'string' from '/u<...>lib/python3.5/string.py'>\n",
      "tempDe                     list        n=132173\n",
      "tempDeList                 list        n=200\n",
      "tempEn                     list        n=132173\n",
      "tempEnList                 list        n=200\n",
      "time                       module      <module 'time' (built-in)>\n",
      "translate                  function    <function translate at 0x7f40937156a8>\n",
      "wordCount                  function    <function wordCount at 0x7f4093b14f28>\n"
     ]
    }
   ],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<', 'she', 'was', 'making', 'tea', '>', '>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Slow version of gensim.models.doc2vec is being used\n",
      "2017-04-20 19:44:59,021 : WARNING : Slow version of gensim.models.word2vec is being used\n",
      "2017-04-20 19:44:59,022 : INFO : collecting all words and their counts\n",
      "2017-04-20 19:44:59,023 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-04-20 19:44:59,024 : INFO : collected 420 word types from a corpus of 1400 raw words and 200 sentences\n",
      "2017-04-20 19:44:59,025 : INFO : Loading a fresh vocabulary\n",
      "2017-04-20 19:44:59,026 : INFO : min_count=1 retains 420 unique words (100% of original 420, drops 0)\n",
      "2017-04-20 19:44:59,027 : INFO : min_count=1 leaves 1400 word corpus (100% of original 1400, drops 0)\n",
      "2017-04-20 19:44:59,029 : INFO : deleting the raw counts dictionary of 420 items\n",
      "2017-04-20 19:44:59,040 : INFO : sample=0.001 downsamples 40 most-common words\n",
      "2017-04-20 19:44:59,041 : INFO : downsampling leaves estimated 695 word corpus (49.7% of prior 1400)\n",
      "2017-04-20 19:44:59,044 : INFO : estimated required memory for 420 words and 100 dimensions: 546000 bytes\n",
      "2017-04-20 19:44:59,046 : INFO : resetting layer weights\n",
      "/home/shinchan/MLProject/venv/lib/python3.5/site-packages/gensim/models/word2vec.py:772: UserWarning: C extension not loaded for Word2Vec, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  warnings.warn(\"C extension not loaded for Word2Vec, training will be slow. \"\n",
      "2017-04-20 19:44:59,057 : INFO : training model with 4 workers on 420 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-04-20 19:44:59,059 : INFO : expecting 200 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-04-20 19:44:59,073 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-04-20 19:44:59,074 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-04-20 19:44:59,078 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-04-20 19:44:59,651 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-04-20 19:44:59,652 : INFO : training on 7000 raw words (3465 effective words) took 0.6s, 5934 effective words/s\n",
      "2017-04-20 19:44:59,653 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-04-20 19:44:59,654 : INFO : saving Word2Vec object under english_word2vec_new, separately None\n",
      "2017-04-20 19:44:59,655 : INFO : not storing attribute syn0norm\n",
      "2017-04-20 19:44:59,656 : INFO : not storing attribute cum_table\n",
      "2017-04-20 19:44:59,666 : INFO : saved english_word2vec_new\n",
      "2017-04-20 19:44:59,668 : WARNING : Slow version of gensim.models.word2vec is being used\n",
      "2017-04-20 19:44:59,669 : INFO : collecting all words and their counts\n",
      "2017-04-20 19:44:59,670 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-04-20 19:44:59,671 : INFO : collected 465 word types from a corpus of 1400 raw words and 200 sentences\n",
      "2017-04-20 19:44:59,673 : INFO : Loading a fresh vocabulary\n",
      "2017-04-20 19:44:59,676 : INFO : min_count=1 retains 465 unique words (100% of original 465, drops 0)\n",
      "2017-04-20 19:44:59,677 : INFO : min_count=1 leaves 1400 word corpus (100% of original 1400, drops 0)\n",
      "2017-04-20 19:44:59,680 : INFO : deleting the raw counts dictionary of 465 items\n",
      "2017-04-20 19:44:59,682 : INFO : sample=0.001 downsamples 39 most-common words\n",
      "2017-04-20 19:44:59,683 : INFO : downsampling leaves estimated 725 word corpus (51.8% of prior 1400)\n",
      "2017-04-20 19:44:59,684 : INFO : estimated required memory for 465 words and 100 dimensions: 604500 bytes\n",
      "2017-04-20 19:44:59,693 : INFO : resetting layer weights\n",
      "2017-04-20 19:44:59,705 : INFO : training model with 4 workers on 465 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-04-20 19:44:59,706 : INFO : expecting 200 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-04-20 19:44:59,717 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-04-20 19:44:59,722 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-04-20 19:44:59,725 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-04-20 19:45:00,276 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-04-20 19:45:00,277 : INFO : training on 7000 raw words (3621 effective words) took 0.6s, 6389 effective words/s\n",
      "2017-04-20 19:45:00,278 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-04-20 19:45:00,279 : INFO : saving Word2Vec object under german_word2vec_new, separately None\n",
      "2017-04-20 19:45:00,280 : INFO : not storing attribute syn0norm\n",
      "2017-04-20 19:45:00,282 : INFO : not storing attribute cum_table\n",
      "2017-04-20 19:45:00,292 : INFO : saved german_word2vec_new\n"
     ]
    }
   ],
   "source": [
    "print(tempEnList[71])\n",
    "\n",
    "import gensim, logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "\n",
    "modelEn = gensim.models.Word2Vec(tempEnList,size=100,workers=4,min_count=1)\n",
    "modelEn.save('english_word2vec_new')\n",
    "\n",
    "modelDe = gensim.models.Word2Vec(tempDeList,size=100,workers=4,min_count=1)\n",
    "modelDe.save('german_word2vec_new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.84587778e-03,  -4.85543627e-03,   5.30939316e-03,\n",
       "        -2.99719686e-04,  -8.41117348e-04,  -2.50016968e-03,\n",
       "         3.50646302e-03,  -3.02183349e-03,  -2.08449853e-03,\n",
       "         4.19956446e-03,  -2.86381319e-03,  -1.83603226e-03,\n",
       "        -4.59576771e-03,  -1.63961959e-03,   2.61394982e-03,\n",
       "         2.91213859e-03,  -2.89459084e-03,   1.30257569e-03,\n",
       "         4.52770339e-03,   4.31183679e-03,  -3.94775253e-03,\n",
       "         2.03103991e-03,   1.30981774e-04,  -4.98418754e-04,\n",
       "        -3.02989152e-03,   3.24156578e-03,  -2.00152770e-03,\n",
       "        -4.40166006e-03,   7.68821570e-04,  -3.14034405e-03,\n",
       "        -1.10089837e-03,   1.44912710e-03,   4.71979799e-03,\n",
       "         3.25755123e-03,   4.67696274e-03,  -3.65484069e-04,\n",
       "        -1.16903603e-03,  -2.97307270e-03,  -3.27815767e-03,\n",
       "         1.34747289e-03,  -1.69048188e-04,  -4.75517241e-03,\n",
       "        -2.04933545e-04,   3.05122929e-03,  -1.75330241e-03,\n",
       "         3.16640548e-03,  -1.91130803e-03,  -2.18943181e-03,\n",
       "        -9.38843295e-04,  -3.18842591e-03,   4.46912879e-03,\n",
       "        -2.97110365e-03,   1.48681502e-04,   5.44771506e-03,\n",
       "         4.00107168e-03,   2.99600597e-05,   2.63713324e-03,\n",
       "         3.92923830e-03,   2.86331028e-03,   4.44792956e-03,\n",
       "         1.20633258e-03,  -4.13555326e-03,   1.32793537e-03,\n",
       "        -2.67775031e-03,  -5.02031343e-03,   2.16660392e-03,\n",
       "         3.30296834e-03,   1.74564857e-03,   1.63564528e-03,\n",
       "        -3.70332017e-03,  -1.64634164e-03,   2.86348560e-03,\n",
       "        -4.54193633e-03,  -3.01139592e-03,  -4.11891518e-03,\n",
       "        -3.22760898e-04,   3.48803680e-03,   3.07053770e-03,\n",
       "         1.57244946e-03,  -3.06125497e-03,   3.95213906e-03,\n",
       "         2.19312985e-03,   2.79833982e-03,  -3.88863147e-03,\n",
       "         3.68461733e-05,   2.03846706e-04,  -2.90291617e-03,\n",
       "         2.41447915e-03,   2.83766305e-03,  -4.85299155e-03,\n",
       "        -9.17117693e-04,  -3.92526714e-03,   6.37200603e-04,\n",
       "        -2.65629659e-03,   6.23291417e-04,  -2.83403532e-03,\n",
       "         2.90179346e-03,  -9.55230134e-05,  -2.27083662e-03,\n",
       "        -2.81965313e-03], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelEn['tea']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data_en = [ 'i' if (e == '<' or e == '>') else e for e in data_en ]\n",
    "#data_de = [ 'ich' if (e == '<' or e == '>') else e for e in data_de ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data_en = ['i', 'i', 'took', 'the', 'bus', 'back', 'i', 'i', 'without', 'air', 'we', 'would', 'die', 'i', 'i', 'i', 'study', 'chinese', 'in', 'beijing', 'i']\n",
    "#data_de = ['ich', 'ich', 'nahm', 'den', 'bus', 'zurück', 'ich', 'ich', 'ohne', 'luft', 'würden', 'wir', 'sterben', 'ich', 'ich', 'ich', 'lerne', 'in', 'peking', 'chinesisch', 'ich']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(data_en[21:], data_de[21:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1400 en_chars, 1400 de_chars, 100 vec size.\n",
      "---------------\n",
      "Input Sentence : < i took the bus back >\n",
      "Expected Translation by Google : <Ich nahm den Bus zurück>\n",
      "word 1 : erste 0.29  ,  geschlossen 0.24  ,  selbstverständlich 0.24  ,  \n",
      "word 2 : ohne 0.32  ,  gekleidet 0.25  ,  klassenkameraden 0.22  ,  \n",
      "word 3 : verrucht 0.27  ,  niemand 0.25  ,  wasserdicht 0.24  ,  \n",
      "word 4 : geschlossen 0.23  ,  artikel 0.22  ,  weit 0.22  ,  \n",
      "word 5 : diebstahl 0.36  ,  entsetzlich 0.31  ,  etwas 0.26  ,  \n",
      "word 6 : werde 0.31  ,  hatte 0.30  ,  geschlossen 0.30  ,  \n",
      "word 7 : erste 0.27  ,  selbstverständlich 0.25  ,  gedicht 0.23  ,  \n",
      "iter 0, loss: 32.203955\n",
      "iter 1000, loss: 11.841262\n",
      "iter 2000, loss: 4.353990\n",
      "iter 3000, loss: 1.600954\n",
      "iter 4000, loss: 0.588675\n",
      "iter 5000, loss: 0.216464\n",
      "iter 6000, loss: 0.079604\n",
      "iter 7000, loss: 0.029281\n",
      "iter 8000, loss: 0.010777\n",
      "iter 9000, loss: 0.003973\n",
      "---------------\n",
      "Input Sentence : < i took the bus back >\n",
      "Expected Translation by Google : <Ich nahm den Bus zurück>\n",
      "word 1 : < 1.00  ,  > 0.90  ,  ist 0.79  ,  \n",
      "word 2 : ich 0.98  ,  > 0.78  ,  < 0.75  ,  \n",
      "word 3 : ist 0.74  ,  mir 0.50  ,  das 0.49  ,  \n",
      "word 4 : die 0.87  ,  > 0.80  ,  < 0.76  ,  \n",
      "word 5 : das 0.58  ,  ist 0.57  ,  sind 0.53  ,  \n",
      "word 6 : < 0.72  ,  das 0.61  ,  ist 0.57  ,  \n",
      "word 7 : > 1.00  ,  < 0.89  ,  ist 0.81  ,  \n",
      "iter 10000, loss: 0.001472\n",
      "iter 11000, loss: 0.000552\n",
      "iter 12000, loss: 0.000213\n",
      "iter 13000, loss: 0.000089\n",
      "iter 14000, loss: 0.000043\n",
      "iter 15000, loss: 0.000026\n",
      "iter 16000, loss: 0.000020\n",
      "iter 17000, loss: 0.000017\n",
      "iter 18000, loss: 0.000016\n",
      "iter 19000, loss: 0.000016\n",
      "---------------\n",
      "Input Sentence : < i took the bus back >\n",
      "Expected Translation by Google : <Ich nahm den Bus zurück>\n",
      "word 1 : < 1.00  ,  > 0.90  ,  ist 0.78  ,  \n",
      "word 2 : ich 0.99  ,  > 0.75  ,  < 0.73  ,  \n",
      "word 3 : ist 0.65  ,  mir 0.45  ,  nahm 0.42  ,  \n",
      "word 4 : die 0.88  ,  > 0.77  ,  < 0.71  ,  \n",
      "word 5 : dieses 0.43  ,  ohne 0.43  ,  bus 0.41  ,  \n",
      "word 6 : < 0.57  ,  hat 0.45  ,  dich 0.44  ,  \n",
      "word 7 : > 1.00  ,  < 0.89  ,  ist 0.81  ,  \n",
      "iter 20000, loss: 0.000016\n",
      "iter 21000, loss: 0.000016\n",
      "iter 22000, loss: 0.000015\n",
      "iter 23000, loss: 0.000015\n",
      "iter 24000, loss: 0.000015\n",
      "iter 25000, loss: 0.000015\n",
      "iter 26000, loss: 0.000015\n",
      "iter 27000, loss: 0.000015\n",
      "iter 28000, loss: 0.000015\n",
      "iter 29000, loss: 0.000015\n",
      "---------------\n",
      "Input Sentence : < i took the bus back >\n",
      "Expected Translation by Google : <Ich nahm den Bus zurück>\n",
      "word 1 : < 1.00  ,  > 0.89  ,  ist 0.79  ,  \n",
      "word 2 : ich 1.00  ,  > 0.69  ,  < 0.66  ,  \n",
      "word 3 : ist 0.54  ,  nahm 0.42  ,  mir 0.38  ,  \n",
      "word 4 : die 0.87  ,  > 0.72  ,  < 0.65  ,  \n",
      "word 5 : bus 0.33  ,  unverschlossen 0.29  ,  versuche 0.29  ,  \n",
      "word 6 : < 0.38  ,  dich 0.38  ,  hat 0.36  ,  \n",
      "word 7 : > 1.00  ,  < 0.89  ,  ist 0.81  ,  \n",
      "iter 30000, loss: 0.000015\n",
      "iter 31000, loss: 0.000015\n",
      "iter 32000, loss: 0.000015\n",
      "iter 33000, loss: 0.000015\n",
      "iter 34000, loss: 0.000015\n",
      "iter 35000, loss: 0.000015\n",
      "iter 36000, loss: 0.000015\n",
      "iter 37000, loss: 0.000015\n",
      "iter 38000, loss: 0.000015\n",
      "iter 39000, loss: 0.000015\n",
      "---------------\n",
      "Input Sentence : < i took the bus back >\n",
      "Expected Translation by Google : <Ich nahm den Bus zurück>\n",
      "word 1 : < 1.00  ,  > 0.90  ,  ist 0.79  ,  \n",
      "word 2 : ich 0.99  ,  > 0.73  ,  < 0.70  ,  \n",
      "word 3 : ist 0.61  ,  nahm 0.52  ,  mir 0.46  ,  \n",
      "word 4 : die 0.86  ,  > 0.73  ,  < 0.65  ,  \n",
      "word 5 : bus 0.43  ,  versuche 0.36  ,  unverschlossen 0.31  ,  \n",
      "word 6 : < 0.47  ,  dich 0.43  ,  hat 0.42  ,  \n",
      "word 7 : > 1.00  ,  < 0.89  ,  ist 0.81  ,  \n",
      "iter 40000, loss: 0.000015\n",
      "iter 41000, loss: 0.000015\n",
      "iter 42000, loss: 0.000015\n",
      "iter 43000, loss: 0.000015\n",
      "iter 44000, loss: 0.000015\n",
      "iter 45000, loss: 0.000014\n",
      "iter 46000, loss: 0.000014\n",
      "iter 47000, loss: 0.000014\n",
      "iter 48000, loss: 0.000014\n",
      "iter 49000, loss: 0.000014\n",
      "---------------\n",
      "Input Sentence : < i took the bus back >\n",
      "Expected Translation by Google : <Ich nahm den Bus zurück>\n",
      "word 1 : < 1.00  ,  > 0.89  ,  ist 0.78  ,  \n",
      "word 2 : ich 0.99  ,  > 0.74  ,  < 0.71  ,  \n",
      "word 3 : ist 0.62  ,  nahm 0.50  ,  mir 0.46  ,  \n",
      "word 4 : die 0.85  ,  > 0.78  ,  < 0.70  ,  \n",
      "word 5 : bus 0.40  ,  versuche 0.35  ,  dieses 0.33  ,  \n",
      "word 6 : < 0.55  ,  das 0.52  ,  dich 0.47  ,  \n",
      "word 7 : > 1.00  ,  < 0.89  ,  ist 0.81  ,  \n",
      "iter 50000, loss: 0.000014\n",
      "iter 51000, loss: 0.000014\n",
      "iter 52000, loss: 0.000014\n",
      "iter 53000, loss: 0.000014\n",
      "iter 54000, loss: 0.000014\n",
      "iter 55000, loss: 0.000014\n",
      "iter 56000, loss: 0.000014\n",
      "iter 57000, loss: 0.000014\n",
      "iter 58000, loss: 0.000014\n",
      "iter 59000, loss: 0.000014\n",
      "---------------\n",
      "Input Sentence : < i took the bus back >\n",
      "Expected Translation by Google : <Ich nahm den Bus zurück>\n",
      "word 1 : < 1.00  ,  > 0.89  ,  ist 0.78  ,  \n",
      "word 2 : ich 0.99  ,  > 0.75  ,  < 0.72  ,  \n",
      "word 3 : ist 0.64  ,  nahm 0.53  ,  mir 0.47  ,  \n",
      "word 4 : die 0.83  ,  > 0.80  ,  < 0.73  ,  \n",
      "word 5 : bus 0.43  ,  versuche 0.35  ,  dieses 0.32  ,  \n",
      "word 6 : < 0.57  ,  das 0.51  ,  dich 0.46  ,  \n",
      "word 7 : > 1.00  ,  < 0.89  ,  ist 0.81  ,  \n",
      "iter 60000, loss: 0.000014\n",
      "iter 61000, loss: 0.000014\n",
      "iter 62000, loss: 0.000014\n",
      "iter 63000, loss: 0.000014\n",
      "iter 64000, loss: 0.000014\n",
      "iter 65000, loss: 0.000014\n",
      "iter 66000, loss: 0.000014\n",
      "iter 67000, loss: 0.000014\n",
      "iter 68000, loss: 0.000014\n",
      "iter 69000, loss: 0.000014\n",
      "---------------\n",
      "Input Sentence : < i took the bus back >\n",
      "Expected Translation by Google : <Ich nahm den Bus zurück>\n",
      "word 1 : < 1.00  ,  > 0.90  ,  ist 0.79  ,  \n",
      "word 2 : ich 0.99  ,  > 0.76  ,  < 0.72  ,  \n",
      "word 3 : ist 0.48  ,  nahm 0.46  ,  mir 0.34  ,  \n",
      "word 4 : die 0.80  ,  > 0.78  ,  < 0.70  ,  \n",
      "word 5 : bus 0.41  ,  versuche 0.36  ,  unverschlossen 0.31  ,  \n",
      "word 6 : < 0.54  ,  das 0.52  ,  ist 0.45  ,  \n",
      "word 7 : > 1.00  ,  < 0.89  ,  ist 0.82  ,  \n",
      "iter 70000, loss: 0.000014\n",
      "iter 71000, loss: 0.000014\n",
      "iter 72000, loss: 0.000014\n",
      "iter 73000, loss: 0.000014\n",
      "iter 74000, loss: 0.000014\n",
      "iter 75000, loss: 0.000014\n",
      "iter 76000, loss: 0.000014\n",
      "iter 77000, loss: 0.000014\n",
      "iter 78000, loss: 0.000014\n",
      "iter 79000, loss: 0.000014\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from mtranslate import translate\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "\n",
    "\n",
    "# data I/O\n",
    "#data = open('input_en_de_1000.txt', 'r').read() # should be simple plain text file\n",
    "#data = data_full\n",
    "chars_en = list(set(data_en))\n",
    "chars_de = list(set(data_de))\n",
    "vocab_size = 100\n",
    "\n",
    "print('data has %d en_chars, %d de_chars, %d vec size.' % (len(data_en), len(data_de), vocab_size))\n",
    "\n",
    "char_en_to_vec = { ch:modelEn[ch] for ch in chars_en }\n",
    "char_de_to_vec = { ch:modelDe[ch] for ch in chars_de }\n",
    "\n",
    "#vec_en_to_char = {  }\n",
    "#vec_de_to_char = {  }\n",
    "\n",
    "#char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "#ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters---> 43     loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "\n",
    "hidden_size = 150 # size of hidden layer of neurons\n",
    "seq_length = 7 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-2 #1e-1\n",
    "\n",
    "#globals\n",
    "#en_ch_cnt = (len(data_full)//2) #len(data_full)//2\n",
    "test_sentence = ['<', 'i', 'took', 'the', 'bus', 'back', '>']\n",
    "#test_sentence = [ '<', 'the', 'boy', 'is', 'wearing', 'glasses', '>']\n",
    "#test_sentence = [ '<', 'i', 'ate', 'a', 'delicious', 'apple', '>']\n",
    "test_sentence_de= translate(' '.join(test_sentence),'de', 'en').split()\n",
    "\n",
    "test_sentence_vec = [char_en_to_vec[ch] for ch in test_sentence]\n",
    "#'<', 'ich', 'nahm', 'den', 'bus', 'zurück', '>'\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "modelParametersDict = {}\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "  \"\"\"\n",
    "  inputs,targets are both list of integers.\n",
    "  hprev is Hx1 array of initial hidden state\n",
    "  returns the loss, gradients on model parameters, and last hidden state\n",
    "  \"\"\"\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  loss = 0\n",
    "  # forward pass\n",
    "  for t in range(len(inputs)):\n",
    "    #xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "    #xs[t][inputs[t]] = 1\n",
    "    xs[t] = np.reshape(np.array(inputs[t]), (vocab_size,1) )\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "    #ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "    \n",
    "\n",
    "    #loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "    \n",
    "    loss += 0.5*np.square(np.subtract(np.reshape(np.array(targets[t]),(vocab_size,1)) , ys[t]))\n",
    "    \n",
    "  # backward pass: compute gradients going backwards\n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "  for t in reversed(range(len(inputs))):\n",
    "    #dy = np.copy(ps[t])\n",
    "    #dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "    dy = -1*np.subtract(np.reshape(np.array(targets[t]),(vocab_size,1)) , ys[t])\n",
    "    \n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    dby += dy\n",
    "    dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "    dbh += dhraw\n",
    "    dWxh += np.dot(dhraw, xs[t].T)\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "    dhnext = np.dot(Whh.T, dhraw)\n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "\n",
    "\n",
    "def sample(h, n):\n",
    "\n",
    "  n = len(test_sentence_vec)\n",
    "\n",
    "  translated_vecs = []\n",
    "  for t in range(n):\n",
    "    #x = np.zeros((vocab_size, 1))\n",
    "    #x[char_to_ix[test_sentence[t]]] = 1\n",
    "    x = np.reshape(np.array(test_sentence_vec[t]), (vocab_size,1) )\n",
    "        \n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    y = np.dot(Why, h) + by   \n",
    "    translated_vecs.append(y)\n",
    "  return translated_vecs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "while True:\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  if p+seq_length+1 >= len(data_en) or n == 0: \n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "    p = 0 # go from start of data\n",
    "  inputs = [char_en_to_vec[ch] for ch in data_en[p:p+seq_length]]\n",
    "  #targets = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_de_to_vec[ch] for ch in data_de[p:p+seq_length]]\n",
    "\n",
    "  # sample from the model now and then\n",
    "  if n % 10000 == 0:\n",
    "    slen = len(test_sentence_vec)\n",
    "    sample_vecs = sample(hprev, slen)\n",
    "    \n",
    "    print('---------------')\n",
    "    print('Input Sentence : '+' '.join(test_sentence))\n",
    "    print('Expected Translation by Google : '+' '.join(test_sentence_de))\n",
    "    \n",
    "    i = 0\n",
    "    for v in sample_vecs:\n",
    "        top_n = modelDe.similar_by_vector(v.flatten(), topn=3, restrict_vocab=None)\n",
    "        print(\"word \"+str(i+1)+\" : \",end='')\n",
    "        for j in range(3):\n",
    "            print(top_n[j][0],\"%.2f\" %top_n[j][1] ,\" , \" ,end=' ')\n",
    "        i=i+1\n",
    "        print()\n",
    "    if(n==1000000):\n",
    "        modelParametersDict['Wxh'] = Wxh\n",
    "        modelParametersDict['Whh'] = Whh\n",
    "        modelParametersDict['Why'] = Why\n",
    "        modelParametersDict['bh'] = bh\n",
    "        modelParametersDict['by'] = by\n",
    "        modelParametersDict['hprev'] = hprev\n",
    "        modelParametersDict['char_en_to_vec'] = char_en_to_vec\n",
    "        with open('modelParameters.pickle', 'wb') as handle:\n",
    "            pickle.dump(modelParametersDict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        break\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient\n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "  if n % 1000 == 0: print('iter %d, loss: %f' % (n, np.mean(smooth_loss)) ) # print progress\n",
    "  \n",
    "  # perform parameter update with Adagrad\n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "    mem += dparam * dparam\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "  p += seq_length # move data pointer\n",
    "  n += 1 # iteration counter \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "Input Sentence : < i took the bus back >\n",
      "Expected Translation by Google : <Ich nahm den Bus zurück>\n",
      "word 1 : < 1.00  ,  > 0.90  ,  ist 0.79  ,  \n",
      "word 2 : ich 0.98  ,  > 0.78  ,  < 0.76  ,  \n",
      "word 3 : ist 0.74  ,  mir 0.52  ,  das 0.51  ,  \n",
      "word 4 : die 0.87  ,  > 0.81  ,  < 0.77  ,  \n",
      "word 5 : das 0.51  ,  ist 0.44  ,  sind 0.44  ,  \n",
      "word 6 : < 0.70  ,  das 0.59  ,  ist 0.55  ,  \n",
      "word 7 : > 1.00  ,  < 0.89  ,  ist 0.81  ,  \n"
     ]
    }
   ],
   "source": [
    "# test_sentence_vec, hprev, \n",
    "\n",
    "slen = len(test_sentence_vec)\n",
    "sample_vecs = sample(hprev, slen)\n",
    "\n",
    "print('---------------')\n",
    "print('Input Sentence : '+' '.join(test_sentence))\n",
    "print('Expected Translation by Google : '+' '.join(test_sentence_de))\n",
    "\n",
    "i = 0\n",
    "for v in sample_vecs:\n",
    "    top_n = modelDe.similar_by_vector(v.flatten(), topn=3, restrict_vocab=None)\n",
    "    print(\"word \"+str(i+1)+\" : \",end='')\n",
    "    for j in range(3):\n",
    "        print(top_n[j][0],\"%.2f\" %top_n[j][1] ,\" , \" ,end=' ')\n",
    "    i=i+1\n",
    "    print()\n",
    "#modelDe.similar_by_vector(yy, topn=1, restrict_vocab=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['geht',\n",
       " 'schwer',\n",
       " 'reich',\n",
       " '>',\n",
       " 'haare',\n",
       " 'lied',\n",
       " 'wir',\n",
       " 'das',\n",
       " 'hat',\n",
       " 'ich',\n",
       " 'war',\n",
       " 'salz',\n",
       " 'für',\n",
       " 'zu',\n",
       " 'geduld',\n",
       " 'nahm',\n",
       " 'kennen',\n",
       " 'ins',\n",
       " 'bus',\n",
       " 'mir',\n",
       " 'der',\n",
       " '<',\n",
       " 'blonde',\n",
       " 'sterben',\n",
       " 'peking',\n",
       " 'glauben',\n",
       " 'ohne',\n",
       " 'früh',\n",
       " 'er',\n",
       " 'trägt',\n",
       " 'eine',\n",
       " 'tom',\n",
       " 'junge',\n",
       " 'würden',\n",
       " 'den',\n",
       " 'lerne',\n",
       " 'danke',\n",
       " 'bett',\n",
       " 'bitte',\n",
       " 'brille',\n",
       " 'chinesisch',\n",
       " 'zurück',\n",
       " 'dieses',\n",
       " 'in',\n",
       " 'luft',\n",
       " 'ihre']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars_de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_vecs[0].flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_sentence = ['i', 'i', 'took', 'the', 'bus', 'back', 'i']\n",
    "test_sentence_vec = [char_en_to_vec[ch] for ch in test_sentence]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Variable                   Type        Data/Info\n",
    "------------------------------------------------\n",
    "Whh                        ndarray     50x50: 2500 elems, type `float64`, 20000 bytes\n",
    "Why                        ndarray     497x50: 24850 elems, type `float64`, 198800 bytes (194.140625 kb)\n",
    "Wxh                        ndarray     50x497: 24850 elems, type `float64`, 198800 bytes (194.140625 kb)\n",
    "bh                         ndarray     50x1: 50 elems, type `float64`, 400 bytes\n",
    "by                         ndarray     497x1: 497 elems, type `float64`, 3976 bytes\n",
    "char_to_ix                 dict        n=497\n",
    "chars                      list        n=497\n",
    "dWhh                       ndarray     50x50: 2500 elems, type `float64`, 20000 bytes\n",
    "dWhy                       ndarray     497x50: 24850 elems, type `float64`, 198800 bytes (194.140625 kb)\n",
    "dWxh                       ndarray     50x497: 24850 elems, type `float64`, 198800 bytes (194.140625 kb)\n",
    "data                       list        n=1400\n",
    "data_de_processed          list        n=21680\n",
    "data_de_processed_tokens   list        n=100\n",
    "data_en_processed          list        n=21680\n",
    "data_en_processed_tokens   list        n=100\n",
    "data_full                  list        n=1400\n",
    "data_size                  int         1400\n",
    "dbh                        ndarray     50x1: 50 elems, type `float64`, 400 bytes\n",
    "dby                        ndarray     497x1: 497 elems, type `float64`, 3976 bytes\n",
    "dparam                     ndarray     497x1: 497 elems, type `float64`, 3976 bytes\n",
    "en_ch_cnt                  int         700\n",
    "gensim                     module      <module 'gensim' from '/h<...>ages/gensim/__init__.py'>\n",
    "hidden_size                int         50\n",
    "hprev                      ndarray     50x1: 50 elems, type `float64`, 400 bytes\n",
    "i                          int         259\n",
    "inputs                     list        n=7\n",
    "ix_to_char                 dict        n=497\n",
    "json                       module      <module 'json' from '/hom<...>hon3.5/json/__init__.py'>\n",
    "learning_rate              float       0.01\n",
    "loss                       float64     23.793629885\n",
    "lossFun                    function    <function lossFun at 0x7f61930aeb70>\n",
    "mWhh                       ndarray     50x50: 2500 elems, type `float64`, 20000 bytes\n",
    "mWhy                       ndarray     497x50: 24850 elems, type `float64`, 198800 bytes (194.140625 kb)\n",
    "mWxh                       ndarray     50x497: 24850 elems, type `float64`, 198800 bytes (194.140625 kb)\n",
    "math                       module      <module 'math' from '/hom<...>3.5/lib-dynload/math.so'>\n",
    "maxFinalLength             int         7\n",
    "maxRawLength               int         5\n",
    "mbh                        ndarray     50x1: 50 elems, type `float64`, 400 bytes\n",
    "mby                        ndarray     497x1: 497 elems, type `float64`, 3976 bytes\n",
    "mem                        ndarray     497x1: 497 elems, type `float64`, 3976 bytes\n",
    "modelEn                    Word2Vec    Word2Vec(vocab=6972, size=100, alpha=0.025)\n",
    "modelGe                    Word2Vec    Word2Vec(vocab=9279, size=100, alpha=0.025)\n",
    "n                          int         2459\n",
    "np                         module      <module 'numpy' from '/ho<...>kages/numpy/__init__.py'>\n",
    "p                          int         581\n",
    "param                      ndarray     497x1: 497 elems, type `float64`, 3976 bytes\n",
    "random                     module      <module 'random' from '/h<...>lib/python3.5/random.py'>\n",
    "replacePunctuation         function    <function replacePunctuation at 0x7f61b38527b8>\n",
    "sample                     function    <function sample at 0x7f61c4e2b0d0>\n",
    "sample_ix                  list        n=7\n",
    "seq_length                 int         7\n",
    "slen                       int         7\n",
    "smooth_loss                float64     21.9345595918\n",
    "start                      int         255\n",
    "string                     module      <module 'string' from '/h<...>lib/python3.5/string.py'>\n",
    "targets                    list        n=7\n",
    "tempDe                     list        n=132173\n",
    "tempEn                     list        n=132173\n",
    "test_sentence              list        n=7\n",
    "time                       module      <module 'time' (built-in)>\n",
    "translate                  function    <function translate at 0x7f61b27228c8>\n",
    "txt                        str         < ich ist ist > > >\n",
    "vocab_size                 int         497\n",
    "wordCount                  function    <function wordCount at 0x7f61b2b3d510>\n",
    "x                          ndarray     100: 100 elems, type `float32`, 400 bytes\n",
    "y                          ndarray     100: 100 elems, type `float32`, 400 bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
