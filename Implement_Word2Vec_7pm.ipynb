{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length after processing : 21680\n"
     ]
    }
   ],
   "source": [
    "#DATA PREPROCESSING\n",
    "\n",
    "import json\n",
    "\n",
    "data = json.load(open('en_de_corpus.json', 'r'))\n",
    "\n",
    "#Script to remove punctuation, ensure <= max allowed length + padding\n",
    "\n",
    "import string\n",
    "import random\n",
    "import math\n",
    "\n",
    "#replace punctuation with space, ensure all sentences with  1 space\n",
    "#also all capital to small \n",
    "def replacePunctuation(x):\n",
    "    puncAndNum = string.punctuation+'01233456789'\n",
    "    x = x.lower()\n",
    "    for char in x:\n",
    "        if char in puncAndNum:\n",
    "            #print(char)\n",
    "            x=x.replace(char,' ')\n",
    "    return ' '.join(x.split())\n",
    "\n",
    "#max allowed raw length=5, pad to make it 5, allow only equal pairs\n",
    "tempEn = list(map(replacePunctuation,data['en']))\n",
    "tempDe = list(map(replacePunctuation,data['de']))\n",
    "maxRawLength = 5\n",
    "maxFinalLength = maxRawLength + 2\n",
    "\n",
    "def wordCount(x):\n",
    "    return len(x.split())\n",
    "\n",
    "data_en_processed = list()\n",
    "data_de_processed = list()\n",
    "for i in range(len(tempEn)):\n",
    "    if(wordCount(tempEn[i]) <= maxRawLength and wordCount(tempDe[i]) <= maxRawLength\n",
    "       and wordCount(tempEn[i])==wordCount(tempDe[i])):\n",
    "        #if(i%10==0):\n",
    "        #    print((maxFinalLength -1 - wordCount(tempEn[i])))\n",
    "        data_en_processed.append('< '+tempEn[i]+' >'*(maxFinalLength -1 - wordCount(tempEn[i])))\n",
    "        data_de_processed.append('< '+tempDe[i]+' >'*(maxFinalLength -1 - wordCount(tempDe[i])))\n",
    "\n",
    "print(\"length after processing : \"+str(len(data_en_processed)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< thanks for your patience > > | < danke für ihre geduld > >\n",
      "<Dank für Ihre Geduld>>\n",
      "\n",
      "< tom goes to bed early > | < tom geht früh ins bett >\n",
      "<Tom geht früh ins Bett>\n",
      "\n",
      "< lions are stronger than wolves > | < löwen sind stärker als wölfe >\n",
      "<Löwen sind stärker als Wölfe>\n",
      "\n",
      "< this flower smells sweet > > | < diese blume duftet süß > >\n",
      "<Diese blume riecht süß>>\n",
      "\n",
      "< this book makes pleasant reading > | < dieses buch liest sich schön >\n",
      "<Dieses Buch macht angenehmes Lesen>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#pick 5 random samples and display, compare with google translate\n",
    "import time\n",
    "from mtranslate import translate\n",
    "\n",
    "\n",
    "start = math.ceil(random.random()*1000)\n",
    "for i in range(start,start+5):\n",
    "    print(data_en_processed[i]+\" | \"+data_de_processed[i])\n",
    "    print(translate(data_en_processed[i],'de','en'))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<', 'er', 'unterrichtet', 'arabisch', '>', '>', '>']\n",
      "****************\n",
      "['<', 'i', 'took', 'the', 'bus', 'back', '>']\n",
      "['<', 'ich', 'nahm', 'den', 'bus', 'zurück', '>']\n"
     ]
    }
   ],
   "source": [
    "# Convert to list of lists from list of strings, remove space\n",
    "\n",
    "data_en_processed_tokens = [s.split() for s in data_en_processed]\n",
    "data_de_processed_tokens = [s.split() for s in data_de_processed]\n",
    "print(data_de_processed_tokens[77])\n",
    "print(\"****************\")\n",
    "\n",
    "#Lower the size\n",
    "data_en_processed_tokens = data_en_processed_tokens[:100] #senCount\n",
    "data_de_processed_tokens = data_de_processed_tokens[:100]\n",
    "data_full = data_en_processed_tokens + data_de_processed_tokens\n",
    "len(data_full)\n",
    "\n",
    "data_full = sum(data_full, [])\n",
    "len(data_full)\n",
    "\n",
    "print(data_full[0:7])\n",
    "print(data_full[(len(data_full)//2)+0 :(len(data_full)//2)+7])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700\n",
      "700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "501"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Partition back into En and De\n",
    "\n",
    "data_en =  data_full[:(len(data_full)//2)]\n",
    "data_de =  data_full[(len(data_full)//2):]\n",
    "print(len(data_en))\n",
    "print(len(data_de))\n",
    "data_en.index('tea')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<', 'i', 'took', 'the', 'bus', 'back', '>']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_de)\n",
    "data_en[0:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<', 'she', 'was', 'making', 'tea', '>', '>'] ['<', 'sie', 'machte', 'gerade', 'tee', '>', '>']\n"
     ]
    }
   ],
   "source": [
    "tempEnList = list()\n",
    "for i in range(int(len(data_en)/7)):\n",
    "    tempEnList.append(data_en[i*7:(i+1)*7])\n",
    "    \n",
    "#tempEnList.append(['tea'])\n",
    "\n",
    "tempDeList = list()\n",
    "for i in range(int(len(data_de)/7)):\n",
    "    tempDeList.append(data_de[i*7:(i+1)*7])\n",
    "    \n",
    "print(tempEnList[71],tempDeList[71])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable                   Type        Data/Info\n",
      "------------------------------------------------\n",
      "data                       dict        n=2\n",
      "data_de                    list        n=700\n",
      "data_de_processed          list        n=21680\n",
      "data_de_processed_tokens   list        n=100\n",
      "data_en                    list        n=700\n",
      "data_en_processed          list        n=21680\n",
      "data_en_processed_tokens   list        n=100\n",
      "data_full                  list        n=1400\n",
      "i                          int         99\n",
      "json                       module      <module 'json' from '/usr<...>hon3.5/json/__init__.py'>\n",
      "math                       module      <module 'math' (built-in)>\n",
      "maxFinalLength             int         7\n",
      "maxRawLength               int         5\n",
      "random                     module      <module 'random' from '/h<...>lib/python3.5/random.py'>\n",
      "replacePunctuation         function    <function replacePunctuation at 0x7fdcd397b268>\n",
      "start                      int         8\n",
      "string                     module      <module 'string' from '/u<...>lib/python3.5/string.py'>\n",
      "tempDe                     list        n=132173\n",
      "tempDeList                 list        n=100\n",
      "tempEn                     list        n=132173\n",
      "tempEnList                 list        n=100\n",
      "time                       module      <module 'time' (built-in)>\n",
      "translate                  function    <function translate at 0x7fdcd27d16a8>\n",
      "wordCount                  function    <function wordCount at 0x7fdcd2bd4f28>\n"
     ]
    }
   ],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<', 'she', 'was', 'making', 'tea', '>', '>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Slow version of gensim.models.doc2vec is being used\n",
      "2017-04-20 22:08:11,681 : WARNING : Slow version of gensim.models.word2vec is being used\n",
      "2017-04-20 22:08:11,685 : INFO : collecting all words and their counts\n",
      "2017-04-20 22:08:11,689 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-04-20 22:08:11,693 : INFO : collected 247 word types from a corpus of 700 raw words and 100 sentences\n",
      "2017-04-20 22:08:11,694 : INFO : Loading a fresh vocabulary\n",
      "2017-04-20 22:08:11,696 : INFO : min_count=1 retains 247 unique words (100% of original 247, drops 0)\n",
      "2017-04-20 22:08:11,700 : INFO : min_count=1 leaves 700 word corpus (100% of original 700, drops 0)\n",
      "2017-04-20 22:08:11,704 : INFO : deleting the raw counts dictionary of 247 items\n",
      "2017-04-20 22:08:11,711 : INFO : sample=0.001 downsamples 52 most-common words\n",
      "2017-04-20 22:08:11,712 : INFO : downsampling leaves estimated 334 word corpus (47.8% of prior 700)\n",
      "2017-04-20 22:08:11,714 : INFO : estimated required memory for 247 words and 100 dimensions: 321100 bytes\n",
      "2017-04-20 22:08:11,717 : INFO : resetting layer weights\n",
      "/home/shinchan/MLProject/venv/lib/python3.5/site-packages/gensim/models/word2vec.py:772: UserWarning: C extension not loaded for Word2Vec, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  warnings.warn(\"C extension not loaded for Word2Vec, training will be slow. \"\n",
      "2017-04-20 22:08:11,727 : INFO : training model with 4 workers on 247 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-04-20 22:08:11,728 : INFO : expecting 100 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-04-20 22:08:11,741 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-04-20 22:08:11,758 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-04-20 22:08:11,763 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-04-20 22:08:12,200 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-04-20 22:08:12,201 : INFO : training on 3500 raw words (1667 effective words) took 0.5s, 3608 effective words/s\n",
      "2017-04-20 22:08:12,204 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-04-20 22:08:12,209 : INFO : saving Word2Vec object under english_word2vec_new, separately None\n",
      "2017-04-20 22:08:12,210 : INFO : not storing attribute syn0norm\n",
      "2017-04-20 22:08:12,214 : INFO : not storing attribute cum_table\n",
      "2017-04-20 22:08:12,221 : INFO : saved english_word2vec_new\n",
      "2017-04-20 22:08:12,222 : WARNING : Slow version of gensim.models.word2vec is being used\n",
      "2017-04-20 22:08:12,224 : INFO : collecting all words and their counts\n",
      "2017-04-20 22:08:12,225 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-04-20 22:08:12,226 : INFO : collected 267 word types from a corpus of 700 raw words and 100 sentences\n",
      "2017-04-20 22:08:12,228 : INFO : Loading a fresh vocabulary\n",
      "2017-04-20 22:08:12,231 : INFO : min_count=1 retains 267 unique words (100% of original 267, drops 0)\n",
      "2017-04-20 22:08:12,232 : INFO : min_count=1 leaves 700 word corpus (100% of original 700, drops 0)\n",
      "2017-04-20 22:08:12,236 : INFO : deleting the raw counts dictionary of 267 items\n",
      "2017-04-20 22:08:12,238 : INFO : sample=0.001 downsamples 52 most-common words\n",
      "2017-04-20 22:08:12,239 : INFO : downsampling leaves estimated 350 word corpus (50.1% of prior 700)\n",
      "2017-04-20 22:08:12,240 : INFO : estimated required memory for 267 words and 100 dimensions: 347100 bytes\n",
      "2017-04-20 22:08:12,247 : INFO : resetting layer weights\n",
      "2017-04-20 22:08:12,255 : INFO : training model with 4 workers on 267 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-04-20 22:08:12,257 : INFO : expecting 100 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-04-20 22:08:12,266 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-04-20 22:08:12,271 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-04-20 22:08:12,272 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-04-20 22:08:12,639 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-04-20 22:08:12,639 : INFO : training on 3500 raw words (1752 effective words) took 0.4s, 4617 effective words/s\n",
      "2017-04-20 22:08:12,640 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-04-20 22:08:12,641 : INFO : saving Word2Vec object under german_word2vec_new, separately None\n",
      "2017-04-20 22:08:12,642 : INFO : not storing attribute syn0norm\n",
      "2017-04-20 22:08:12,644 : INFO : not storing attribute cum_table\n",
      "2017-04-20 22:08:12,651 : INFO : saved german_word2vec_new\n"
     ]
    }
   ],
   "source": [
    "print(tempEnList[71])\n",
    "\n",
    "import gensim, logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "\n",
    "modelEn = gensim.models.Word2Vec(tempEnList,size=100,workers=4,min_count=1)\n",
    "modelEn.save('english_word2vec_new')\n",
    "\n",
    "modelDe = gensim.models.Word2Vec(tempDeList,size=100,workers=4,min_count=1)\n",
    "modelDe.save('german_word2vec_new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -4.45688935e-03,   3.03892558e-03,   3.52939847e-03,\n",
       "        -3.26736644e-03,   6.11908326e-04,   3.92267853e-03,\n",
       "         5.67528419e-04,   7.38426170e-04,  -1.37521396e-03,\n",
       "         5.21868840e-03,  -3.27561307e-03,  -4.72859340e-03,\n",
       "        -1.30712346e-03,  -1.58777600e-03,  -3.79763427e-03,\n",
       "        -1.47244055e-03,  -2.60739122e-03,  -1.10851892e-03,\n",
       "         3.34509386e-04,  -1.11845857e-03,  -4.67869081e-03,\n",
       "        -1.97918201e-03,   4.69967781e-04,   1.27610331e-03,\n",
       "        -3.38368234e-04,  -1.61627703e-03,  -4.27298248e-04,\n",
       "         1.79897610e-03,  -2.74017593e-03,   4.98781912e-03,\n",
       "         3.80989746e-03,   3.28027323e-04,   5.27766161e-03,\n",
       "        -7.19803909e-04,   2.10271333e-03,   2.71167234e-03,\n",
       "         9.28262671e-05,   1.59450306e-03,   8.88162293e-04,\n",
       "         2.81876512e-03,  -4.82553989e-03,  -9.79068805e-04,\n",
       "        -3.15095240e-04,   1.77644298e-03,   2.31294008e-03,\n",
       "        -6.38867787e-04,   3.80676007e-03,   1.10165891e-03,\n",
       "         4.04635537e-03,  -3.59322759e-04,   1.65412945e-04,\n",
       "        -2.67629302e-03,   3.72746820e-03,  -2.53521465e-03,\n",
       "         4.29157075e-03,   9.47109191e-04,  -3.02363466e-03,\n",
       "         3.21077718e-03,   4.49112663e-03,   4.22496488e-03,\n",
       "        -4.28512553e-03,  -3.08617833e-03,  -3.76487360e-03,\n",
       "         3.13857966e-03,  -2.50661490e-03,  -2.33986764e-03,\n",
       "         6.36414870e-06,   1.66281220e-03,  -3.61984619e-03,\n",
       "         5.97886159e-04,  -2.61812820e-03,  -6.60494203e-04,\n",
       "        -1.96531648e-03,  -2.17001769e-03,  -4.32683574e-03,\n",
       "        -1.46452186e-03,  -1.92337774e-03,  -2.38700202e-04,\n",
       "        -2.81839119e-03,  -3.34164617e-03,  -1.37363467e-03,\n",
       "         2.06418382e-03,   5.41261688e-04,   4.01015487e-03,\n",
       "        -2.69618351e-04,  -2.19564652e-03,   1.39768911e-03,\n",
       "         4.29253734e-04,   4.40295180e-03,   3.57712782e-03,\n",
       "        -1.98089541e-03,   2.54222541e-03,   4.12920024e-03,\n",
       "         2.24181655e-04,   1.24821172e-03,  -2.61734752e-03,\n",
       "        -3.74738080e-03,   6.01345731e-04,   3.19122948e-04,\n",
       "        -5.33394574e-04], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelEn['tea']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data_en = [ 'i' if (e == '<' or e == '>') else e for e in data_en ]\n",
    "#data_de = [ 'ich' if (e == '<' or e == '>') else e for e in data_de ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data_en = ['i', 'i', 'took', 'the', 'bus', 'back', 'i', 'i', 'without', 'air', 'we', 'would', 'die', 'i', 'i', 'i', 'study', 'chinese', 'in', 'beijing', 'i']\n",
    "#data_de = ['ich', 'ich', 'nahm', 'den', 'bus', 'zurück', 'ich', 'ich', 'ohne', 'luft', 'würden', 'wir', 'sterben', 'ich', 'ich', 'ich', 'lerne', 'in', 'peking', 'chinesisch', 'ich']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(data_en[21:], data_de[21:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 700 en_chars, 700 de_chars, 100 vec size.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-20 22:08:13,548 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved model at iteration : 0\n",
      "---------------\n",
      "Input Sentence : < i took the bus back >\n",
      "Expected Translation by Google : <Ich nahm den Bus zurück>\n",
      "word 1 : seine 0.28  ,  es 0.26  ,  dollar 0.26  ,  \n",
      "word 2 : das 0.25  ,  verloren 0.25  ,  blonde 0.24  ,  \n",
      "word 3 : löwen 0.25  ,  lest 0.20  ,  zucker 0.20  ,  \n",
      "word 4 : leider 0.27  ,  englisch 0.27  ,  trägt 0.24  ,  \n",
      "word 5 : ich 0.26  ,  immer 0.24  ,  dollar 0.23  ,  \n",
      "word 6 : autorität 0.28  ,  töchter 0.28  ,  sich 0.22  ,  \n",
      "word 7 : dvd 0.33  ,  das 0.25  ,  legte 0.23  ,  \n",
      "iter 0, loss: 32.203955\n",
      "iter 1000, loss: 11.841262\n",
      "iter 2000, loss: 4.353989\n",
      "iter 3000, loss: 1.600952\n",
      "iter 4000, loss: 0.588673\n",
      "iter 5000, loss: 0.216462\n",
      "iter 6000, loss: 0.079601\n",
      "iter 7000, loss: 0.029278\n",
      "iter 8000, loss: 0.010774\n",
      "iter 9000, loss: 0.003970\n",
      "saved model at iteration : 10000\n",
      "---------------\n",
      "Input Sentence : < i took the bus back >\n",
      "Expected Translation by Google : <Ich nahm den Bus zurück>\n",
      "word 1 : < 1.00  ,  > 0.51  ,  mittag 0.35  ,  \n",
      "word 2 : ich 0.99  ,  wir 0.34  ,  legte 0.31  ,  \n",
      "word 3 : du 0.53  ,  > 0.51  ,  ist 0.51  ,  \n",
      "word 4 : die 0.75  ,  > 0.53  ,  der 0.53  ,  \n",
      "word 5 : du 0.46  ,  bus 0.44  ,  wir 0.38  ,  \n",
      "word 6 : ist 0.56  ,  wir 0.39  ,  unnütz 0.31  ,  \n",
      "word 7 : > 1.00  ,  < 0.50  ,  der 0.43  ,  \n",
      "iter 10000, loss: 0.001468\n",
      "iter 11000, loss: 0.000548\n",
      "iter 12000, loss: 0.000210\n",
      "iter 13000, loss: 0.000085\n",
      "iter 14000, loss: 0.000039\n",
      "iter 15000, loss: 0.000022\n",
      "iter 16000, loss: 0.000016\n",
      "iter 17000, loss: 0.000013\n",
      "iter 18000, loss: 0.000012\n",
      "iter 19000, loss: 0.000012\n",
      "saved model at iteration : 20000\n",
      "---------------\n",
      "Input Sentence : < i took the bus back >\n",
      "Expected Translation by Google : <Ich nahm den Bus zurück>\n",
      "word 1 : < 1.00  ,  > 0.51  ,  früh 0.35  ,  \n",
      "word 2 : ich 0.99  ,  wir 0.34  ,  englisch 0.30  ,  \n",
      "word 3 : nahm 0.47  ,  du 0.45  ,  ist 0.45  ,  \n",
      "word 4 : die 0.68  ,  der 0.58  ,  > 0.44  ,  \n",
      "word 5 : bus 0.50  ,  du 0.45  ,  patient 0.34  ,  \n",
      "word 6 : zurück 0.38  ,  unnütz 0.31  ,  ist 0.29  ,  \n",
      "word 7 : > 1.00  ,  < 0.51  ,  der 0.44  ,  \n",
      "iter 20000, loss: 0.000012\n",
      "iter 21000, loss: 0.000011\n",
      "iter 22000, loss: 0.000011\n",
      "iter 23000, loss: 0.000011\n",
      "iter 24000, loss: 0.000011\n",
      "iter 25000, loss: 0.000011\n",
      "iter 26000, loss: 0.000011\n",
      "iter 27000, loss: 0.000011\n",
      "iter 28000, loss: 0.000011\n",
      "iter 29000, loss: 0.000011\n",
      "saved model at iteration : 30000\n",
      "---------------\n",
      "Input Sentence : < i took the bus back >\n",
      "Expected Translation by Google : <Ich nahm den Bus zurück>\n",
      "word 1 : < 0.99  ,  > 0.49  ,  früh 0.35  ,  \n",
      "word 2 : ich 0.98  ,  wir 0.36  ,  legte 0.31  ,  \n",
      "word 3 : nahm 0.48  ,  du 0.42  ,  der 0.40  ,  \n",
      "word 4 : die 0.59  ,  der 0.58  ,  den 0.51  ,  \n",
      "word 5 : bus 0.52  ,  du 0.41  ,  patient 0.36  ,  \n",
      "word 6 : zurück 0.50  ,  unnütz 0.27  ,  schick 0.26  ,  \n",
      "word 7 : > 0.99  ,  < 0.51  ,  der 0.44  ,  \n",
      "iter 30000, loss: 0.000010\n",
      "iter 31000, loss: 0.000010\n",
      "iter 32000, loss: 0.000010\n",
      "iter 33000, loss: 0.000010\n",
      "iter 34000, loss: 0.000010\n",
      "iter 35000, loss: 0.000010\n",
      "iter 36000, loss: 0.000010\n",
      "iter 37000, loss: 0.000010\n",
      "iter 38000, loss: 0.000010\n",
      "iter 39000, loss: 0.000010\n",
      "saved model at iteration : 40000\n",
      "---------------\n",
      "Input Sentence : < i took the bus back >\n",
      "Expected Translation by Google : <Ich nahm den Bus zurück>\n",
      "word 1 : < 1.00  ,  > 0.53  ,  mittag 0.36  ,  \n",
      "word 2 : ich 0.99  ,  wir 0.34  ,  legte 0.30  ,  \n",
      "word 3 : nahm 0.50  ,  ist 0.41  ,  der 0.38  ,  \n",
      "word 4 : den 0.57  ,  die 0.54  ,  der 0.54  ,  \n",
      "word 5 : bus 0.59  ,  patient 0.38  ,  du 0.35  ,  \n",
      "word 6 : zurück 0.59  ,  unnütz 0.27  ,  wann 0.27  ,  \n",
      "word 7 : > 0.99  ,  < 0.51  ,  die 0.45  ,  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from mtranslate import translate\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "\n",
    "\n",
    "# data I/O\n",
    "#data = open('input_en_de_1000.txt', 'r').read() # should be simple plain text file\n",
    "#data = data_full\n",
    "chars_en = list(set(data_en))\n",
    "chars_de = list(set(data_de))\n",
    "vocab_size = 100\n",
    "\n",
    "print('data has %d en_chars, %d de_chars, %d vec size.' % (len(data_en), len(data_de), vocab_size))\n",
    "\n",
    "char_en_to_vec = { ch:modelEn[ch] for ch in chars_en }\n",
    "char_de_to_vec = { ch:modelDe[ch] for ch in chars_de }\n",
    "\n",
    "#vec_en_to_char = {  }\n",
    "#vec_de_to_char = {  }\n",
    "\n",
    "#char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "#ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters---> 43     loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "\n",
    "hidden_size = 250 # size of hidden layer of neurons\n",
    "seq_length = 7 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-2 #1e-1\n",
    "\n",
    "#globals\n",
    "#en_ch_cnt = (len(data_full)//2) #len(data_full)//2\n",
    "test_sentence = ['<', 'i', 'took', 'the', 'bus', 'back', '>']\n",
    "#test_sentence = [ '<', 'the', 'boy', 'is', 'wearing', 'glasses', '>']\n",
    "#test_sentence = [ '<', 'i', 'ate', 'a', 'delicious', 'apple', '>']\n",
    "test_sentence_de= translate(' '.join(test_sentence),'de', 'en').split()\n",
    "\n",
    "test_sentence_vec = [char_en_to_vec[ch] for ch in test_sentence]\n",
    "#'<', 'ich', 'nahm', 'den', 'bus', 'zurück', '>'\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "modelParametersDict = {}\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "  \"\"\"\n",
    "  inputs,targets are both list of integers.\n",
    "  hprev is Hx1 array of initial hidden state\n",
    "  returns the loss, gradients on model parameters, and last hidden state\n",
    "  \"\"\"\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  loss = 0\n",
    "  # forward pass\n",
    "  for t in range(len(inputs)):\n",
    "    #xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "    #xs[t][inputs[t]] = 1\n",
    "    xs[t] = np.reshape(np.array(inputs[t]), (vocab_size,1) )\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "    #ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "    \n",
    "\n",
    "    #loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "    \n",
    "    loss += 0.5*np.square(np.subtract(np.reshape(np.array(targets[t]),(vocab_size,1)) , ys[t]))\n",
    "    \n",
    "  # backward pass: compute gradients going backwards\n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "  for t in reversed(range(len(inputs))):\n",
    "    #dy = np.copy(ps[t])\n",
    "    #dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "    dy = -1*np.subtract(np.reshape(np.array(targets[t]),(vocab_size,1)) , ys[t])\n",
    "    \n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    dby += dy\n",
    "    dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "    dbh += dhraw\n",
    "    dWxh += np.dot(dhraw, xs[t].T)\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "    dhnext = np.dot(Whh.T, dhraw)\n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "\n",
    "\n",
    "def sample(h, n):\n",
    "\n",
    "  n = len(test_sentence_vec)\n",
    "\n",
    "  translated_vecs = []\n",
    "  for t in range(n):\n",
    "    #x = np.zeros((vocab_size, 1))\n",
    "    #x[char_to_ix[test_sentence[t]]] = 1\n",
    "    x = np.reshape(np.array(test_sentence_vec[t]), (vocab_size,1) )\n",
    "        \n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    y = np.dot(Why, h) + by   \n",
    "    translated_vecs.append(y)\n",
    "  return translated_vecs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "while True:\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  if p+seq_length+1 >= len(data_en) or n == 0: \n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "    p = 0 # go from start of data\n",
    "  inputs = [char_en_to_vec[ch] for ch in data_en[p:p+seq_length]]\n",
    "  #targets = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_de_to_vec[ch] for ch in data_de[p:p+seq_length]]\n",
    "\n",
    "  # sample from the model now and then\n",
    "  if n % 10000 == 0:\n",
    "    slen = len(test_sentence_vec)\n",
    "    sample_vecs = sample(hprev, slen)\n",
    "    \n",
    "    print('---------------')\n",
    "    print('Input Sentence : '+' '.join(test_sentence))\n",
    "    print('Expected Translation by Google : '+' '.join(test_sentence_de))\n",
    "    \n",
    "    i = 0\n",
    "    for v in sample_vecs:\n",
    "        top_n = modelDe.similar_by_vector(v.flatten(), topn=3, restrict_vocab=None)\n",
    "        print(\"word \"+str(i+1)+\" : \",end='')\n",
    "        for j in range(3):\n",
    "            print(top_n[j][0],\"%.2f\" %top_n[j][1] ,\" , \" ,end=' ')\n",
    "        i=i+1\n",
    "        print()\n",
    "    if(n==40000):\n",
    "        break\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient\n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "  if n % 1000 == 0: print('iter %d, loss: %f' % (n, np.mean(smooth_loss)) ) # print progress\n",
    "  \n",
    "  # perform parameter update with Adagrad\n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "    mem += dparam * dparam\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "  p += seq_length # move data pointer\n",
    "  n += 1 # iteration counter \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[  2.07670734e-03],\n",
      "       [ -4.06371152e-04],\n",
      "       [ -3.42080426e-04],\n",
      "       [  5.15222125e-03],\n",
      "       [  1.84234927e-03],\n",
      "       [ -2.08671396e-04],\n",
      "       [ -4.25859257e-04],\n",
      "       [ -5.63789543e-03],\n",
      "       [ -1.80087852e-03],\n",
      "       [ -3.57092576e-03],\n",
      "       [ -3.60609454e-04],\n",
      "       [ -1.20417018e-03],\n",
      "       [ -2.11774490e-03],\n",
      "       [ -1.88793261e-03],\n",
      "       [ -2.86027079e-04],\n",
      "       [  5.10478535e-04],\n",
      "       [ -3.80731930e-03],\n",
      "       [ -6.12193420e-04],\n",
      "       [  2.33198220e-03],\n",
      "       [ -1.26030728e-03],\n",
      "       [  2.41871383e-03],\n",
      "       [  3.50779070e-03],\n",
      "       [  3.33497706e-03],\n",
      "       [  5.50822940e-03],\n",
      "       [  2.07445445e-03],\n",
      "       [  7.92457594e-05],\n",
      "       [ -9.84727590e-04],\n",
      "       [ -1.92324555e-03],\n",
      "       [  2.54320804e-03],\n",
      "       [  5.20021176e-04],\n",
      "       [  2.96060715e-03],\n",
      "       [ -8.77065247e-04],\n",
      "       [ -2.61733737e-03],\n",
      "       [  5.13997292e-04],\n",
      "       [ -1.98663648e-03],\n",
      "       [  1.57545716e-03],\n",
      "       [  3.06609369e-03],\n",
      "       [  2.18998863e-03],\n",
      "       [ -3.11040908e-03],\n",
      "       [  9.93166997e-05],\n",
      "       [ -4.27415497e-03],\n",
      "       [ -3.95796637e-03],\n",
      "       [ -1.44683043e-03],\n",
      "       [ -3.88084559e-03],\n",
      "       [  1.13871676e-03],\n",
      "       [  2.63527547e-03],\n",
      "       [ -2.81060018e-03],\n",
      "       [  1.58929465e-03],\n",
      "       [ -7.52325349e-04],\n",
      "       [ -1.60145371e-03],\n",
      "       [  2.78943026e-03],\n",
      "       [ -3.65450845e-03],\n",
      "       [  2.00839273e-03],\n",
      "       [ -4.33855152e-04],\n",
      "       [ -2.74493939e-03],\n",
      "       [ -1.68368513e-03],\n",
      "       [ -4.50019105e-03],\n",
      "       [ -1.09279690e-04],\n",
      "       [ -2.34261355e-03],\n",
      "       [  1.75378676e-03],\n",
      "       [  3.98800570e-03],\n",
      "       [ -4.43796510e-03],\n",
      "       [ -1.46612629e-03],\n",
      "       [  3.31144821e-03],\n",
      "       [ -3.21242397e-03],\n",
      "       [  1.34169268e-03],\n",
      "       [ -3.15313572e-04],\n",
      "       [  2.20168411e-03],\n",
      "       [ -1.17425195e-03],\n",
      "       [  4.08300761e-03],\n",
      "       [ -4.97645582e-03],\n",
      "       [ -3.62098859e-03],\n",
      "       [ -3.65913663e-03],\n",
      "       [  4.10239933e-04],\n",
      "       [  1.00605175e-03],\n",
      "       [  7.75237013e-04],\n",
      "       [ -2.82825070e-03],\n",
      "       [  2.35748137e-04],\n",
      "       [ -2.61793227e-04],\n",
      "       [  1.67300296e-03],\n",
      "       [  5.59822969e-04],\n",
      "       [ -2.07574438e-03],\n",
      "       [ -2.00078719e-03],\n",
      "       [ -3.60215478e-04],\n",
      "       [ -6.77185821e-04],\n",
      "       [ -1.14121189e-03],\n",
      "       [ -1.71030932e-03],\n",
      "       [ -2.34703936e-03],\n",
      "       [ -2.89957708e-03],\n",
      "       [  1.84100297e-03],\n",
      "       [ -6.35360285e-04],\n",
      "       [  4.23308003e-03],\n",
      "       [ -2.59338998e-04],\n",
      "       [  4.49956255e-03],\n",
      "       [  2.90619410e-03],\n",
      "       [  5.30864746e-04],\n",
      "       [  2.69356868e-03],\n",
      "       [  1.72123913e-03],\n",
      "       [ -9.00437562e-04],\n",
      "       [  3.21337333e-03]]), array([[  1.21479451e-03],\n",
      "       [  7.17585468e-05],\n",
      "       [  6.68523738e-04],\n",
      "       [  4.97227954e-03],\n",
      "       [  1.52433089e-03],\n",
      "       [ -8.75212351e-04],\n",
      "       [ -1.06947502e-03],\n",
      "       [ -5.18696801e-03],\n",
      "       [ -2.42357200e-03],\n",
      "       [ -3.52951052e-03],\n",
      "       [  2.21853157e-04],\n",
      "       [ -1.72261838e-03],\n",
      "       [ -1.28163156e-03],\n",
      "       [ -2.21123838e-04],\n",
      "       [ -6.19928082e-04],\n",
      "       [ -1.30228398e-03],\n",
      "       [ -3.19583028e-03],\n",
      "       [ -9.36993319e-04],\n",
      "       [  1.62717045e-03],\n",
      "       [ -1.39846589e-03],\n",
      "       [  3.83045159e-03],\n",
      "       [  3.60502613e-03],\n",
      "       [  3.11223457e-03],\n",
      "       [  4.79900195e-03],\n",
      "       [  1.31904638e-03],\n",
      "       [ -9.37550967e-05],\n",
      "       [  2.06735659e-04],\n",
      "       [ -9.85556008e-04],\n",
      "       [  2.85511743e-03],\n",
      "       [ -7.50949604e-04],\n",
      "       [  3.89203268e-03],\n",
      "       [ -1.29032977e-03],\n",
      "       [ -1.77764847e-03],\n",
      "       [  1.55826800e-04],\n",
      "       [ -2.74249309e-03],\n",
      "       [  1.87808239e-03],\n",
      "       [  1.97153727e-03],\n",
      "       [  6.83254983e-04],\n",
      "       [ -3.38217353e-03],\n",
      "       [  6.31160895e-06],\n",
      "       [ -5.08042525e-03],\n",
      "       [ -4.31713436e-03],\n",
      "       [ -3.63162540e-03],\n",
      "       [ -2.34331250e-03],\n",
      "       [  8.85688573e-04],\n",
      "       [  2.25082547e-03],\n",
      "       [ -2.68254635e-03],\n",
      "       [  1.73510905e-03],\n",
      "       [  1.41593048e-04],\n",
      "       [ -3.64449562e-04],\n",
      "       [  3.45090313e-03],\n",
      "       [ -4.49164462e-03],\n",
      "       [  1.93283459e-03],\n",
      "       [ -1.17205447e-03],\n",
      "       [ -3.09145823e-03],\n",
      "       [ -1.62388403e-03],\n",
      "       [ -3.45706356e-03],\n",
      "       [ -2.54927039e-04],\n",
      "       [ -2.94638202e-03],\n",
      "       [  2.17589094e-03],\n",
      "       [  3.68810110e-03],\n",
      "       [ -4.11964185e-03],\n",
      "       [ -2.32933077e-03],\n",
      "       [  3.80881337e-03],\n",
      "       [ -9.49981833e-04],\n",
      "       [  1.03658229e-03],\n",
      "       [ -3.18516196e-04],\n",
      "       [  8.05019166e-04],\n",
      "       [ -1.01705153e-03],\n",
      "       [  3.14777264e-03],\n",
      "       [ -4.57831272e-03],\n",
      "       [ -3.14033473e-03],\n",
      "       [ -3.45693940e-03],\n",
      "       [ -4.32680582e-04],\n",
      "       [  1.33358202e-03],\n",
      "       [ -1.18981749e-03],\n",
      "       [ -2.76945081e-03],\n",
      "       [  5.03601611e-04],\n",
      "       [ -3.52405028e-04],\n",
      "       [  1.80583305e-03],\n",
      "       [  7.23598785e-05],\n",
      "       [ -2.03370339e-03],\n",
      "       [ -2.34817473e-03],\n",
      "       [ -4.51510851e-04],\n",
      "       [  2.48892303e-04],\n",
      "       [ -6.95134233e-04],\n",
      "       [ -1.80450092e-03],\n",
      "       [ -1.33720363e-03],\n",
      "       [ -2.08787416e-03],\n",
      "       [  2.44213843e-03],\n",
      "       [  2.67462562e-05],\n",
      "       [  4.01843225e-03],\n",
      "       [ -9.00414286e-04],\n",
      "       [  4.66445599e-03],\n",
      "       [  1.19548538e-03],\n",
      "       [  1.69428881e-04],\n",
      "       [  1.39832690e-03],\n",
      "       [  1.52884277e-03],\n",
      "       [ -1.89828909e-03],\n",
      "       [  2.71069024e-03]]), array([[ -4.40545868e-04],\n",
      "       [  3.03618639e-03],\n",
      "       [  3.38254644e-03],\n",
      "       [  4.08507560e-04],\n",
      "       [ -1.15783526e-03],\n",
      "       [  1.28834237e-03],\n",
      "       [ -1.13936905e-03],\n",
      "       [  2.81931102e-04],\n",
      "       [  2.05055174e-03],\n",
      "       [  4.02814510e-03],\n",
      "       [  2.16230554e-03],\n",
      "       [  2.41387699e-03],\n",
      "       [ -8.92916498e-04],\n",
      "       [  2.45807928e-03],\n",
      "       [  3.64757380e-03],\n",
      "       [ -2.00194111e-03],\n",
      "       [  2.19828117e-03],\n",
      "       [  3.16599794e-04],\n",
      "       [  1.35879000e-03],\n",
      "       [  6.84548740e-04],\n",
      "       [  6.77064423e-04],\n",
      "       [ -7.43401385e-04],\n",
      "       [  1.39103995e-03],\n",
      "       [ -1.83080084e-03],\n",
      "       [ -1.82010606e-04],\n",
      "       [ -3.62737290e-03],\n",
      "       [  7.57399749e-04],\n",
      "       [ -1.76540045e-03],\n",
      "       [  6.09973905e-03],\n",
      "       [  1.43730125e-03],\n",
      "       [  3.15816887e-04],\n",
      "       [  3.24779432e-03],\n",
      "       [  3.06893403e-03],\n",
      "       [  8.21829691e-04],\n",
      "       [  9.65922319e-04],\n",
      "       [  1.49235967e-03],\n",
      "       [  1.62806793e-05],\n",
      "       [ -1.79501655e-03],\n",
      "       [ -6.29640354e-04],\n",
      "       [  1.62311851e-03],\n",
      "       [ -3.24498051e-04],\n",
      "       [ -2.75412877e-03],\n",
      "       [ -6.84242005e-04],\n",
      "       [ -1.22461525e-03],\n",
      "       [ -1.66003312e-03],\n",
      "       [ -1.20180846e-03],\n",
      "       [ -2.64729274e-03],\n",
      "       [ -1.14490408e-03],\n",
      "       [  3.36227012e-04],\n",
      "       [ -9.29812473e-04],\n",
      "       [  1.45846553e-03],\n",
      "       [  2.17901876e-04],\n",
      "       [  1.75158592e-03],\n",
      "       [  1.83833874e-03],\n",
      "       [  8.35371060e-04],\n",
      "       [  6.00151378e-04],\n",
      "       [  1.48705062e-03],\n",
      "       [  6.69616856e-04],\n",
      "       [  3.73225359e-03],\n",
      "       [  1.04739354e-03],\n",
      "       [ -1.85135181e-04],\n",
      "       [  2.97564255e-04],\n",
      "       [ -1.67384461e-05],\n",
      "       [  8.94267221e-04],\n",
      "       [ -4.58194693e-04],\n",
      "       [  2.91742431e-03],\n",
      "       [  1.01706964e-03],\n",
      "       [ -3.05063430e-03],\n",
      "       [  1.42512603e-03],\n",
      "       [ -7.64878763e-04],\n",
      "       [  1.26179939e-03],\n",
      "       [  3.18321240e-03],\n",
      "       [  1.38300501e-03],\n",
      "       [ -8.38947346e-04],\n",
      "       [ -1.48354906e-03],\n",
      "       [  5.44298772e-04],\n",
      "       [ -2.22001253e-04],\n",
      "       [  1.15766740e-03],\n",
      "       [ -1.10695159e-03],\n",
      "       [  2.25487524e-03],\n",
      "       [  2.16147390e-03],\n",
      "       [ -2.42765443e-03],\n",
      "       [ -1.22941957e-03],\n",
      "       [  2.86573163e-03],\n",
      "       [ -2.89140599e-03],\n",
      "       [  7.00683034e-04],\n",
      "       [ -2.22155956e-03],\n",
      "       [ -9.77640496e-04],\n",
      "       [  3.03232109e-03],\n",
      "       [  5.84009616e-04],\n",
      "       [  1.43073087e-03],\n",
      "       [ -2.18457366e-03],\n",
      "       [  2.13058026e-03],\n",
      "       [ -2.85052143e-04],\n",
      "       [  2.79468452e-03],\n",
      "       [  3.23958653e-03],\n",
      "       [  1.10798312e-03],\n",
      "       [  1.87692312e-03],\n",
      "       [ -2.29404263e-04],\n",
      "       [ -3.62434248e-03]]), array([[  8.49848703e-04],\n",
      "       [  5.98618277e-04],\n",
      "       [  2.65311852e-03],\n",
      "       [  6.57707017e-04],\n",
      "       [ -2.47894837e-04],\n",
      "       [ -2.22224717e-04],\n",
      "       [  1.57841794e-03],\n",
      "       [ -1.69789568e-03],\n",
      "       [ -1.69995819e-03],\n",
      "       [  2.41643928e-03],\n",
      "       [  8.39151305e-04],\n",
      "       [  9.00910375e-04],\n",
      "       [ -1.31955159e-03],\n",
      "       [  5.69399251e-04],\n",
      "       [  8.50978741e-04],\n",
      "       [ -1.24571555e-04],\n",
      "       [  2.20310848e-03],\n",
      "       [  1.03775720e-03],\n",
      "       [  2.65107784e-04],\n",
      "       [ -2.87226879e-04],\n",
      "       [ -1.11817002e-03],\n",
      "       [ -2.69194798e-04],\n",
      "       [  3.18376072e-04],\n",
      "       [ -4.64000467e-04],\n",
      "       [  1.03369154e-04],\n",
      "       [  9.33293070e-04],\n",
      "       [  1.46333640e-03],\n",
      "       [ -1.16264803e-03],\n",
      "       [  1.92835994e-03],\n",
      "       [  2.85599355e-04],\n",
      "       [  1.15170208e-03],\n",
      "       [  5.90544801e-04],\n",
      "       [  1.69022867e-03],\n",
      "       [  1.35376011e-03],\n",
      "       [  7.32087409e-04],\n",
      "       [ -7.80187896e-04],\n",
      "       [  2.92610670e-03],\n",
      "       [  9.73955144e-04],\n",
      "       [ -1.41528596e-03],\n",
      "       [ -7.20259442e-04],\n",
      "       [ -2.97558576e-03],\n",
      "       [ -7.66100754e-04],\n",
      "       [  1.11835286e-05],\n",
      "       [ -1.10847264e-03],\n",
      "       [ -9.79425275e-04],\n",
      "       [  1.66237019e-04],\n",
      "       [ -1.35561543e-03],\n",
      "       [  1.33830756e-03],\n",
      "       [  2.43341337e-03],\n",
      "       [  5.86880944e-04],\n",
      "       [  1.64202032e-03],\n",
      "       [  1.21526075e-03],\n",
      "       [ -8.57826491e-05],\n",
      "       [ -5.12396364e-04],\n",
      "       [ -1.83881984e-03],\n",
      "       [ -2.27333539e-03],\n",
      "       [ -1.49065859e-03],\n",
      "       [ -1.19697009e-03],\n",
      "       [ -7.49856983e-04],\n",
      "       [  1.50082136e-03],\n",
      "       [  7.37721138e-04],\n",
      "       [ -5.55467238e-04],\n",
      "       [ -2.05912380e-03],\n",
      "       [  9.22395437e-04],\n",
      "       [ -1.41291676e-03],\n",
      "       [  6.65061615e-04],\n",
      "       [ -1.27461765e-03],\n",
      "       [  7.78127951e-05],\n",
      "       [  9.51832302e-04],\n",
      "       [  1.91918969e-04],\n",
      "       [ -1.36573357e-03],\n",
      "       [ -2.70952395e-03],\n",
      "       [ -1.24491046e-03],\n",
      "       [  6.64516084e-04],\n",
      "       [  3.22120616e-04],\n",
      "       [  6.99808012e-04],\n",
      "       [ -1.91557306e-03],\n",
      "       [  8.22730074e-04],\n",
      "       [ -2.66331881e-03],\n",
      "       [  1.40101538e-03],\n",
      "       [  4.71535162e-04],\n",
      "       [  4.42483032e-04],\n",
      "       [ -3.40020145e-03],\n",
      "       [ -2.43543050e-04],\n",
      "       [ -1.38528706e-03],\n",
      "       [  2.22431015e-03],\n",
      "       [ -2.29074539e-03],\n",
      "       [  2.27296735e-05],\n",
      "       [ -7.21253186e-04],\n",
      "       [ -7.47796620e-04],\n",
      "       [  1.75210026e-03],\n",
      "       [  1.18802907e-03],\n",
      "       [ -3.25056585e-04],\n",
      "       [ -2.99186014e-04],\n",
      "       [  1.80916524e-03],\n",
      "       [  1.11949978e-03],\n",
      "       [  1.74989727e-03],\n",
      "       [  1.34726252e-03],\n",
      "       [  1.04732690e-03],\n",
      "       [  1.83121846e-03]]), array([[ -3.05598541e-04],\n",
      "       [  5.00956032e-04],\n",
      "       [  1.89603066e-04],\n",
      "       [  8.93221148e-04],\n",
      "       [ -2.64805056e-03],\n",
      "       [ -5.57430555e-04],\n",
      "       [ -2.78137412e-03],\n",
      "       [ -2.21540544e-03],\n",
      "       [  1.70814495e-03],\n",
      "       [  5.30727504e-04],\n",
      "       [ -1.39364812e-03],\n",
      "       [  1.22020997e-03],\n",
      "       [  2.09651602e-03],\n",
      "       [  3.24687684e-03],\n",
      "       [ -1.69981137e-03],\n",
      "       [  6.81755288e-04],\n",
      "       [  7.12433016e-04],\n",
      "       [ -8.61026545e-04],\n",
      "       [  2.92265221e-04],\n",
      "       [ -8.77531356e-04],\n",
      "       [  3.67394450e-04],\n",
      "       [  4.05584457e-04],\n",
      "       [  6.49365640e-04],\n",
      "       [ -8.83653084e-04],\n",
      "       [  5.42709339e-04],\n",
      "       [ -9.67438370e-04],\n",
      "       [ -3.02778325e-03],\n",
      "       [  2.19064198e-03],\n",
      "       [  2.12137145e-03],\n",
      "       [ -7.43301970e-04],\n",
      "       [  2.75450353e-03],\n",
      "       [ -2.00481383e-03],\n",
      "       [  3.27735771e-03],\n",
      "       [ -1.12797147e-03],\n",
      "       [ -6.11267864e-04],\n",
      "       [ -9.26943020e-05],\n",
      "       [ -1.06000056e-03],\n",
      "       [ -8.53667386e-04],\n",
      "       [ -2.05528247e-03],\n",
      "       [ -2.44425330e-03],\n",
      "       [ -2.74336386e-03],\n",
      "       [  1.20132218e-03],\n",
      "       [  1.30346618e-03],\n",
      "       [  2.94903714e-04],\n",
      "       [ -8.47306151e-04],\n",
      "       [ -1.61845170e-03],\n",
      "       [  4.12713788e-04],\n",
      "       [ -4.21187301e-04],\n",
      "       [  9.88527045e-04],\n",
      "       [  3.30863793e-03],\n",
      "       [ -9.94470250e-04],\n",
      "       [  1.06541015e-03],\n",
      "       [  3.10156398e-03],\n",
      "       [  1.49348284e-03],\n",
      "       [  7.70259923e-04],\n",
      "       [ -2.68255006e-04],\n",
      "       [  9.36382940e-04],\n",
      "       [ -1.38995926e-03],\n",
      "       [  1.72047367e-03],\n",
      "       [  6.62725066e-04],\n",
      "       [  1.27756593e-03],\n",
      "       [ -1.78650260e-03],\n",
      "       [  1.56582243e-03],\n",
      "       [  2.41265026e-04],\n",
      "       [ -1.10136165e-03],\n",
      "       [ -9.69340305e-04],\n",
      "       [ -5.63203198e-04],\n",
      "       [ -1.48718744e-03],\n",
      "       [ -2.35999299e-03],\n",
      "       [  7.92877031e-05],\n",
      "       [  1.23525608e-03],\n",
      "       [  2.05354550e-03],\n",
      "       [  2.30869735e-03],\n",
      "       [  3.99041690e-04],\n",
      "       [  1.50424747e-03],\n",
      "       [  2.40739518e-03],\n",
      "       [ -2.32362514e-03],\n",
      "       [  2.40518301e-03],\n",
      "       [  8.19055059e-05],\n",
      "       [  2.05836686e-03],\n",
      "       [  2.77908512e-03],\n",
      "       [  3.16764113e-03],\n",
      "       [ -2.84175734e-03],\n",
      "       [  1.18234678e-03],\n",
      "       [ -1.28071670e-03],\n",
      "       [ -4.94778033e-04],\n",
      "       [ -8.82848645e-04],\n",
      "       [  5.37275939e-04],\n",
      "       [  1.58724955e-03],\n",
      "       [ -3.98940791e-03],\n",
      "       [ -3.44217831e-03],\n",
      "       [  1.86896941e-04],\n",
      "       [ -1.91666181e-04],\n",
      "       [  1.66988955e-03],\n",
      "       [  1.54708861e-03],\n",
      "       [ -1.91681063e-04],\n",
      "       [ -2.07181968e-04],\n",
      "       [ -2.26129075e-03],\n",
      "       [  1.44201233e-03],\n",
      "       [ -2.09349456e-03]]), array([[ -2.28321355e-03],\n",
      "       [ -1.80941010e-04],\n",
      "       [ -1.00183926e-03],\n",
      "       [ -7.34973996e-04],\n",
      "       [ -7.45247150e-04],\n",
      "       [  3.29560709e-03],\n",
      "       [ -2.51068086e-04],\n",
      "       [  3.15349405e-03],\n",
      "       [ -6.53086368e-04],\n",
      "       [  1.36081953e-03],\n",
      "       [  6.40079034e-04],\n",
      "       [  3.15612989e-03],\n",
      "       [ -2.22952180e-04],\n",
      "       [  9.05034349e-04],\n",
      "       [ -1.92219809e-03],\n",
      "       [  4.29229765e-03],\n",
      "       [ -3.66546030e-03],\n",
      "       [  1.55983416e-03],\n",
      "       [ -2.83171464e-03],\n",
      "       [  3.22179545e-04],\n",
      "       [ -2.91871882e-03],\n",
      "       [  1.07877968e-03],\n",
      "       [  1.93502362e-03],\n",
      "       [  5.36853468e-06],\n",
      "       [  1.00748271e-03],\n",
      "       [ -3.06517058e-03],\n",
      "       [ -1.96298839e-03],\n",
      "       [  3.13943875e-03],\n",
      "       [  3.02666858e-03],\n",
      "       [ -5.22513326e-04],\n",
      "       [  8.20390135e-04],\n",
      "       [  1.32119419e-03],\n",
      "       [ -2.74074152e-04],\n",
      "       [  1.96406396e-04],\n",
      "       [ -3.16889151e-03],\n",
      "       [ -2.10934911e-03],\n",
      "       [  1.14336121e-03],\n",
      "       [ -9.89630814e-04],\n",
      "       [ -2.56084790e-03],\n",
      "       [ -3.60745350e-03],\n",
      "       [ -1.24951914e-03],\n",
      "       [ -1.45072034e-03],\n",
      "       [  1.79492450e-03],\n",
      "       [  9.18564217e-04],\n",
      "       [  3.28437677e-03],\n",
      "       [ -2.88586839e-04],\n",
      "       [  1.93833996e-03],\n",
      "       [ -2.54516220e-03],\n",
      "       [ -7.91669282e-04],\n",
      "       [  1.48255635e-03],\n",
      "       [ -2.85616664e-03],\n",
      "       [  5.80786636e-04],\n",
      "       [ -3.05385904e-03],\n",
      "       [ -1.30309324e-03],\n",
      "       [ -1.01606979e-03],\n",
      "       [  1.78603709e-04],\n",
      "       [  1.44007722e-03],\n",
      "       [  2.14656914e-03],\n",
      "       [  8.57253495e-05],\n",
      "       [ -7.53315773e-04],\n",
      "       [ -4.97334632e-04],\n",
      "       [  7.57935884e-04],\n",
      "       [  1.02798201e-04],\n",
      "       [  3.50233241e-04],\n",
      "       [ -1.34544383e-03],\n",
      "       [ -3.84352330e-03],\n",
      "       [  1.70536322e-03],\n",
      "       [  1.14382103e-03],\n",
      "       [  2.33611162e-03],\n",
      "       [  8.38421118e-04],\n",
      "       [ -1.85112941e-03],\n",
      "       [  5.76904607e-04],\n",
      "       [  4.72786454e-04],\n",
      "       [  8.83256893e-04],\n",
      "       [  1.99142805e-03],\n",
      "       [  2.79385434e-04],\n",
      "       [ -1.87700623e-03],\n",
      "       [ -5.47996798e-05],\n",
      "       [  6.43145662e-04],\n",
      "       [ -2.14456930e-03],\n",
      "       [  3.32665910e-04],\n",
      "       [ -7.29052434e-04],\n",
      "       [ -1.51472166e-04],\n",
      "       [ -7.19371411e-04],\n",
      "       [  1.31085651e-03],\n",
      "       [ -4.83265607e-03],\n",
      "       [  2.66741997e-04],\n",
      "       [  9.23902190e-04],\n",
      "       [ -3.11726774e-03],\n",
      "       [ -3.58832367e-04],\n",
      "       [  3.44791239e-04],\n",
      "       [  1.46822191e-03],\n",
      "       [  2.26442253e-04],\n",
      "       [  1.56100529e-03],\n",
      "       [ -1.14553491e-03],\n",
      "       [ -3.19921896e-03],\n",
      "       [ -3.60461315e-03],\n",
      "       [ -4.36696150e-03],\n",
      "       [  2.47797550e-05],\n",
      "       [ -3.58297927e-03]]), array([[  1.48090884e-03],\n",
      "       [ -8.27191217e-04],\n",
      "       [ -7.15710864e-04],\n",
      "       [  4.72432063e-03],\n",
      "       [  1.37927268e-03],\n",
      "       [ -4.30813530e-04],\n",
      "       [ -3.56693236e-04],\n",
      "       [ -6.56805471e-03],\n",
      "       [ -1.96557109e-03],\n",
      "       [ -4.35578881e-03],\n",
      "       [ -7.65283688e-04],\n",
      "       [ -9.15603278e-04],\n",
      "       [ -1.62195344e-03],\n",
      "       [ -1.56964132e-03],\n",
      "       [ -9.40452188e-04],\n",
      "       [  1.79032410e-04],\n",
      "       [ -4.25930135e-03],\n",
      "       [ -1.47002152e-03],\n",
      "       [  1.54900744e-03],\n",
      "       [ -4.41628532e-04],\n",
      "       [  1.63860643e-03],\n",
      "       [  3.79630057e-03],\n",
      "       [  2.70183446e-03],\n",
      "       [  5.28074830e-03],\n",
      "       [ -1.56973300e-04],\n",
      "       [  1.20864726e-04],\n",
      "       [ -6.93585847e-04],\n",
      "       [ -9.59903554e-04],\n",
      "       [  1.83143989e-03],\n",
      "       [  9.95076065e-04],\n",
      "       [  3.13829946e-03],\n",
      "       [ -9.79482090e-04],\n",
      "       [ -2.89963470e-03],\n",
      "       [ -1.53957059e-04],\n",
      "       [ -1.61008663e-03],\n",
      "       [  2.51213933e-03],\n",
      "       [  2.04053946e-03],\n",
      "       [  2.48609821e-03],\n",
      "       [ -4.99185560e-03],\n",
      "       [ -6.89340531e-06],\n",
      "       [ -3.92887567e-03],\n",
      "       [ -6.01774979e-03],\n",
      "       [ -1.08615098e-03],\n",
      "       [ -3.58159826e-03],\n",
      "       [  5.69847637e-04],\n",
      "       [  2.05774728e-03],\n",
      "       [ -2.36750559e-03],\n",
      "       [  2.31010688e-03],\n",
      "       [ -1.05820177e-03],\n",
      "       [ -1.53570183e-03],\n",
      "       [  3.22027840e-03],\n",
      "       [ -3.51230407e-03],\n",
      "       [  2.27004045e-03],\n",
      "       [ -8.70816537e-04],\n",
      "       [ -2.34443506e-03],\n",
      "       [ -1.16470409e-03],\n",
      "       [ -5.09412564e-03],\n",
      "       [ -1.68877829e-04],\n",
      "       [ -2.18152812e-03],\n",
      "       [  3.16731457e-03],\n",
      "       [  3.53351373e-03],\n",
      "       [ -3.52450616e-03],\n",
      "       [ -1.80726042e-03],\n",
      "       [  3.69299827e-03],\n",
      "       [ -3.44365233e-03],\n",
      "       [  8.66169713e-04],\n",
      "       [ -3.00519632e-04],\n",
      "       [  1.85021181e-03],\n",
      "       [ -1.54245561e-03],\n",
      "       [  4.34791118e-03],\n",
      "       [ -4.75099147e-03],\n",
      "       [ -2.51354536e-03],\n",
      "       [ -3.20913299e-03],\n",
      "       [  2.07997452e-04],\n",
      "       [  8.28430837e-04],\n",
      "       [  1.42737864e-03],\n",
      "       [ -3.39583256e-03],\n",
      "       [  2.61198125e-05],\n",
      "       [  5.04161410e-04],\n",
      "       [  1.12180663e-03],\n",
      "       [  5.74394991e-04],\n",
      "       [ -1.18213747e-03],\n",
      "       [ -2.26686502e-03],\n",
      "       [ -3.09601909e-04],\n",
      "       [ -4.25509321e-04],\n",
      "       [ -1.06659141e-03],\n",
      "       [ -1.39107922e-03],\n",
      "       [ -2.28729090e-03],\n",
      "       [ -1.93241596e-03],\n",
      "       [  1.11625042e-03],\n",
      "       [ -2.95602354e-04],\n",
      "       [  2.07011098e-03],\n",
      "       [ -1.74919858e-03],\n",
      "       [  4.30821123e-03],\n",
      "       [  2.99370541e-03],\n",
      "       [  1.32506283e-04],\n",
      "       [  1.35797473e-03],\n",
      "       [  1.34244843e-03],\n",
      "       [ -3.92585993e-04],\n",
      "       [  2.49116765e-03]])]\n",
      "saved model at iteration outside loop\n",
      "---------------\n",
      "Input Sentence : i i took the bus back i\n",
      "Expected Translation by Google : <Ich nahm den Bus zurück>\n",
      "word 1 : ich 0.96  ,  benutzen 0.32  ,  pause 0.29  ,  \n",
      "word 2 : ich 0.91  ,  pause 0.30  ,  schön 0.29  ,  \n",
      "word 3 : nahm 0.45  ,  der 0.39  ,  mir 0.39  ,  \n",
      "word 4 : der 0.53  ,  die 0.52  ,  den 0.52  ,  \n",
      "word 5 : bus 0.52  ,  du 0.37  ,  patient 0.36  ,  \n",
      "word 6 : zurück 0.58  ,  unnütz 0.27  ,  wann 0.27  ,  \n",
      "word 7 : ich 0.93  ,  zwilling 0.29  ,  schön 0.28  ,  \n"
     ]
    }
   ],
   "source": [
    "# test_sentence_vec, hprev, \n",
    "\n",
    "print(sample_vecs)\n",
    "\n",
    "modelParametersDict['Wxh'] = Wxh\n",
    "modelParametersDict['Whh'] = Whh\n",
    "modelParametersDict['Why'] = Why\n",
    "modelParametersDict['bh'] = bh\n",
    "modelParametersDict['by'] = by\n",
    "modelParametersDict['hprev'] = hprev\n",
    "modelParametersDict['char_en_to_vec'] = char_en_to_vec\n",
    "with open('modelParameters.pickle', 'wb') as handle:\n",
    "    print(\"saved model at iteration outside loop\")\n",
    "    pickle.dump(modelParametersDict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "        \n",
    "slen = len(test_sentence_vec)\n",
    "sample_vecs = sample(hprev, slen)\n",
    "\n",
    "print('---------------')\n",
    "print('Input Sentence : '+' '.join(test_sentence))\n",
    "print('Expected Translation by Google : '+' '.join(test_sentence_de))\n",
    "\n",
    "i = 0\n",
    "for v in sample_vecs:\n",
    "    top_n = modelDe.similar_by_vector(v.flatten(), topn=3, restrict_vocab=None)\n",
    "    print(\"word \"+str(i+1)+\" : \",end='')\n",
    "    for j in range(3):\n",
    "        print(top_n[j][0],\"%.2f\" %top_n[j][1] ,\" , \" ,end=' ')\n",
    "    i=i+1\n",
    "    print()\n",
    "#modelDe.similar_by_vector(yy, topn=1, restrict_vocab=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lerne',\n",
       " 'als',\n",
       " 'großvater',\n",
       " 'kein',\n",
       " 'mary',\n",
       " 'willkommen',\n",
       " 'diese',\n",
       " 'duftet',\n",
       " 'vorbei',\n",
       " 'sterben',\n",
       " 'unterrichtet',\n",
       " 'solltest',\n",
       " 'verletzen',\n",
       " '<',\n",
       " 'bitte',\n",
       " 'telefon',\n",
       " 'es',\n",
       " 'muss',\n",
       " 'füße',\n",
       " 'hoffnung',\n",
       " 'zucker',\n",
       " 'normal',\n",
       " 'beschreiben',\n",
       " 'borg',\n",
       " 'ich',\n",
       " 'wir',\n",
       " 'und',\n",
       " 'apfel',\n",
       " '>',\n",
       " 'alleine',\n",
       " 'hat',\n",
       " 'sehr',\n",
       " 'karl',\n",
       " 'schwarz',\n",
       " 'glauben',\n",
       " 'könnten',\n",
       " 'nahm',\n",
       " 'jetzt',\n",
       " 'früh',\n",
       " 'abschied',\n",
       " 'willst',\n",
       " 'gewesen',\n",
       " 'wann',\n",
       " 'bei',\n",
       " 'wurde',\n",
       " 'würdest',\n",
       " 'fast',\n",
       " 'höre',\n",
       " 'drei',\n",
       " 'beeilen',\n",
       " 'wo',\n",
       " 'keine',\n",
       " 'geht',\n",
       " 'niederländische',\n",
       " 'brille',\n",
       " 'habe',\n",
       " 'eine',\n",
       " 'nichts',\n",
       " 'zurück',\n",
       " 'schnell',\n",
       " 'fliegen',\n",
       " 'roman',\n",
       " 'hatten',\n",
       " 'schwöre',\n",
       " 'schmidt',\n",
       " 'schön',\n",
       " 'verloren',\n",
       " 'gott',\n",
       " 'begann',\n",
       " 'steht',\n",
       " 'ohne',\n",
       " 'roch',\n",
       " 'machte',\n",
       " 'hält',\n",
       " 'für',\n",
       " 'zugelassen',\n",
       " 'blume',\n",
       " 'spreche',\n",
       " 'bus',\n",
       " 'alt',\n",
       " 'weinen',\n",
       " 'etwas',\n",
       " 'reise',\n",
       " 'macht',\n",
       " 'sofort',\n",
       " 'eifersüchtig',\n",
       " 'vögel',\n",
       " 'schulde',\n",
       " 'radio',\n",
       " 'ab',\n",
       " 'kannst',\n",
       " 'mich',\n",
       " 'mittag',\n",
       " 'da',\n",
       " 'mach',\n",
       " 'wäre',\n",
       " 'hatte',\n",
       " 'pistole',\n",
       " 'kaum',\n",
       " 'botschaft',\n",
       " 'ins',\n",
       " 'musik',\n",
       " 'kennen',\n",
       " 'tom',\n",
       " 'lest',\n",
       " 'irrst',\n",
       " 'aß',\n",
       " 'du',\n",
       " 'mir',\n",
       " 'schwirr',\n",
       " 'sich',\n",
       " 'werden',\n",
       " 'schönes',\n",
       " 'lese',\n",
       " 'kommen',\n",
       " 'gefahr',\n",
       " 'die',\n",
       " 'danke',\n",
       " 'stets',\n",
       " 'kostet',\n",
       " 'sagte',\n",
       " 'töricht',\n",
       " 'dich',\n",
       " 'trägt',\n",
       " 'mag',\n",
       " 'wählerisch',\n",
       " 'ehrlich',\n",
       " 'würden',\n",
       " 'gestern',\n",
       " 'ihn',\n",
       " 'auf',\n",
       " 'der',\n",
       " 'tapfer',\n",
       " 'dein',\n",
       " 'ihm',\n",
       " 'marschierten',\n",
       " 'junge',\n",
       " 'geschlossen',\n",
       " 'was',\n",
       " 'benutzen',\n",
       " 'wiederholen',\n",
       " 'herr',\n",
       " 'zwilling',\n",
       " 'liest',\n",
       " 'können',\n",
       " 'sie',\n",
       " 'lied',\n",
       " 'niemand',\n",
       " 'hund',\n",
       " 'trag',\n",
       " 'zu',\n",
       " 'liebe',\n",
       " 'reich',\n",
       " 'verkaufst',\n",
       " 'wölfe',\n",
       " 'peking',\n",
       " 'blonde',\n",
       " 'zeitschriften',\n",
       " 'dem',\n",
       " 'geben',\n",
       " 'lernen',\n",
       " 'wirklich',\n",
       " 'hunger',\n",
       " 'deine',\n",
       " 'laden',\n",
       " 'getränk',\n",
       " 'denke',\n",
       " 'mein',\n",
       " 'stärker',\n",
       " 'liegt',\n",
       " 'legte',\n",
       " 'chinesisch',\n",
       " 'gefällt',\n",
       " 'fisch',\n",
       " 'zerstört',\n",
       " 'scheut',\n",
       " 'sonne',\n",
       " 'winzige',\n",
       " 'seine',\n",
       " 'gekleidet',\n",
       " 'ist',\n",
       " 'gerade',\n",
       " 'jedes',\n",
       " 'karthago',\n",
       " 'luft',\n",
       " 'französisch',\n",
       " 'süß',\n",
       " 'verrückt',\n",
       " 'am',\n",
       " 'das',\n",
       " 'unnütz',\n",
       " 'gerne',\n",
       " 'tot',\n",
       " 'englisch',\n",
       " 'eltern',\n",
       " 'mal',\n",
       " 'arabisch',\n",
       " 'sind',\n",
       " 'schwermütig',\n",
       " 'intensiver',\n",
       " 'nicht',\n",
       " 'haare',\n",
       " 'schreibtisch',\n",
       " 'kim',\n",
       " 'uns',\n",
       " 'kann',\n",
       " 'ernste',\n",
       " 'tee',\n",
       " 'oft',\n",
       " 'guck',\n",
       " 'zusammen',\n",
       " 'bin',\n",
       " 'wetter',\n",
       " 'so',\n",
       " 'löwen',\n",
       " 'immer',\n",
       " 'mittagessen',\n",
       " 'schick',\n",
       " 'jegliche',\n",
       " 'tony',\n",
       " 'geduld',\n",
       " 'dieser',\n",
       " 'schwer',\n",
       " 'zweierreihen',\n",
       " 'töchter',\n",
       " 'herausgegeben',\n",
       " 'war',\n",
       " 'viel',\n",
       " 'patient',\n",
       " 'stark',\n",
       " 'buch',\n",
       " 'sein',\n",
       " 'anrufen',\n",
       " 'beide',\n",
       " 'selten',\n",
       " 'bett',\n",
       " 'autorität',\n",
       " 'wie',\n",
       " 'schneit',\n",
       " 'einen',\n",
       " 'nach',\n",
       " 'zum',\n",
       " 'ein',\n",
       " 'pause',\n",
       " 'wer',\n",
       " 'nieder',\n",
       " 'dollar',\n",
       " 'hilfe',\n",
       " 'singen',\n",
       " 'den',\n",
       " 'kinder',\n",
       " 'denkt',\n",
       " 'dvd',\n",
       " 'leider',\n",
       " 'groß',\n",
       " 'schwimmt',\n",
       " 'winkte',\n",
       " 'ihre',\n",
       " 'salz',\n",
       " 'er',\n",
       " 'dieses',\n",
       " 'funktioniert',\n",
       " 'lächelte',\n",
       " 'werde',\n",
       " 'aßen',\n",
       " 'anzuschreien',\n",
       " 'in']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars_de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_vecs[0].flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_sentence = ['i', 'i', 'took', 'the', 'bus', 'back', 'i']\n",
    "test_sentence_vec = [char_en_to_vec[ch] for ch in test_sentence]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable                   Type              Data/Info\n",
      "------------------------------------------------------\n",
      "Whh                        ndarray           250x250: 62500 elems, type `float64`, 500000 bytes (488.28125 kb)\n",
      "Why                        ndarray           100x250: 25000 elems, type `float64`, 200000 bytes (195.3125 kb)\n",
      "Wxh                        ndarray           250x100: 25000 elems, type `float64`, 200000 bytes (195.3125 kb)\n",
      "bh                         ndarray           250x1: 250 elems, type `float64`, 2000 bytes\n",
      "by                         ndarray           100x1: 100 elems, type `float64`, 800 bytes\n",
      "char_de_to_vec             dict              n=267\n",
      "char_en_to_vec             dict              n=247\n",
      "chars_de                   list              n=267\n",
      "chars_en                   list              n=247\n",
      "dWhh                       ndarray           250x250: 62500 elems, type `float64`, 500000 bytes (488.28125 kb)\n",
      "dWhy                       ndarray           100x250: 25000 elems, type `float64`, 200000 bytes (195.3125 kb)\n",
      "dWxh                       ndarray           250x100: 25000 elems, type `float64`, 200000 bytes (195.3125 kb)\n",
      "data                       dict              n=2\n",
      "data_de                    list              n=700\n",
      "data_de_processed          list              n=21680\n",
      "data_de_processed_tokens   list              n=100\n",
      "data_en                    list              n=700\n",
      "data_en_processed          list              n=21680\n",
      "data_en_processed_tokens   list              n=100\n",
      "data_full                  list              n=1400\n",
      "dbh                        ndarray           250x1: 250 elems, type `float64`, 2000 bytes\n",
      "dby                        ndarray           100x1: 100 elems, type `float64`, 800 bytes\n",
      "dparam                     ndarray           100x1: 100 elems, type `float64`, 800 bytes\n",
      "gensim                     module            <module 'gensim' from '/h<...>ages/gensim/__init__.py'>\n",
      "handle                     BufferedWriter    <_io.BufferedWriter name=<...>'modelParameters.pickle'>\n",
      "hidden_size                int               250\n",
      "hprev                      ndarray           250x1: 250 elems, type `float64`, 2000 bytes\n",
      "i                          int               7\n",
      "inputs                     list              n=7\n",
      "j                          int               2\n",
      "json                       module            <module 'json' from '/usr<...>hon3.5/json/__init__.py'>\n",
      "learning_rate              float             0.01\n",
      "logging                    module            <module 'logging' from '/<...>3.5/logging/__init__.py'>\n",
      "loss                       ndarray           100x1: 100 elems, type `float64`, 800 bytes\n",
      "lossFun                    function          <function lossFun at 0x7fdcba467598>\n",
      "mWhh                       ndarray           250x250: 62500 elems, type `float64`, 500000 bytes (488.28125 kb)\n",
      "mWhy                       ndarray           100x250: 25000 elems, type `float64`, 200000 bytes (195.3125 kb)\n",
      "mWxh                       ndarray           250x100: 25000 elems, type `float64`, 200000 bytes (195.3125 kb)\n",
      "math                       module            <module 'math' (built-in)>\n",
      "maxFinalLength             int               7\n",
      "maxRawLength               int               5\n",
      "mbh                        ndarray           250x1: 250 elems, type `float64`, 2000 bytes\n",
      "mby                        ndarray           100x1: 100 elems, type `float64`, 800 bytes\n",
      "mem                        ndarray           100x1: 100 elems, type `float64`, 800 bytes\n",
      "modelDe                    Word2Vec          Word2Vec(vocab=267, size=100, alpha=0.025)\n",
      "modelEn                    Word2Vec          Word2Vec(vocab=247, size=100, alpha=0.025)\n",
      "modelParametersDict        dict              n=7\n",
      "n                          int               40000\n",
      "np                         module            <module 'numpy' from '/ho<...>kages/numpy/__init__.py'>\n",
      "p                          int               28\n",
      "param                      ndarray           100x1: 100 elems, type `float64`, 800 bytes\n",
      "pickle                     module            <module 'pickle' from '/u<...>lib/python3.5/pickle.py'>\n",
      "pprint                     function          <function pprint at 0x7fdcf24369d8>\n",
      "random                     module            <module 'random' from '/h<...>lib/python3.5/random.py'>\n",
      "replacePunctuation         function          <function replacePunctuation at 0x7fdcd397b268>\n",
      "sample                     function          <function sample at 0x7fdcba467620>\n",
      "sample_vecs                list              n=7\n",
      "seq_length                 int               7\n",
      "slen                       int               7\n",
      "smooth_loss                ndarray           100x1: 100 elems, type `float64`, 800 bytes\n",
      "start                      int               8\n",
      "string                     module            <module 'string' from '/u<...>lib/python3.5/string.py'>\n",
      "targets                    list              n=7\n",
      "tempDe                     list              n=132173\n",
      "tempDeList                 list              n=100\n",
      "tempEn                     list              n=132173\n",
      "tempEnList                 list              n=100\n",
      "test_sentence              list              n=7\n",
      "test_sentence_de           list              n=5\n",
      "test_sentence_vec          list              n=7\n",
      "time                       module            <module 'time' (built-in)>\n",
      "top_n                      list              n=3\n",
      "translate                  function          <function translate at 0x7fdcd27d16a8>\n",
      "v                          ndarray           100x1: 100 elems, type `float64`, 800 bytes\n",
      "vocab_size                 int               100\n",
      "wordCount                  function          <function wordCount at 0x7fdcd2bd4f28>\n"
     ]
    }
   ],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-20-11e3c2eba05c>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-20-11e3c2eba05c>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Variable                   Type        Data/Info\u001b[0m\n\u001b[0m                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Variable                   Type        Data/Info\n",
    "------------------------------------------------\n",
    "Whh                        ndarray     50x50: 2500 elems, type `float64`, 20000 bytes\n",
    "Why                        ndarray     497x50: 24850 elems, type `float64`, 198800 bytes (194.140625 kb)\n",
    "Wxh                        ndarray     50x497: 24850 elems, type `float64`, 198800 bytes (194.140625 kb)\n",
    "bh                         ndarray     50x1: 50 elems, type `float64`, 400 bytes\n",
    "by                         ndarray     497x1: 497 elems, type `float64`, 3976 bytes\n",
    "char_to_ix                 dict        n=497\n",
    "chars                      list        n=497\n",
    "dWhh                       ndarray     50x50: 2500 elems, type `float64`, 20000 bytes\n",
    "dWhy                       ndarray     497x50: 24850 elems, type `float64`, 198800 bytes (194.140625 kb)\n",
    "dWxh                       ndarray     50x497: 24850 elems, type `float64`, 198800 bytes (194.140625 kb)\n",
    "data                       list        n=1400\n",
    "data_de_processed          list        n=21680\n",
    "data_de_processed_tokens   list        n=100\n",
    "data_en_processed          list        n=21680\n",
    "data_en_processed_tokens   list        n=100\n",
    "data_full                  list        n=1400\n",
    "data_size                  int         1400\n",
    "dbh                        ndarray     50x1: 50 elems, type `float64`, 400 bytes\n",
    "dby                        ndarray     497x1: 497 elems, type `float64`, 3976 bytes\n",
    "dparam                     ndarray     497x1: 497 elems, type `float64`, 3976 bytes\n",
    "en_ch_cnt                  int         700\n",
    "gensim                     module      <module 'gensim' from '/h<...>ages/gensim/__init__.py'>\n",
    "hidden_size                int         50\n",
    "hprev                      ndarray     50x1: 50 elems, type `float64`, 400 bytes\n",
    "i                          int         259\n",
    "inputs                     list        n=7\n",
    "ix_to_char                 dict        n=497\n",
    "json                       module      <module 'json' from '/hom<...>hon3.5/json/__init__.py'>\n",
    "learning_rate              float       0.01\n",
    "loss                       float64     23.793629885\n",
    "lossFun                    function    <function lossFun at 0x7f61930aeb70>\n",
    "mWhh                       ndarray     50x50: 2500 elems, type `float64`, 20000 bytes\n",
    "mWhy                       ndarray     497x50: 24850 elems, type `float64`, 198800 bytes (194.140625 kb)\n",
    "mWxh                       ndarray     50x497: 24850 elems, type `float64`, 198800 bytes (194.140625 kb)\n",
    "math                       module      <module 'math' from '/hom<...>3.5/lib-dynload/math.so'>\n",
    "maxFinalLength             int         7\n",
    "maxRawLength               int         5\n",
    "mbh                        ndarray     50x1: 50 elems, type `float64`, 400 bytes\n",
    "mby                        ndarray     497x1: 497 elems, type `float64`, 3976 bytes\n",
    "mem                        ndarray     497x1: 497 elems, type `float64`, 3976 bytes\n",
    "modelEn                    Word2Vec    Word2Vec(vocab=6972, size=100, alpha=0.025)\n",
    "modelGe                    Word2Vec    Word2Vec(vocab=9279, size=100, alpha=0.025)\n",
    "n                          int         2459\n",
    "np                         module      <module 'numpy' from '/ho<...>kages/numpy/__init__.py'>\n",
    "p                          int         581\n",
    "param                      ndarray     497x1: 497 elems, type `float64`, 3976 bytes\n",
    "random                     module      <module 'random' from '/h<...>lib/python3.5/random.py'>\n",
    "replacePunctuation         function    <function replacePunctuation at 0x7f61b38527b8>\n",
    "sample                     function    <function sample at 0x7f61c4e2b0d0>\n",
    "sample_ix                  list        n=7\n",
    "seq_length                 int         7\n",
    "slen                       int         7\n",
    "smooth_loss                float64     21.9345595918\n",
    "start                      int         255\n",
    "string                     module      <module 'string' from '/h<...>lib/python3.5/string.py'>\n",
    "targets                    list        n=7\n",
    "tempDe                     list        n=132173\n",
    "tempEn                     list        n=132173\n",
    "test_sentence              list        n=7\n",
    "time                       module      <module 'time' (built-in)>\n",
    "translate                  function    <function translate at 0x7f61b27228c8>\n",
    "txt                        str         < ich ist ist > > >\n",
    "vocab_size                 int         497\n",
    "wordCount                  function    <function wordCount at 0x7f61b2b3d510>\n",
    "x                          ndarray     100: 100 elems, type `float32`, 400 bytes\n",
    "y                          ndarray     100: 100 elems, type `float32`, 400 bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
