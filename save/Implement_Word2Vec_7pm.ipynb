{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length after processing : 21680\n"
     ]
    }
   ],
   "source": [
    "#DATA PREPROCESSING\n",
    "\n",
    "import json\n",
    "\n",
    "data = json.load(open('en_de_corpus.json', 'r'))\n",
    "\n",
    "#Script to remove punctuation, ensure <= max allowed length + padding\n",
    "\n",
    "import string\n",
    "import random\n",
    "import math\n",
    "\n",
    "#replace punctuation with space, ensure all sentences with  1 space\n",
    "#also all capital to small \n",
    "def replacePunctuation(x):\n",
    "    puncAndNum = string.punctuation+'01233456789'\n",
    "    x = x.lower()\n",
    "    for char in x:\n",
    "        if char in puncAndNum:\n",
    "            #print(char)\n",
    "            x=x.replace(char,' ')\n",
    "    return ' '.join(x.split())\n",
    "\n",
    "#max allowed raw length=5, pad to make it 5, allow only equal pairs\n",
    "tempEn = list(map(replacePunctuation,data['en']))\n",
    "tempDe = list(map(replacePunctuation,data['de']))\n",
    "maxRawLength = 5\n",
    "maxFinalLength = maxRawLength + 2\n",
    "\n",
    "def wordCount(x):\n",
    "    return len(x.split())\n",
    "\n",
    "data_en_processed = list()\n",
    "data_de_processed = list()\n",
    "for i in range(len(tempEn)):\n",
    "    if(wordCount(tempEn[i]) <= maxRawLength and wordCount(tempDe[i]) <= maxRawLength\n",
    "       and wordCount(tempEn[i])==wordCount(tempDe[i])):\n",
    "        #if(i%10==0):\n",
    "        #    print((maxFinalLength -1 - wordCount(tempEn[i])))\n",
    "        data_en_processed.append('< '+tempEn[i]+' >'*(maxFinalLength -1 - wordCount(tempEn[i])))\n",
    "        data_de_processed.append('< '+tempDe[i]+' >'*(maxFinalLength -1 - wordCount(tempDe[i])))\n",
    "\n",
    "print(\"length after processing : \"+str(len(data_en_processed)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< the ring is cursed > > | < der ring ist verflucht > >\n",
      "<Der Ring ist verflucht>>\n",
      "\n",
      "< who are you quarreling with > | < mit wem streitest du dich >\n",
      "<Wer bist du streitet mit>\n",
      "\n",
      "< you should ve notified us > | < ihr hättet uns benachrichtigen sollen >\n",
      "<Sie haben uns mitgeteilt>\n",
      "\n",
      "< she really likes antiques > > | < sie mag antiquitäten sehr > >\n",
      "<Sie mag wirklich antiquitäten>>\n",
      "\n",
      "< are you japanese > > > | < bist du japaner > > >\n",
      "<Bist du japanisch>>>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#pick 5 random samples and display, compare with google translate\n",
    "import time\n",
    "from mtranslate import translate\n",
    "\n",
    "\n",
    "start = math.ceil(random.random()*1000)\n",
    "for i in range(start,start+5):\n",
    "    print(data_en_processed[i]+\" | \"+data_de_processed[i])\n",
    "    print(translate(data_en_processed[i],'de','en'))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<', 'er', 'unterrichtet', 'arabisch', '>', '>', '>']\n",
      "****************\n",
      "['<', 'i', 'took', 'the', 'bus', 'back', '>']\n",
      "['<', 'ich', 'nahm', 'den', 'bus', 'zurück', '>']\n"
     ]
    }
   ],
   "source": [
    "# Convert to list of lists from list of strings, remove space\n",
    "\n",
    "data_en_processed_tokens = [s.split() for s in data_en_processed]\n",
    "data_de_processed_tokens = [s.split() for s in data_de_processed]\n",
    "print(data_de_processed_tokens[77])\n",
    "print(\"****************\")\n",
    "\n",
    "#Lower the size\n",
    "data_en_processed_tokens = data_en_processed_tokens[:100] #senCount\n",
    "data_de_processed_tokens = data_de_processed_tokens[:100]\n",
    "data_full = data_en_processed_tokens + data_de_processed_tokens\n",
    "len(data_full)\n",
    "\n",
    "data_full = sum(data_full, [])\n",
    "len(data_full)\n",
    "\n",
    "print(data_full[0:7])\n",
    "print(data_full[(len(data_full)//2)+0 :(len(data_full)//2)+7])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700\n",
      "700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "501"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Partition back into En and De\n",
    "\n",
    "data_en =  data_full[:(len(data_full)//2)]\n",
    "data_de =  data_full[(len(data_full)//2):]\n",
    "print(len(data_en))\n",
    "print(len(data_de))\n",
    "data_en.index('tea')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<', 'i', 'took', 'the', 'bus', 'back', '>']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_de)\n",
    "data_en[0:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<', 'she', 'was', 'making', 'tea', '>', '>'] ['<', 'sie', 'machte', 'gerade', 'tee', '>', '>']\n"
     ]
    }
   ],
   "source": [
    "tempEnList = list()\n",
    "for i in range(int(len(data_en)/7)):\n",
    "    tempEnList.append(data_en[i*7:(i+1)*7])\n",
    "    \n",
    "#tempEnList.append(['tea'])\n",
    "\n",
    "tempDeList = list()\n",
    "for i in range(int(len(data_de)/7)):\n",
    "    tempDeList.append(data_de[i*7:(i+1)*7])\n",
    "    \n",
    "print(tempEnList[71],tempDeList[71])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable                   Type        Data/Info\n",
      "------------------------------------------------\n",
      "data                       dict        n=2\n",
      "data_de                    list        n=700\n",
      "data_de_processed          list        n=21680\n",
      "data_de_processed_tokens   list        n=100\n",
      "data_en                    list        n=700\n",
      "data_en_processed          list        n=21680\n",
      "data_en_processed_tokens   list        n=100\n",
      "data_full                  list        n=1400\n",
      "i                          int         99\n",
      "json                       module      <module 'json' from '/usr<...>hon3.5/json/__init__.py'>\n",
      "math                       module      <module 'math' (built-in)>\n",
      "maxFinalLength             int         7\n",
      "maxRawLength               int         5\n",
      "random                     module      <module 'random' from '/h<...>lib/python3.5/random.py'>\n",
      "replacePunctuation         function    <function replacePunctuation at 0x7f53550ba268>\n",
      "start                      int         385\n",
      "string                     module      <module 'string' from '/u<...>lib/python3.5/string.py'>\n",
      "tempDe                     list        n=132173\n",
      "tempDeList                 list        n=100\n",
      "tempEn                     list        n=132173\n",
      "tempEnList                 list        n=100\n",
      "time                       module      <module 'time' (built-in)>\n",
      "translate                  function    <function translate at 0x7f533ff556a8>\n",
      "wordCount                  function    <function wordCount at 0x7f5354314f28>\n"
     ]
    }
   ],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<', 'she', 'was', 'making', 'tea', '>', '>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Slow version of gensim.models.doc2vec is being used\n",
      "2017-04-20 20:36:08,174 : WARNING : Slow version of gensim.models.word2vec is being used\n",
      "2017-04-20 20:36:08,175 : INFO : collecting all words and their counts\n",
      "2017-04-20 20:36:08,177 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-04-20 20:36:08,179 : INFO : collected 247 word types from a corpus of 700 raw words and 100 sentences\n",
      "2017-04-20 20:36:08,180 : INFO : Loading a fresh vocabulary\n",
      "2017-04-20 20:36:08,183 : INFO : min_count=1 retains 247 unique words (100% of original 247, drops 0)\n",
      "2017-04-20 20:36:08,187 : INFO : min_count=1 leaves 700 word corpus (100% of original 700, drops 0)\n",
      "2017-04-20 20:36:08,194 : INFO : deleting the raw counts dictionary of 247 items\n",
      "2017-04-20 20:36:08,195 : INFO : sample=0.001 downsamples 52 most-common words\n",
      "2017-04-20 20:36:08,196 : INFO : downsampling leaves estimated 334 word corpus (47.8% of prior 700)\n",
      "2017-04-20 20:36:08,197 : INFO : estimated required memory for 247 words and 100 dimensions: 321100 bytes\n",
      "2017-04-20 20:36:08,198 : INFO : resetting layer weights\n",
      "/home/shinchan/MLProject/venv/lib/python3.5/site-packages/gensim/models/word2vec.py:772: UserWarning: C extension not loaded for Word2Vec, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  warnings.warn(\"C extension not loaded for Word2Vec, training will be slow. \"\n",
      "2017-04-20 20:36:08,203 : INFO : training model with 4 workers on 247 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-04-20 20:36:08,204 : INFO : expecting 100 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-04-20 20:36:08,217 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-04-20 20:36:08,226 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-04-20 20:36:08,233 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-04-20 20:36:08,484 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-04-20 20:36:08,485 : INFO : training on 3500 raw words (1660 effective words) took 0.3s, 6099 effective words/s\n",
      "2017-04-20 20:36:08,486 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-04-20 20:36:08,487 : INFO : saving Word2Vec object under english_word2vec_new, separately None\n",
      "2017-04-20 20:36:08,488 : INFO : not storing attribute syn0norm\n",
      "2017-04-20 20:36:08,489 : INFO : not storing attribute cum_table\n",
      "2017-04-20 20:36:08,494 : INFO : saved english_word2vec_new\n",
      "2017-04-20 20:36:08,495 : WARNING : Slow version of gensim.models.word2vec is being used\n",
      "2017-04-20 20:36:08,497 : INFO : collecting all words and their counts\n",
      "2017-04-20 20:36:08,498 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-04-20 20:36:08,499 : INFO : collected 267 word types from a corpus of 700 raw words and 100 sentences\n",
      "2017-04-20 20:36:08,500 : INFO : Loading a fresh vocabulary\n",
      "2017-04-20 20:36:08,503 : INFO : min_count=1 retains 267 unique words (100% of original 267, drops 0)\n",
      "2017-04-20 20:36:08,504 : INFO : min_count=1 leaves 700 word corpus (100% of original 700, drops 0)\n",
      "2017-04-20 20:36:08,507 : INFO : deleting the raw counts dictionary of 267 items\n",
      "2017-04-20 20:36:08,508 : INFO : sample=0.001 downsamples 52 most-common words\n",
      "2017-04-20 20:36:08,509 : INFO : downsampling leaves estimated 350 word corpus (50.1% of prior 700)\n",
      "2017-04-20 20:36:08,510 : INFO : estimated required memory for 267 words and 100 dimensions: 347100 bytes\n",
      "2017-04-20 20:36:08,513 : INFO : resetting layer weights\n",
      "2017-04-20 20:36:08,519 : INFO : training model with 4 workers on 267 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-04-20 20:36:08,520 : INFO : expecting 100 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-04-20 20:36:08,524 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-04-20 20:36:08,526 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-04-20 20:36:08,528 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-04-20 20:36:08,822 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-04-20 20:36:08,823 : INFO : training on 3500 raw words (1756 effective words) took 0.3s, 5856 effective words/s\n",
      "2017-04-20 20:36:08,824 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-04-20 20:36:08,825 : INFO : saving Word2Vec object under german_word2vec_new, separately None\n",
      "2017-04-20 20:36:08,826 : INFO : not storing attribute syn0norm\n",
      "2017-04-20 20:36:08,827 : INFO : not storing attribute cum_table\n",
      "2017-04-20 20:36:08,832 : INFO : saved german_word2vec_new\n"
     ]
    }
   ],
   "source": [
    "print(tempEnList[71])\n",
    "\n",
    "import gensim, logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "\n",
    "modelEn = gensim.models.Word2Vec(tempEnList,size=100,workers=4,min_count=1)\n",
    "modelEn.save('english_word2vec_new')\n",
    "\n",
    "modelDe = gensim.models.Word2Vec(tempDeList,size=100,workers=4,min_count=1)\n",
    "modelDe.save('german_word2vec_new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -3.77632771e-03,   2.55744439e-03,  -3.59815755e-03,\n",
       "        -1.41804863e-03,   3.11783934e-03,   4.36906354e-04,\n",
       "         3.11766332e-03,  -2.53593037e-03,   3.71849653e-03,\n",
       "        -5.48245625e-05,  -2.89772800e-03,  -7.27310136e-04,\n",
       "        -1.10763060e-04,  -7.34131201e-04,  -1.99156045e-03,\n",
       "        -1.72657054e-03,  -1.43761386e-03,   1.91982789e-03,\n",
       "         2.63948319e-03,   5.03148604e-03,   9.70299123e-04,\n",
       "        -9.09068389e-04,  -2.41712178e-03,   4.96005639e-03,\n",
       "         2.04374990e-03,   1.93760823e-03,  -1.79638469e-03,\n",
       "         1.47405302e-03,   4.66396520e-03,   3.18021886e-03,\n",
       "        -3.91805591e-03,   2.72602658e-03,  -3.01644532e-03,\n",
       "         3.26596596e-03,  -2.56225234e-03,   4.75428533e-03,\n",
       "         2.49239593e-03,  -3.15603521e-03,  -1.44279050e-03,\n",
       "         1.69415225e-03,   1.33182632e-03,  -4.02857969e-03,\n",
       "        -2.26699235e-03,   5.31167025e-05,   5.48876356e-04,\n",
       "         4.20867838e-03,  -4.80213156e-03,  -9.30247072e-04,\n",
       "         2.50406866e-03,   1.48889096e-03,   1.66218798e-03,\n",
       "         1.50830936e-04,   1.98237132e-03,   3.88170476e-03,\n",
       "         3.72464163e-03,   3.86014394e-03,  -2.38485401e-03,\n",
       "        -3.74545879e-03,   4.44434816e-03,   3.96565627e-03,\n",
       "        -3.82613018e-03,   1.35545048e-03,  -1.75351591e-03,\n",
       "        -3.30750458e-03,   2.59715412e-03,  -1.19932636e-03,\n",
       "         3.40619870e-03,  -2.97311554e-03,  -1.07422915e-04,\n",
       "        -3.01763811e-03,  -3.38177802e-03,   1.37390293e-06,\n",
       "        -3.30909528e-03,  -3.49026453e-03,  -3.13009298e-03,\n",
       "         4.63381177e-03,  -4.50513884e-03,  -5.03305253e-03,\n",
       "         1.13073853e-03,  -3.36790923e-03,   1.98320788e-03,\n",
       "        -4.91783442e-03,  -1.43163220e-03,  -2.59862398e-04,\n",
       "         4.10542032e-03,   2.26416020e-03,   4.15199297e-03,\n",
       "        -4.27712890e-04,  -4.84340126e-03,   4.02138988e-03,\n",
       "        -2.98807421e-03,   3.72791803e-03,  -2.40916735e-03,\n",
       "         2.67186027e-04,  -3.47193214e-03,   2.80283671e-03,\n",
       "        -1.20268448e-03,   2.38318127e-04,  -4.31246008e-04,\n",
       "         4.06199088e-03], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelEn['tea']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data_en = [ 'i' if (e == '<' or e == '>') else e for e in data_en ]\n",
    "#data_de = [ 'ich' if (e == '<' or e == '>') else e for e in data_de ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data_en = ['i', 'i', 'took', 'the', 'bus', 'back', 'i', 'i', 'without', 'air', 'we', 'would', 'die', 'i', 'i', 'i', 'study', 'chinese', 'in', 'beijing', 'i']\n",
    "#data_de = ['ich', 'ich', 'nahm', 'den', 'bus', 'zurück', 'ich', 'ich', 'ohne', 'luft', 'würden', 'wir', 'sterben', 'ich', 'ich', 'ich', 'lerne', 'in', 'peking', 'chinesisch', 'ich']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(data_en[21:], data_de[21:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 700 en_chars, 700 de_chars, 100 vec size.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-20 20:36:11,631 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "Input Sentence : < i took the bus back >\n",
      "Expected Translation by Google : <Ich nahm den Bus zurück>\n",
      "word 1 : viel 0.20  ,  zeitschriften 0.20  ,  leider 0.20  ,  \n",
      "word 2 : gefällt 0.24  ,  gestern 0.24  ,  viel 0.20  ,  \n",
      "word 3 : lese 0.31  ,  der 0.27  ,  hat 0.24  ,  \n",
      "word 4 : war 0.27  ,  hilfe 0.26  ,  seine 0.26  ,  \n",
      "word 5 : jetzt 0.28  ,  eifersüchtig 0.24  ,  wie 0.23  ,  \n",
      "word 6 : selten 0.28  ,  karthago 0.27  ,  machte 0.23  ,  \n",
      "word 7 : gefällt 0.27  ,  verrückt 0.26  ,  karthago 0.26  ,  \n",
      "iter 0, loss: 32.203955\n",
      "iter 1000, loss: 11.841261\n",
      "iter 2000, loss: 4.353988\n",
      "iter 3000, loss: 1.600952\n",
      "iter 4000, loss: 0.588672\n",
      "iter 5000, loss: 0.216462\n",
      "iter 6000, loss: 0.079601\n",
      "iter 7000, loss: 0.029278\n",
      "iter 8000, loss: 0.010774\n",
      "iter 9000, loss: 0.003970\n",
      "---------------\n",
      "Input Sentence : < i took the bus back >\n",
      "Expected Translation by Google : <Ich nahm den Bus zurück>\n",
      "word 1 : < 0.99  ,  > 0.40  ,  hoffnung 0.33  ,  \n",
      "word 2 : ich 0.99  ,  süß 0.36  ,  wie 0.34  ,  \n",
      "word 3 : nahm 0.42  ,  arabisch 0.39  ,  ich 0.37  ,  \n",
      "word 4 : die 0.74  ,  den 0.42  ,  bus 0.36  ,  \n",
      "word 5 : > 0.63  ,  bus 0.51  ,  ich 0.44  ,  \n",
      "word 6 : zurück 0.56  ,  liebe 0.36  ,  was 0.35  ,  \n",
      "word 7 : > 1.00  ,  diese 0.37  ,  telefon 0.36  ,  \n",
      "iter 10000, loss: 0.001468\n",
      "iter 11000, loss: 0.000548\n",
      "iter 12000, loss: 0.000209\n",
      "iter 13000, loss: 0.000085\n",
      "iter 14000, loss: 0.000039\n",
      "iter 15000, loss: 0.000022\n",
      "iter 16000, loss: 0.000015\n",
      "iter 17000, loss: 0.000013\n",
      "iter 18000, loss: 0.000012\n",
      "iter 19000, loss: 0.000011\n",
      "---------------\n",
      "Input Sentence : < i took the bus back >\n",
      "Expected Translation by Google : <Ich nahm den Bus zurück>\n",
      "word 1 : < 0.99  ,  > 0.38  ,  er 0.32  ,  \n",
      "word 2 : ich 0.99  ,  süß 0.34  ,  denke 0.31  ,  \n",
      "word 3 : nahm 0.51  ,  gestern 0.31  ,  danke 0.30  ,  \n",
      "word 4 : die 0.66  ,  den 0.58  ,  wir 0.36  ,  \n",
      "word 5 : bus 0.63  ,  > 0.50  ,  < 0.28  ,  \n",
      "word 6 : zurück 0.62  ,  wölfe 0.33  ,  liebe 0.31  ,  \n",
      "word 7 : > 0.99  ,  < 0.38  ,  diese 0.37  ,  \n",
      "iter 20000, loss: 0.000011\n",
      "iter 21000, loss: 0.000011\n",
      "iter 22000, loss: 0.000011\n",
      "iter 23000, loss: 0.000011\n",
      "iter 24000, loss: 0.000011\n",
      "iter 25000, loss: 0.000011\n",
      "iter 26000, loss: 0.000010\n",
      "iter 27000, loss: 0.000010\n",
      "iter 28000, loss: 0.000010\n",
      "iter 29000, loss: 0.000010\n",
      "---------------\n",
      "Input Sentence : < i took the bus back >\n",
      "Expected Translation by Google : <Ich nahm den Bus zurück>\n",
      "word 1 : < 0.99  ,  > 0.39  ,  hoffnung 0.33  ,  \n",
      "word 2 : ich 0.99  ,  süß 0.32  ,  denke 0.32  ,  \n",
      "word 3 : nahm 0.44  ,  eltern 0.31  ,  gestern 0.30  ,  \n",
      "word 4 : den 0.63  ,  die 0.57  ,  sie 0.33  ,  \n",
      "word 5 : bus 0.64  ,  > 0.44  ,  werde 0.27  ,  \n",
      "word 6 : zurück 0.65  ,  wölfe 0.31  ,  musik 0.28  ,  \n",
      "word 7 : > 0.98  ,  < 0.40  ,  diese 0.37  ,  \n",
      "iter 30000, loss: 0.000010\n",
      "iter 31000, loss: 0.000010\n",
      "iter 32000, loss: 0.000010\n",
      "iter 33000, loss: 0.000010\n",
      "iter 34000, loss: 0.000010\n",
      "iter 35000, loss: 0.000010\n",
      "iter 36000, loss: 0.000010\n",
      "iter 37000, loss: 0.000010\n",
      "iter 38000, loss: 0.000010\n",
      "iter 39000, loss: 0.000009\n",
      "---------------\n",
      "Input Sentence : < i took the bus back >\n",
      "Expected Translation by Google : <Ich nahm den Bus zurück>\n",
      "word 1 : < 1.00  ,  > 0.35  ,  er 0.33  ,  \n",
      "word 2 : ich 0.99  ,  süß 0.32  ,  denke 0.32  ,  \n",
      "word 3 : nahm 0.51  ,  gestern 0.32  ,  schön 0.29  ,  \n",
      "word 4 : den 0.72  ,  die 0.44  ,  wir 0.34  ,  \n",
      "word 5 : bus 0.69  ,  > 0.38  ,  borg 0.31  ,  \n",
      "word 6 : zurück 0.70  ,  was 0.29  ,  guck 0.28  ,  \n",
      "word 7 : > 0.98  ,  < 0.39  ,  diese 0.37  ,  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from mtranslate import translate\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "\n",
    "\n",
    "# data I/O\n",
    "#data = open('input_en_de_1000.txt', 'r').read() # should be simple plain text file\n",
    "#data = data_full\n",
    "chars_en = list(set(data_en))\n",
    "chars_de = list(set(data_de))\n",
    "vocab_size = 100\n",
    "\n",
    "print('data has %d en_chars, %d de_chars, %d vec size.' % (len(data_en), len(data_de), vocab_size))\n",
    "\n",
    "char_en_to_vec = { ch:modelEn[ch] for ch in chars_en }\n",
    "char_de_to_vec = { ch:modelDe[ch] for ch in chars_de }\n",
    "\n",
    "#vec_en_to_char = {  }\n",
    "#vec_de_to_char = {  }\n",
    "\n",
    "#char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "#ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters---> 43     loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "\n",
    "hidden_size = 250 # size of hidden layer of neurons\n",
    "seq_length = 7 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-2 #1e-1\n",
    "\n",
    "#globals\n",
    "#en_ch_cnt = (len(data_full)//2) #len(data_full)//2\n",
    "test_sentence = ['<', 'i', 'took', 'the', 'bus', 'back', '>']\n",
    "#test_sentence = [ '<', 'the', 'boy', 'is', 'wearing', 'glasses', '>']\n",
    "#test_sentence = [ '<', 'i', 'ate', 'a', 'delicious', 'apple', '>']\n",
    "test_sentence_de= translate(' '.join(test_sentence),'de', 'en').split()\n",
    "\n",
    "test_sentence_vec = [char_en_to_vec[ch] for ch in test_sentence]\n",
    "#'<', 'ich', 'nahm', 'den', 'bus', 'zurück', '>'\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "modelParametersDict = {}\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "  \"\"\"\n",
    "  inputs,targets are both list of integers.\n",
    "  hprev is Hx1 array of initial hidden state\n",
    "  returns the loss, gradients on model parameters, and last hidden state\n",
    "  \"\"\"\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  loss = 0\n",
    "  # forward pass\n",
    "  for t in range(len(inputs)):\n",
    "    #xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "    #xs[t][inputs[t]] = 1\n",
    "    xs[t] = np.reshape(np.array(inputs[t]), (vocab_size,1) )\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "    #ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "    \n",
    "\n",
    "    #loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "    \n",
    "    loss += 0.5*np.square(np.subtract(np.reshape(np.array(targets[t]),(vocab_size,1)) , ys[t]))\n",
    "    \n",
    "  # backward pass: compute gradients going backwards\n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "  for t in reversed(range(len(inputs))):\n",
    "    #dy = np.copy(ps[t])\n",
    "    #dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "    dy = -1*np.subtract(np.reshape(np.array(targets[t]),(vocab_size,1)) , ys[t])\n",
    "    \n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    dby += dy\n",
    "    dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "    dbh += dhraw\n",
    "    dWxh += np.dot(dhraw, xs[t].T)\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "    dhnext = np.dot(Whh.T, dhraw)\n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "\n",
    "\n",
    "def sample(h, n):\n",
    "\n",
    "  n = len(test_sentence_vec)\n",
    "\n",
    "  translated_vecs = []\n",
    "  for t in range(n):\n",
    "    #x = np.zeros((vocab_size, 1))\n",
    "    #x[char_to_ix[test_sentence[t]]] = 1\n",
    "    x = np.reshape(np.array(test_sentence_vec[t]), (vocab_size,1) )\n",
    "        \n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    y = np.dot(Why, h) + by   \n",
    "    translated_vecs.append(y)\n",
    "  return translated_vecs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "while True:\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  if p+seq_length+1 >= len(data_en) or n == 0: \n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "    p = 0 # go from start of data\n",
    "  inputs = [char_en_to_vec[ch] for ch in data_en[p:p+seq_length]]\n",
    "  #targets = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_de_to_vec[ch] for ch in data_de[p:p+seq_length]]\n",
    "\n",
    "  # sample from the model now and then\n",
    "  if n % 10000 == 0:\n",
    "    slen = len(test_sentence_vec)\n",
    "    sample_vecs = sample(hprev, slen)\n",
    "    \n",
    "    print('---------------')\n",
    "    print('Input Sentence : '+' '.join(test_sentence))\n",
    "    print('Expected Translation by Google : '+' '.join(test_sentence_de))\n",
    "    \n",
    "    i = 0\n",
    "    for v in sample_vecs:\n",
    "        top_n = modelDe.similar_by_vector(v.flatten(), topn=3, restrict_vocab=None)\n",
    "        print(\"word \"+str(i+1)+\" : \",end='')\n",
    "        for j in range(3):\n",
    "            print(top_n[j][0],\"%.2f\" %top_n[j][1] ,\" , \" ,end=' ')\n",
    "        i=i+1\n",
    "        print()\n",
    "    if(n==40000):\n",
    "        modelParametersDict['Wxh'] = Wxh\n",
    "        modelParametersDict['Whh'] = Whh\n",
    "        modelParametersDict['Why'] = Why\n",
    "        modelParametersDict['bh'] = bh\n",
    "        modelParametersDict['by'] = by\n",
    "        modelParametersDict['hprev'] = hprev\n",
    "        modelParametersDict['char_en_to_vec'] = char_en_to_vec\n",
    "        with open('modelParameters.pickle', 'wb') as handle:\n",
    "            pickle.dump(modelParametersDict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        break\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient\n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "  if n % 1000 == 0: print('iter %d, loss: %f' % (n, np.mean(smooth_loss)) ) # print progress\n",
    "  \n",
    "  # perform parameter update with Adagrad\n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "    mem += dparam * dparam\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "  p += seq_length # move data pointer\n",
    "  n += 1 # iteration counter \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_sentence_vec, hprev, \n",
    "\n",
    "slen = len(test_sentence_vec)\n",
    "sample_vecs = sample(hprev, slen)\n",
    "\n",
    "print('---------------')\n",
    "print('Input Sentence : '+' '.join(test_sentence))\n",
    "print('Expected Translation by Google : '+' '.join(test_sentence_de))\n",
    "\n",
    "i = 0\n",
    "for v in sample_vecs:\n",
    "    top_n = modelDe.similar_by_vector(v.flatten(), topn=3, restrict_vocab=None)\n",
    "    print(\"word \"+str(i+1)+\" : \",end='')\n",
    "    for j in range(3):\n",
    "        print(top_n[j][0],\"%.2f\" %top_n[j][1] ,\" , \" ,end=' ')\n",
    "    i=i+1\n",
    "    print()\n",
    "#modelDe.similar_by_vector(yy, topn=1, restrict_vocab=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['geht',\n",
       " 'schwer',\n",
       " 'reich',\n",
       " '>',\n",
       " 'haare',\n",
       " 'lied',\n",
       " 'wir',\n",
       " 'das',\n",
       " 'hat',\n",
       " 'ich',\n",
       " 'war',\n",
       " 'salz',\n",
       " 'für',\n",
       " 'zu',\n",
       " 'geduld',\n",
       " 'nahm',\n",
       " 'kennen',\n",
       " 'ins',\n",
       " 'bus',\n",
       " 'mir',\n",
       " 'der',\n",
       " '<',\n",
       " 'blonde',\n",
       " 'sterben',\n",
       " 'peking',\n",
       " 'glauben',\n",
       " 'ohne',\n",
       " 'früh',\n",
       " 'er',\n",
       " 'trägt',\n",
       " 'eine',\n",
       " 'tom',\n",
       " 'junge',\n",
       " 'würden',\n",
       " 'den',\n",
       " 'lerne',\n",
       " 'danke',\n",
       " 'bett',\n",
       " 'bitte',\n",
       " 'brille',\n",
       " 'chinesisch',\n",
       " 'zurück',\n",
       " 'dieses',\n",
       " 'in',\n",
       " 'luft',\n",
       " 'ihre']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars_de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_vecs[0].flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_sentence = ['i', 'i', 'took', 'the', 'bus', 'back', 'i']\n",
    "test_sentence_vec = [char_en_to_vec[ch] for ch in test_sentence]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Variable                   Type        Data/Info\n",
    "------------------------------------------------\n",
    "Whh                        ndarray     50x50: 2500 elems, type `float64`, 20000 bytes\n",
    "Why                        ndarray     497x50: 24850 elems, type `float64`, 198800 bytes (194.140625 kb)\n",
    "Wxh                        ndarray     50x497: 24850 elems, type `float64`, 198800 bytes (194.140625 kb)\n",
    "bh                         ndarray     50x1: 50 elems, type `float64`, 400 bytes\n",
    "by                         ndarray     497x1: 497 elems, type `float64`, 3976 bytes\n",
    "char_to_ix                 dict        n=497\n",
    "chars                      list        n=497\n",
    "dWhh                       ndarray     50x50: 2500 elems, type `float64`, 20000 bytes\n",
    "dWhy                       ndarray     497x50: 24850 elems, type `float64`, 198800 bytes (194.140625 kb)\n",
    "dWxh                       ndarray     50x497: 24850 elems, type `float64`, 198800 bytes (194.140625 kb)\n",
    "data                       list        n=1400\n",
    "data_de_processed          list        n=21680\n",
    "data_de_processed_tokens   list        n=100\n",
    "data_en_processed          list        n=21680\n",
    "data_en_processed_tokens   list        n=100\n",
    "data_full                  list        n=1400\n",
    "data_size                  int         1400\n",
    "dbh                        ndarray     50x1: 50 elems, type `float64`, 400 bytes\n",
    "dby                        ndarray     497x1: 497 elems, type `float64`, 3976 bytes\n",
    "dparam                     ndarray     497x1: 497 elems, type `float64`, 3976 bytes\n",
    "en_ch_cnt                  int         700\n",
    "gensim                     module      <module 'gensim' from '/h<...>ages/gensim/__init__.py'>\n",
    "hidden_size                int         50\n",
    "hprev                      ndarray     50x1: 50 elems, type `float64`, 400 bytes\n",
    "i                          int         259\n",
    "inputs                     list        n=7\n",
    "ix_to_char                 dict        n=497\n",
    "json                       module      <module 'json' from '/hom<...>hon3.5/json/__init__.py'>\n",
    "learning_rate              float       0.01\n",
    "loss                       float64     23.793629885\n",
    "lossFun                    function    <function lossFun at 0x7f61930aeb70>\n",
    "mWhh                       ndarray     50x50: 2500 elems, type `float64`, 20000 bytes\n",
    "mWhy                       ndarray     497x50: 24850 elems, type `float64`, 198800 bytes (194.140625 kb)\n",
    "mWxh                       ndarray     50x497: 24850 elems, type `float64`, 198800 bytes (194.140625 kb)\n",
    "math                       module      <module 'math' from '/hom<...>3.5/lib-dynload/math.so'>\n",
    "maxFinalLength             int         7\n",
    "maxRawLength               int         5\n",
    "mbh                        ndarray     50x1: 50 elems, type `float64`, 400 bytes\n",
    "mby                        ndarray     497x1: 497 elems, type `float64`, 3976 bytes\n",
    "mem                        ndarray     497x1: 497 elems, type `float64`, 3976 bytes\n",
    "modelEn                    Word2Vec    Word2Vec(vocab=6972, size=100, alpha=0.025)\n",
    "modelGe                    Word2Vec    Word2Vec(vocab=9279, size=100, alpha=0.025)\n",
    "n                          int         2459\n",
    "np                         module      <module 'numpy' from '/ho<...>kages/numpy/__init__.py'>\n",
    "p                          int         581\n",
    "param                      ndarray     497x1: 497 elems, type `float64`, 3976 bytes\n",
    "random                     module      <module 'random' from '/h<...>lib/python3.5/random.py'>\n",
    "replacePunctuation         function    <function replacePunctuation at 0x7f61b38527b8>\n",
    "sample                     function    <function sample at 0x7f61c4e2b0d0>\n",
    "sample_ix                  list        n=7\n",
    "seq_length                 int         7\n",
    "slen                       int         7\n",
    "smooth_loss                float64     21.9345595918\n",
    "start                      int         255\n",
    "string                     module      <module 'string' from '/h<...>lib/python3.5/string.py'>\n",
    "targets                    list        n=7\n",
    "tempDe                     list        n=132173\n",
    "tempEn                     list        n=132173\n",
    "test_sentence              list        n=7\n",
    "time                       module      <module 'time' (built-in)>\n",
    "translate                  function    <function translate at 0x7f61b27228c8>\n",
    "txt                        str         < ich ist ist > > >\n",
    "vocab_size                 int         497\n",
    "wordCount                  function    <function wordCount at 0x7f61b2b3d510>\n",
    "x                          ndarray     100: 100 elems, type `float32`, 400 bytes\n",
    "y                          ndarray     100: 100 elems, type `float32`, 400 bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
